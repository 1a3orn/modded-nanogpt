import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True

# -----------------------------------------------------------------------------
# Custom operators

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

def lm_head_fp8(x: Tensor, w: Tensor) -> Tensor:
    _x = x.flatten(0, -2)
    out: Tensor = torch.ops.nanogpt.mm(_x, w, x_s=2.0, w_s=32.0, grad_s=2.0**29)[0]
    return out.reshape(*x.shape[:-1], -1)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev():
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int):
        super().__init__(in_features, out_features, bias=False)

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x):
        return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len=65536):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        # self.c_q = CastedLinear(dim, dim)
        # self.c_k = CastedLinear(dim, dim)
        # self.c_v = CastedLinear(dim, dim)
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std
        self.qkv_w = nn.Parameter(torch.empty(3, dim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads) # dim // num_heads = head_dim
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # Set attention scale such that the minimum attainable attention entropy
        # (but not necessary the attention entropy itself) is close to 0. By @leloykun
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        # q = self.c_q(x).view(B, T, self.num_heads, -1)
        # k = self.c_k(x).view(B, T, self.num_heads, -1)
        # v = self.c_v(x).view(B, T, self.num_heads, -1)
        # qkv_weight = torch.cat([self.c_q.weight, self.c_k.weight, self.c_v.weight], dim=0).type_as(x)
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3*self.num_heads, -1).chunk(3, dim=-2)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, model_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(model_dim, num_heads, layer_idx) if layer_idx != 7 else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, ve, x0, block_mask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, num_embeddings: int, embedding_dim: int):
        super().__init__()
        self.embed = nn.ModuleList([nn.Embedding(num_embeddings, embedding_dim) for _ in range(3)])

    def forward(self, input_seq) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2], None, None, None, None, None, None, ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual learning
        self.value_embeds = ValueEmbedding(vocab_size, model_dim)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128))
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        assert input_seq.ndim == 1
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        docs = (input_seq == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        def create_doc_swc_block_masks(sliding_window_num_blocks: Tensor):
            kv_idx = block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            window_bm = q_idx - kv_idx < sliding_window_num_blocks
            window_full_bm = window_bm # block-wise sliding window by @YouJiacheng
            # document_bm = (docs_low[q_idx] <= docs_high[kv_idx]) & (docs_low[kv_idx] <= docs_high[q_idx])
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & window_bm & document_bm
            full_bm  = causal_full_bm & window_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            short_sliding_window_num_blocks = sliding_window_num_blocks // 2
            return (
                BlockMask.from_kv_blocks(
                    kv_num_blocks,
                    kv_indices,
                    full_kv_num_blocks,
                    full_kv_indices,
                    BLOCK_SIZE=BLOCK_SIZE,
                    mask_mod=document_causal,
                ),
                BlockMask.from_kv_blocks(
                    torch.clamp_max(kv_num_blocks, torch.clamp_min(short_sliding_window_num_blocks - full_kv_num_blocks, 1)),
                    kv_indices,
                    torch.clamp_max(full_kv_num_blocks, short_sliding_window_num_blocks - 1),
                    full_kv_indices,
                    BLOCK_SIZE=BLOCK_SIZE,
                    mask_mod=document_causal,
                ),
            )

        # Long-short SWA block masks by @leloykun & @YouJiacheng
        long_swa_block_mask, short_swa_block_mask = create_doc_swc_block_masks(sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        is_long_block_mask = [True, False, False, False, True, False]
        for i in range(self.num_encoder_layers):
            block_mask = long_swa_block_mask if is_long_block_mask[i] else short_swa_block_mask
            x = self.blocks[i](x, ve_enc[i], x0, block_mask)
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        is_long_block_mask = list(reversed(is_long_block_mask))
        for i in range(self.num_decoder_layers):
            block_mask = long_swa_block_mask if is_long_block_mask[i] else short_swa_block_mask
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_mask)
        x = norm(x)
        logits = lm_head_fp8(x, self.lm_head.weight) if self.training else self.lm_head(x)
        # @Grad62304977 added tanh softcapping, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use cycle(files) if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    batch_size = 8*64*1024 # batch size in tokens
    num_iterations = 1395 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 64*1024 # FlexAttention sequence length
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
             if console:
                 print(s)
             print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_loader = distributed_data_generator(args.train_files, args.batch_size, rank, world_size)

model = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim >= 2]
embed_params = [model.embed.weight, *model.value_embeds.parameters()]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), fused=True, eps=1e-10)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(it: int):
    t = 1 - it / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_bs = world_size * args.seq_len
        assert args.val_tokens % val_bs == 0
        val_steps = args.val_tokens // val_bs
        val_loader = distributed_data_generator(args.val_files, val_bs, rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION BEGIN -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB"
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250110+cu126 compiled for CUDA 12.6
Thu Jan 16 20:17:39 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   39C    P0             122W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   30C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   29C    P0             115W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0             118W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   39C    P0             120W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   31C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   37C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1395 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1395 train_time:26020ms step_avg:nanms
step:2/1395 train_time:26537ms step_avg:nanms
step:3/1395 train_time:26657ms step_avg:nanms
step:4/1395 train_time:26777ms step_avg:nanms
step:5/1395 train_time:26898ms step_avg:nanms
step:6/1395 train_time:27019ms step_avg:nanms
step:7/1395 train_time:27141ms step_avg:nanms
step:8/1395 train_time:27262ms step_avg:nanms
step:9/1395 train_time:27383ms step_avg:nanms
step:10/1395 train_time:27504ms step_avg:nanms
step:11/1395 train_time:124ms step_avg:nanms
step:12/1395 train_time:246ms step_avg:nanms
step:13/1395 train_time:368ms step_avg:122.56ms
step:14/1395 train_time:490ms step_avg:122.44ms
step:15/1395 train_time:611ms step_avg:122.28ms
step:16/1395 train_time:733ms step_avg:122.20ms
step:17/1395 train_time:856ms step_avg:122.25ms
step:18/1395 train_time:977ms step_avg:122.17ms
step:19/1395 train_time:1101ms step_avg:122.32ms
step:20/1395 train_time:1224ms step_avg:122.38ms
step:21/1395 train_time:1347ms step_avg:122.42ms
step:22/1395 train_time:1468ms step_avg:122.37ms
step:23/1395 train_time:1592ms step_avg:122.45ms
step:24/1395 train_time:1715ms step_avg:122.47ms
step:25/1395 train_time:1836ms step_avg:122.42ms
step:26/1395 train_time:1958ms step_avg:122.37ms
step:27/1395 train_time:2080ms step_avg:122.36ms
step:28/1395 train_time:2202ms step_avg:122.31ms
step:29/1395 train_time:2324ms step_avg:122.34ms
step:30/1395 train_time:2446ms step_avg:122.32ms
step:31/1395 train_time:2570ms step_avg:122.37ms
step:32/1395 train_time:2691ms step_avg:122.33ms
step:33/1395 train_time:2814ms step_avg:122.33ms
step:34/1395 train_time:2936ms step_avg:122.33ms
step:35/1395 train_time:3058ms step_avg:122.31ms
step:36/1395 train_time:3179ms step_avg:122.29ms
step:37/1395 train_time:3303ms step_avg:122.32ms
step:38/1395 train_time:3425ms step_avg:122.31ms
step:39/1395 train_time:3547ms step_avg:122.31ms
step:40/1395 train_time:3669ms step_avg:122.31ms
step:41/1395 train_time:3792ms step_avg:122.31ms
step:42/1395 train_time:3915ms step_avg:122.34ms
step:43/1395 train_time:4039ms step_avg:122.38ms
step:44/1395 train_time:4161ms step_avg:122.38ms
step:45/1395 train_time:4282ms step_avg:122.35ms
step:46/1395 train_time:4405ms step_avg:122.36ms
step:47/1395 train_time:4528ms step_avg:122.39ms
step:48/1395 train_time:4650ms step_avg:122.37ms
step:49/1395 train_time:4773ms step_avg:122.39ms
step:50/1395 train_time:4896ms step_avg:122.41ms
step:51/1395 train_time:5019ms step_avg:122.42ms
step:52/1395 train_time:5141ms step_avg:122.41ms
step:53/1395 train_time:5263ms step_avg:122.40ms
step:54/1395 train_time:5385ms step_avg:122.38ms
step:55/1395 train_time:5507ms step_avg:122.39ms
step:56/1395 train_time:5631ms step_avg:122.41ms
step:57/1395 train_time:5752ms step_avg:122.39ms
step:58/1395 train_time:5875ms step_avg:122.39ms
step:59/1395 train_time:5997ms step_avg:122.38ms
step:60/1395 train_time:6119ms step_avg:122.38ms
step:61/1395 train_time:6241ms step_avg:122.38ms
step:62/1395 train_time:6364ms step_avg:122.38ms
step:63/1395 train_time:6486ms step_avg:122.37ms
step:64/1395 train_time:6609ms step_avg:122.38ms
step:65/1395 train_time:6730ms step_avg:122.37ms
step:66/1395 train_time:6854ms step_avg:122.38ms
step:67/1395 train_time:6977ms step_avg:122.40ms
step:68/1395 train_time:7098ms step_avg:122.38ms
step:69/1395 train_time:7221ms step_avg:122.40ms
step:70/1395 train_time:7343ms step_avg:122.39ms
step:71/1395 train_time:7464ms step_avg:122.36ms
step:72/1395 train_time:7588ms step_avg:122.38ms
step:73/1395 train_time:7709ms step_avg:122.37ms
step:74/1395 train_time:7832ms step_avg:122.37ms
step:75/1395 train_time:7954ms step_avg:122.37ms
step:76/1395 train_time:8076ms step_avg:122.37ms
step:77/1395 train_time:8199ms step_avg:122.37ms
step:78/1395 train_time:8320ms step_avg:122.36ms
step:79/1395 train_time:8442ms step_avg:122.35ms
step:80/1395 train_time:8565ms step_avg:122.36ms
step:81/1395 train_time:8688ms step_avg:122.37ms
step:82/1395 train_time:8812ms step_avg:122.39ms
step:83/1395 train_time:8936ms step_avg:122.41ms
step:84/1395 train_time:9058ms step_avg:122.41ms
step:85/1395 train_time:9181ms step_avg:122.41ms
step:86/1395 train_time:9302ms step_avg:122.40ms
step:87/1395 train_time:9424ms step_avg:122.39ms
step:88/1395 train_time:9547ms step_avg:122.40ms
step:89/1395 train_time:9669ms step_avg:122.39ms
step:90/1395 train_time:9791ms step_avg:122.39ms
step:91/1395 train_time:9915ms step_avg:122.41ms
step:92/1395 train_time:10038ms step_avg:122.42ms
step:93/1395 train_time:10160ms step_avg:122.41ms
step:94/1395 train_time:10282ms step_avg:122.40ms
step:95/1395 train_time:10403ms step_avg:122.39ms
step:96/1395 train_time:10526ms step_avg:122.39ms
step:97/1395 train_time:10647ms step_avg:122.38ms
step:98/1395 train_time:10770ms step_avg:122.38ms
step:99/1395 train_time:10892ms step_avg:122.38ms
step:100/1395 train_time:11016ms step_avg:122.40ms
step:101/1395 train_time:11139ms step_avg:122.41ms
step:102/1395 train_time:11261ms step_avg:122.40ms
step:103/1395 train_time:11382ms step_avg:122.38ms
step:104/1395 train_time:11504ms step_avg:122.38ms
step:105/1395 train_time:11627ms step_avg:122.38ms
step:106/1395 train_time:11751ms step_avg:122.40ms
step:107/1395 train_time:11873ms step_avg:122.40ms
step:108/1395 train_time:11996ms step_avg:122.41ms
step:109/1395 train_time:12119ms step_avg:122.41ms
step:110/1395 train_time:12242ms step_avg:122.42ms
step:111/1395 train_time:12364ms step_avg:122.41ms
step:112/1395 train_time:12487ms step_avg:122.42ms
step:113/1395 train_time:12610ms step_avg:122.43ms
step:114/1395 train_time:12733ms step_avg:122.43ms
step:115/1395 train_time:12856ms step_avg:122.44ms
step:116/1395 train_time:12979ms step_avg:122.44ms
step:117/1395 train_time:13102ms step_avg:122.45ms
step:118/1395 train_time:13226ms step_avg:122.46ms
step:119/1395 train_time:13349ms step_avg:122.47ms
step:120/1395 train_time:13471ms step_avg:122.47ms
step:121/1395 train_time:13596ms step_avg:122.48ms
step:122/1395 train_time:13719ms step_avg:122.49ms
step:123/1395 train_time:13842ms step_avg:122.49ms
step:124/1395 train_time:13965ms step_avg:122.50ms
step:125/1395 train_time:14090ms step_avg:122.52ms
step:125/1395 val_loss:4.3962 train_time:14212ms step_avg:123.58ms
step:126/1395 train_time:14233ms step_avg:122.70ms
step:127/1395 train_time:14353ms step_avg:122.68ms
step:128/1395 train_time:14483ms step_avg:122.74ms
step:129/1395 train_time:14607ms step_avg:122.75ms
step:130/1395 train_time:14729ms step_avg:122.74ms
step:131/1395 train_time:14851ms step_avg:122.74ms
step:132/1395 train_time:14974ms step_avg:122.73ms
step:133/1395 train_time:15096ms step_avg:122.73ms
step:134/1395 train_time:15218ms step_avg:122.73ms
step:135/1395 train_time:15341ms step_avg:122.73ms
step:136/1395 train_time:15465ms step_avg:122.74ms
step:137/1395 train_time:15590ms step_avg:122.75ms
step:138/1395 train_time:15713ms step_avg:122.76ms
step:139/1395 train_time:15836ms step_avg:122.76ms
step:140/1395 train_time:15959ms step_avg:122.76ms
step:141/1395 train_time:16081ms step_avg:122.76ms
step:142/1395 train_time:16204ms step_avg:122.75ms
step:143/1395 train_time:16327ms step_avg:122.76ms
step:144/1395 train_time:16452ms step_avg:122.77ms
step:145/1395 train_time:16575ms step_avg:122.77ms
step:146/1395 train_time:16698ms step_avg:122.78ms
step:147/1395 train_time:16821ms step_avg:122.78ms
step:148/1395 train_time:16944ms step_avg:122.78ms
step:149/1395 train_time:17067ms step_avg:122.79ms
step:150/1395 train_time:17190ms step_avg:122.79ms
step:151/1395 train_time:17313ms step_avg:122.79ms
step:152/1395 train_time:17435ms step_avg:122.78ms
step:153/1395 train_time:17559ms step_avg:122.79ms
step:154/1395 train_time:17682ms step_avg:122.79ms
step:155/1395 train_time:17805ms step_avg:122.79ms
step:156/1395 train_time:17929ms step_avg:122.80ms
step:157/1395 train_time:18051ms step_avg:122.80ms
step:158/1395 train_time:18174ms step_avg:122.80ms
step:159/1395 train_time:18297ms step_avg:122.80ms
step:160/1395 train_time:18420ms step_avg:122.80ms
step:161/1395 train_time:18542ms step_avg:122.80ms
step:162/1395 train_time:18666ms step_avg:122.80ms
step:163/1395 train_time:18789ms step_avg:122.81ms
step:164/1395 train_time:18913ms step_avg:122.81ms
step:165/1395 train_time:19035ms step_avg:122.81ms
step:166/1395 train_time:19158ms step_avg:122.81ms
step:167/1395 train_time:19281ms step_avg:122.81ms
step:168/1395 train_time:19404ms step_avg:122.81ms
step:169/1395 train_time:19527ms step_avg:122.81ms
step:170/1395 train_time:19649ms step_avg:122.81ms
step:171/1395 train_time:19772ms step_avg:122.81ms
step:172/1395 train_time:19896ms step_avg:122.81ms
step:173/1395 train_time:20019ms step_avg:122.81ms
step:174/1395 train_time:20141ms step_avg:122.81ms
step:175/1395 train_time:20263ms step_avg:122.81ms
step:176/1395 train_time:20386ms step_avg:122.81ms
step:177/1395 train_time:20509ms step_avg:122.81ms
step:178/1395 train_time:20633ms step_avg:122.82ms
step:179/1395 train_time:20756ms step_avg:122.82ms
step:180/1395 train_time:20879ms step_avg:122.82ms
step:181/1395 train_time:21002ms step_avg:122.82ms
step:182/1395 train_time:21125ms step_avg:122.82ms
step:183/1395 train_time:21249ms step_avg:122.83ms
step:184/1395 train_time:21372ms step_avg:122.83ms
step:185/1395 train_time:21495ms step_avg:122.83ms
step:186/1395 train_time:21618ms step_avg:122.83ms
step:187/1395 train_time:21740ms step_avg:122.83ms
step:188/1395 train_time:21863ms step_avg:122.82ms
step:189/1395 train_time:21985ms step_avg:122.82ms
step:190/1395 train_time:22109ms step_avg:122.83ms
step:191/1395 train_time:22232ms step_avg:122.83ms
step:192/1395 train_time:22354ms step_avg:122.82ms
step:193/1395 train_time:22476ms step_avg:122.82ms
step:194/1395 train_time:22599ms step_avg:122.82ms
step:195/1395 train_time:22722ms step_avg:122.82ms
step:196/1395 train_time:22845ms step_avg:122.82ms
step:197/1395 train_time:22968ms step_avg:122.82ms
step:198/1395 train_time:23091ms step_avg:122.82ms
step:199/1395 train_time:23214ms step_avg:122.83ms
step:200/1395 train_time:23337ms step_avg:122.83ms
step:201/1395 train_time:23460ms step_avg:122.83ms
step:202/1395 train_time:23584ms step_avg:122.83ms
step:203/1395 train_time:23706ms step_avg:122.83ms
step:204/1395 train_time:23829ms step_avg:122.83ms
step:205/1395 train_time:23952ms step_avg:122.83ms
step:206/1395 train_time:24075ms step_avg:122.83ms
step:207/1395 train_time:24198ms step_avg:122.83ms
step:208/1395 train_time:24320ms step_avg:122.83ms
step:209/1395 train_time:24443ms step_avg:122.83ms
step:210/1395 train_time:24567ms step_avg:122.83ms
step:211/1395 train_time:24691ms step_avg:122.84ms
step:212/1395 train_time:24814ms step_avg:122.84ms
step:213/1395 train_time:24938ms step_avg:122.85ms
step:214/1395 train_time:25061ms step_avg:122.85ms
step:215/1395 train_time:25184ms step_avg:122.85ms
step:216/1395 train_time:25308ms step_avg:122.85ms
step:217/1395 train_time:25432ms step_avg:122.86ms
step:218/1395 train_time:25555ms step_avg:122.86ms
step:219/1395 train_time:25678ms step_avg:122.86ms
step:220/1395 train_time:25801ms step_avg:122.86ms
step:221/1395 train_time:25925ms step_avg:122.87ms
step:222/1395 train_time:26048ms step_avg:122.87ms
step:223/1395 train_time:26171ms step_avg:122.87ms
step:224/1395 train_time:26294ms step_avg:122.87ms
step:225/1395 train_time:26418ms step_avg:122.87ms
step:226/1395 train_time:26541ms step_avg:122.88ms
step:227/1395 train_time:26663ms step_avg:122.87ms
step:228/1395 train_time:26787ms step_avg:122.88ms
step:229/1395 train_time:26911ms step_avg:122.88ms
step:230/1395 train_time:27034ms step_avg:122.88ms
step:231/1395 train_time:27157ms step_avg:122.88ms
step:232/1395 train_time:27282ms step_avg:122.89ms
step:233/1395 train_time:27405ms step_avg:122.89ms
step:234/1395 train_time:27529ms step_avg:122.90ms
step:235/1395 train_time:27652ms step_avg:122.90ms
step:236/1395 train_time:27775ms step_avg:122.90ms
step:237/1395 train_time:27899ms step_avg:122.90ms
step:238/1395 train_time:28022ms step_avg:122.91ms
step:239/1395 train_time:28147ms step_avg:122.91ms
step:240/1395 train_time:28272ms step_avg:122.92ms
step:241/1395 train_time:28396ms step_avg:122.92ms
step:242/1395 train_time:28520ms step_avg:122.93ms
step:243/1395 train_time:28643ms step_avg:122.93ms
step:244/1395 train_time:28766ms step_avg:122.93ms
step:245/1395 train_time:28889ms step_avg:122.93ms
step:246/1395 train_time:29013ms step_avg:122.94ms
step:247/1395 train_time:29136ms step_avg:122.94ms
step:248/1395 train_time:29261ms step_avg:122.94ms
step:249/1395 train_time:29383ms step_avg:122.94ms
step:250/1395 train_time:29507ms step_avg:122.95ms
step:250/1395 val_loss:3.9847 train_time:29630ms step_avg:123.46ms
step:251/1395 train_time:29652ms step_avg:123.04ms
step:252/1395 train_time:29769ms step_avg:123.01ms
step:253/1395 train_time:29896ms step_avg:123.03ms
step:254/1395 train_time:30019ms step_avg:123.03ms
step:255/1395 train_time:30142ms step_avg:123.03ms
step:256/1395 train_time:30265ms step_avg:123.03ms
step:257/1395 train_time:30388ms step_avg:123.03ms
step:258/1395 train_time:30511ms step_avg:123.03ms
step:259/1395 train_time:30634ms step_avg:123.03ms
step:260/1395 train_time:30757ms step_avg:123.03ms
step:261/1395 train_time:30880ms step_avg:123.03ms
step:262/1395 train_time:31004ms step_avg:123.03ms
step:263/1395 train_time:31127ms step_avg:123.03ms
step:264/1395 train_time:31250ms step_avg:123.03ms
step:265/1395 train_time:31374ms step_avg:123.03ms
step:266/1395 train_time:31498ms step_avg:123.04ms
step:267/1395 train_time:31622ms step_avg:123.04ms
step:268/1395 train_time:31746ms step_avg:123.04ms
step:269/1395 train_time:31869ms step_avg:123.05ms
step:270/1395 train_time:31992ms step_avg:123.05ms
step:271/1395 train_time:32116ms step_avg:123.05ms
step:272/1395 train_time:32238ms step_avg:123.05ms
step:273/1395 train_time:32362ms step_avg:123.05ms
step:274/1395 train_time:32485ms step_avg:123.05ms
step:275/1395 train_time:32608ms step_avg:123.05ms
step:276/1395 train_time:32731ms step_avg:123.05ms
step:277/1395 train_time:32855ms step_avg:123.05ms
step:278/1395 train_time:32979ms step_avg:123.06ms
step:279/1395 train_time:33102ms step_avg:123.06ms
step:280/1395 train_time:33225ms step_avg:123.06ms
step:281/1395 train_time:33348ms step_avg:123.06ms
step:282/1395 train_time:33472ms step_avg:123.06ms
step:283/1395 train_time:33594ms step_avg:123.06ms
step:284/1395 train_time:33718ms step_avg:123.06ms
step:285/1395 train_time:33841ms step_avg:123.06ms
step:286/1395 train_time:33965ms step_avg:123.06ms
step:287/1395 train_time:34088ms step_avg:123.06ms
step:288/1395 train_time:34211ms step_avg:123.06ms
step:289/1395 train_time:34335ms step_avg:123.06ms
step:290/1395 train_time:34459ms step_avg:123.07ms
step:291/1395 train_time:34582ms step_avg:123.07ms
step:292/1395 train_time:34705ms step_avg:123.07ms
step:293/1395 train_time:34829ms step_avg:123.07ms
step:294/1395 train_time:34952ms step_avg:123.07ms
step:295/1395 train_time:35077ms step_avg:123.08ms
step:296/1395 train_time:35200ms step_avg:123.08ms
step:297/1395 train_time:35323ms step_avg:123.08ms
step:298/1395 train_time:35446ms step_avg:123.08ms
step:299/1395 train_time:35569ms step_avg:123.08ms
step:300/1395 train_time:35691ms step_avg:123.07ms
step:301/1395 train_time:35815ms step_avg:123.08ms
step:302/1395 train_time:35938ms step_avg:123.08ms
step:303/1395 train_time:36061ms step_avg:123.07ms
step:304/1395 train_time:36185ms step_avg:123.08ms
step:305/1395 train_time:36309ms step_avg:123.08ms
step:306/1395 train_time:36433ms step_avg:123.08ms
step:307/1395 train_time:36557ms step_avg:123.09ms
step:308/1395 train_time:36679ms step_avg:123.08ms
step:309/1395 train_time:36803ms step_avg:123.09ms
step:310/1395 train_time:36926ms step_avg:123.09ms
step:311/1395 train_time:37049ms step_avg:123.09ms
step:312/1395 train_time:37173ms step_avg:123.09ms
step:313/1395 train_time:37300ms step_avg:123.10ms
step:314/1395 train_time:37426ms step_avg:123.11ms
step:315/1395 train_time:37551ms step_avg:123.12ms
step:316/1395 train_time:37677ms step_avg:123.13ms
step:317/1395 train_time:37803ms step_avg:123.14ms
step:318/1395 train_time:37930ms step_avg:123.15ms
step:319/1395 train_time:38056ms step_avg:123.16ms
step:320/1395 train_time:38181ms step_avg:123.16ms
step:321/1395 train_time:38307ms step_avg:123.17ms
step:322/1395 train_time:38431ms step_avg:123.18ms
step:323/1395 train_time:38558ms step_avg:123.19ms
step:324/1395 train_time:38683ms step_avg:123.20ms
step:325/1395 train_time:38809ms step_avg:123.20ms
step:326/1395 train_time:38935ms step_avg:123.21ms
step:327/1395 train_time:39061ms step_avg:123.22ms
step:328/1395 train_time:39187ms step_avg:123.23ms
step:329/1395 train_time:39314ms step_avg:123.24ms
step:330/1395 train_time:39439ms step_avg:123.25ms
step:331/1395 train_time:39564ms step_avg:123.25ms
step:332/1395 train_time:39689ms step_avg:123.26ms
step:333/1395 train_time:39815ms step_avg:123.27ms
step:334/1395 train_time:39940ms step_avg:123.27ms
step:335/1395 train_time:40067ms step_avg:123.28ms
step:336/1395 train_time:40193ms step_avg:123.29ms
step:337/1395 train_time:40320ms step_avg:123.30ms
step:338/1395 train_time:40446ms step_avg:123.31ms
step:339/1395 train_time:40572ms step_avg:123.32ms
step:340/1395 train_time:40697ms step_avg:123.33ms
step:341/1395 train_time:40823ms step_avg:123.33ms
step:342/1395 train_time:40949ms step_avg:123.34ms
step:343/1395 train_time:41075ms step_avg:123.35ms
step:344/1395 train_time:41200ms step_avg:123.35ms
step:345/1395 train_time:41327ms step_avg:123.36ms
step:346/1395 train_time:41453ms step_avg:123.37ms
step:347/1395 train_time:41580ms step_avg:123.38ms
step:348/1395 train_time:41706ms step_avg:123.39ms
step:349/1395 train_time:41832ms step_avg:123.40ms
step:350/1395 train_time:41957ms step_avg:123.40ms
step:351/1395 train_time:42083ms step_avg:123.41ms
step:352/1395 train_time:42208ms step_avg:123.42ms
step:353/1395 train_time:42334ms step_avg:123.42ms
step:354/1395 train_time:42459ms step_avg:123.43ms
step:355/1395 train_time:42585ms step_avg:123.43ms
step:356/1395 train_time:42711ms step_avg:123.44ms
step:357/1395 train_time:42836ms step_avg:123.45ms
step:358/1395 train_time:42962ms step_avg:123.45ms
step:359/1395 train_time:43088ms step_avg:123.46ms
step:360/1395 train_time:43214ms step_avg:123.47ms
step:361/1395 train_time:43340ms step_avg:123.47ms
step:362/1395 train_time:43465ms step_avg:123.48ms
step:363/1395 train_time:43591ms step_avg:123.49ms
step:364/1395 train_time:43718ms step_avg:123.50ms
step:365/1395 train_time:43844ms step_avg:123.50ms
step:366/1395 train_time:43969ms step_avg:123.51ms
step:367/1395 train_time:44096ms step_avg:123.52ms
step:368/1395 train_time:44222ms step_avg:123.52ms
step:369/1395 train_time:44347ms step_avg:123.53ms
step:370/1395 train_time:44473ms step_avg:123.54ms
step:371/1395 train_time:44599ms step_avg:123.54ms
step:372/1395 train_time:44724ms step_avg:123.55ms
step:373/1395 train_time:44850ms step_avg:123.55ms
step:374/1395 train_time:44976ms step_avg:123.56ms
step:375/1395 train_time:45101ms step_avg:123.56ms
step:375/1395 val_loss:3.7906 train_time:45225ms step_avg:123.90ms
step:376/1395 train_time:45252ms step_avg:123.64ms
step:377/1395 train_time:45365ms step_avg:123.61ms
step:378/1395 train_time:45493ms step_avg:123.62ms
step:379/1395 train_time:45619ms step_avg:123.63ms
step:380/1395 train_time:45745ms step_avg:123.63ms
step:381/1395 train_time:45871ms step_avg:123.64ms
step:382/1395 train_time:45995ms step_avg:123.64ms
step:383/1395 train_time:46121ms step_avg:123.65ms
step:384/1395 train_time:46246ms step_avg:123.65ms
step:385/1395 train_time:46373ms step_avg:123.66ms
step:386/1395 train_time:46500ms step_avg:123.67ms
step:387/1395 train_time:46626ms step_avg:123.68ms
step:388/1395 train_time:46752ms step_avg:123.68ms
step:389/1395 train_time:46879ms step_avg:123.69ms
step:390/1395 train_time:47005ms step_avg:123.70ms
step:391/1395 train_time:47132ms step_avg:123.71ms
step:392/1395 train_time:47261ms step_avg:123.72ms
step:393/1395 train_time:47385ms step_avg:123.72ms
step:394/1395 train_time:47511ms step_avg:123.73ms
step:395/1395 train_time:47636ms step_avg:123.73ms
step:396/1395 train_time:47762ms step_avg:123.74ms
step:397/1395 train_time:47889ms step_avg:123.74ms
step:398/1395 train_time:48016ms step_avg:123.75ms
step:399/1395 train_time:48141ms step_avg:123.76ms
step:400/1395 train_time:48267ms step_avg:123.76ms
step:401/1395 train_time:48392ms step_avg:123.77ms
step:402/1395 train_time:48519ms step_avg:123.77ms
step:403/1395 train_time:48644ms step_avg:123.78ms
step:404/1395 train_time:48770ms step_avg:123.78ms
step:405/1395 train_time:48896ms step_avg:123.79ms
step:406/1395 train_time:49022ms step_avg:123.79ms
step:407/1395 train_time:49148ms step_avg:123.80ms
step:408/1395 train_time:49275ms step_avg:123.81ms
step:409/1395 train_time:49402ms step_avg:123.82ms
step:410/1395 train_time:49528ms step_avg:123.82ms
step:411/1395 train_time:49654ms step_avg:123.83ms
step:412/1395 train_time:49781ms step_avg:123.83ms
step:413/1395 train_time:49907ms step_avg:123.84ms
step:414/1395 train_time:50032ms step_avg:123.84ms
step:415/1395 train_time:50158ms step_avg:123.85ms
step:416/1395 train_time:50284ms step_avg:123.85ms
step:417/1395 train_time:50410ms step_avg:123.86ms
step:418/1395 train_time:50537ms step_avg:123.86ms
step:419/1395 train_time:50662ms step_avg:123.87ms
step:420/1395 train_time:50789ms step_avg:123.87ms
step:421/1395 train_time:50914ms step_avg:123.88ms
step:422/1395 train_time:51042ms step_avg:123.89ms
step:423/1395 train_time:51168ms step_avg:123.89ms
step:424/1395 train_time:51294ms step_avg:123.90ms
step:425/1395 train_time:51421ms step_avg:123.91ms
step:426/1395 train_time:51548ms step_avg:123.91ms
step:427/1395 train_time:51674ms step_avg:123.92ms
step:428/1395 train_time:51801ms step_avg:123.93ms
step:429/1395 train_time:51926ms step_avg:123.93ms
step:430/1395 train_time:52052ms step_avg:123.93ms
step:431/1395 train_time:52178ms step_avg:123.94ms
step:432/1395 train_time:52305ms step_avg:123.94ms
step:433/1395 train_time:52431ms step_avg:123.95ms
step:434/1395 train_time:52557ms step_avg:123.95ms
step:435/1395 train_time:52682ms step_avg:123.96ms
step:436/1395 train_time:52809ms step_avg:123.97ms
step:437/1395 train_time:52935ms step_avg:123.97ms
step:438/1395 train_time:53062ms step_avg:123.98ms
step:439/1395 train_time:53188ms step_avg:123.98ms
step:440/1395 train_time:53315ms step_avg:123.99ms
step:441/1395 train_time:53441ms step_avg:123.99ms
step:442/1395 train_time:53567ms step_avg:124.00ms
step:443/1395 train_time:53693ms step_avg:124.00ms
step:444/1395 train_time:53819ms step_avg:124.01ms
step:445/1395 train_time:53946ms step_avg:124.01ms
step:446/1395 train_time:54072ms step_avg:124.02ms
step:447/1395 train_time:54198ms step_avg:124.02ms
step:448/1395 train_time:54325ms step_avg:124.03ms
step:449/1395 train_time:54451ms step_avg:124.03ms
step:450/1395 train_time:54578ms step_avg:124.04ms
step:451/1395 train_time:54704ms step_avg:124.05ms
step:452/1395 train_time:54830ms step_avg:124.05ms
step:453/1395 train_time:54957ms step_avg:124.06ms
step:454/1395 train_time:55083ms step_avg:124.06ms
step:455/1395 train_time:55210ms step_avg:124.07ms
step:456/1395 train_time:55337ms step_avg:124.07ms
step:457/1395 train_time:55463ms step_avg:124.08ms
step:458/1395 train_time:55590ms step_avg:124.08ms
step:459/1395 train_time:55717ms step_avg:124.09ms
step:460/1395 train_time:55843ms step_avg:124.09ms
step:461/1395 train_time:55969ms step_avg:124.10ms
step:462/1395 train_time:56096ms step_avg:124.11ms
step:463/1395 train_time:56222ms step_avg:124.11ms
step:464/1395 train_time:56348ms step_avg:124.12ms
step:465/1395 train_time:56475ms step_avg:124.12ms
step:466/1395 train_time:56602ms step_avg:124.13ms
step:467/1395 train_time:56729ms step_avg:124.13ms
step:468/1395 train_time:56856ms step_avg:124.14ms
step:469/1395 train_time:56983ms step_avg:124.15ms
step:470/1395 train_time:57109ms step_avg:124.15ms
step:471/1395 train_time:57235ms step_avg:124.15ms
step:472/1395 train_time:57360ms step_avg:124.16ms
step:473/1395 train_time:57487ms step_avg:124.16ms
step:474/1395 train_time:57614ms step_avg:124.17ms
step:475/1395 train_time:57740ms step_avg:124.17ms
step:476/1395 train_time:57867ms step_avg:124.18ms
step:477/1395 train_time:57993ms step_avg:124.18ms
step:478/1395 train_time:58120ms step_avg:124.19ms
step:479/1395 train_time:58246ms step_avg:124.19ms
step:480/1395 train_time:58373ms step_avg:124.20ms
step:481/1395 train_time:58499ms step_avg:124.20ms
step:482/1395 train_time:58626ms step_avg:124.21ms
step:483/1395 train_time:58752ms step_avg:124.21ms
step:484/1395 train_time:58879ms step_avg:124.22ms
step:485/1395 train_time:59005ms step_avg:124.22ms
step:486/1395 train_time:59131ms step_avg:124.22ms
step:487/1395 train_time:59258ms step_avg:124.23ms
step:488/1395 train_time:59384ms step_avg:124.23ms
step:489/1395 train_time:59510ms step_avg:124.24ms
step:490/1395 train_time:59637ms step_avg:124.24ms
step:491/1395 train_time:59763ms step_avg:124.25ms
step:492/1395 train_time:59890ms step_avg:124.25ms
step:493/1395 train_time:60018ms step_avg:124.26ms
step:494/1395 train_time:60144ms step_avg:124.26ms
step:495/1395 train_time:60270ms step_avg:124.27ms
step:496/1395 train_time:60396ms step_avg:124.27ms
step:497/1395 train_time:60522ms step_avg:124.28ms
step:498/1395 train_time:60649ms step_avg:124.28ms
step:499/1395 train_time:60774ms step_avg:124.28ms
step:500/1395 train_time:60901ms step_avg:124.29ms
step:500/1395 val_loss:3.6713 train_time:61027ms step_avg:124.54ms
step:501/1395 train_time:61054ms step_avg:124.35ms
step:502/1395 train_time:61165ms step_avg:124.32ms
step:503/1395 train_time:61294ms step_avg:124.33ms
step:504/1395 train_time:61420ms step_avg:124.33ms
step:505/1395 train_time:61547ms step_avg:124.34ms
step:506/1395 train_time:61672ms step_avg:124.34ms
step:507/1395 train_time:61798ms step_avg:124.34ms
step:508/1395 train_time:61923ms step_avg:124.34ms
step:509/1395 train_time:62050ms step_avg:124.35ms
step:510/1395 train_time:62177ms step_avg:124.35ms
step:511/1395 train_time:62303ms step_avg:124.36ms
step:512/1395 train_time:62430ms step_avg:124.36ms
step:513/1395 train_time:62556ms step_avg:124.37ms
step:514/1395 train_time:62682ms step_avg:124.37ms
step:515/1395 train_time:62809ms step_avg:124.37ms
step:516/1395 train_time:62934ms step_avg:124.38ms
step:517/1395 train_time:63060ms step_avg:124.38ms
step:518/1395 train_time:63187ms step_avg:124.38ms
step:519/1395 train_time:63317ms step_avg:124.39ms
step:520/1395 train_time:63445ms step_avg:124.40ms
step:521/1395 train_time:63573ms step_avg:124.41ms
step:522/1395 train_time:63702ms step_avg:124.42ms
step:523/1395 train_time:63830ms step_avg:124.43ms
step:524/1395 train_time:63958ms step_avg:124.43ms
step:525/1395 train_time:64088ms step_avg:124.44ms
step:526/1395 train_time:64216ms step_avg:124.45ms
step:527/1395 train_time:64344ms step_avg:124.46ms
step:528/1395 train_time:64473ms step_avg:124.47ms
step:529/1395 train_time:64601ms step_avg:124.47ms
step:530/1395 train_time:64730ms step_avg:124.48ms
step:531/1395 train_time:64858ms step_avg:124.49ms
step:532/1395 train_time:64987ms step_avg:124.50ms
step:533/1395 train_time:65116ms step_avg:124.51ms
step:534/1395 train_time:65244ms step_avg:124.51ms
step:535/1395 train_time:65373ms step_avg:124.52ms
step:536/1395 train_time:65502ms step_avg:124.53ms
step:537/1395 train_time:65631ms step_avg:124.54ms
step:538/1395 train_time:65759ms step_avg:124.54ms
step:539/1395 train_time:65887ms step_avg:124.55ms
step:540/1395 train_time:66015ms step_avg:124.56ms
step:541/1395 train_time:66143ms step_avg:124.56ms
step:542/1395 train_time:66272ms step_avg:124.57ms
step:543/1395 train_time:66400ms step_avg:124.58ms
step:544/1395 train_time:66529ms step_avg:124.59ms
step:545/1395 train_time:66658ms step_avg:124.59ms
step:546/1395 train_time:66786ms step_avg:124.60ms
step:547/1395 train_time:66915ms step_avg:124.61ms
step:548/1395 train_time:67043ms step_avg:124.62ms
step:549/1395 train_time:67172ms step_avg:124.62ms
step:550/1395 train_time:67302ms step_avg:124.63ms
step:551/1395 train_time:67431ms step_avg:124.64ms
step:552/1395 train_time:67560ms step_avg:124.65ms
step:553/1395 train_time:67688ms step_avg:124.66ms
step:554/1395 train_time:67816ms step_avg:124.66ms
step:555/1395 train_time:67944ms step_avg:124.67ms
step:556/1395 train_time:68073ms step_avg:124.68ms
step:557/1395 train_time:68202ms step_avg:124.68ms
step:558/1395 train_time:68331ms step_avg:124.69ms
step:559/1395 train_time:68459ms step_avg:124.70ms
step:560/1395 train_time:68588ms step_avg:124.70ms
step:561/1395 train_time:68716ms step_avg:124.71ms
step:562/1395 train_time:68845ms step_avg:124.72ms
step:563/1395 train_time:68973ms step_avg:124.73ms
step:564/1395 train_time:69101ms step_avg:124.73ms
step:565/1395 train_time:69230ms step_avg:124.74ms
step:566/1395 train_time:69358ms step_avg:124.74ms
step:567/1395 train_time:69485ms step_avg:124.75ms
step:568/1395 train_time:69614ms step_avg:124.76ms
step:569/1395 train_time:69744ms step_avg:124.77ms
step:570/1395 train_time:69872ms step_avg:124.77ms
step:571/1395 train_time:70001ms step_avg:124.78ms
step:572/1395 train_time:70129ms step_avg:124.78ms
step:573/1395 train_time:70258ms step_avg:124.79ms
step:574/1395 train_time:70387ms step_avg:124.80ms
step:575/1395 train_time:70515ms step_avg:124.81ms
step:576/1395 train_time:70643ms step_avg:124.81ms
step:577/1395 train_time:70772ms step_avg:124.82ms
step:578/1395 train_time:70901ms step_avg:124.83ms
step:579/1395 train_time:71031ms step_avg:124.83ms
step:580/1395 train_time:71159ms step_avg:124.84ms
step:581/1395 train_time:71288ms step_avg:124.85ms
step:582/1395 train_time:71417ms step_avg:124.86ms
step:583/1395 train_time:71545ms step_avg:124.86ms
step:584/1395 train_time:71673ms step_avg:124.87ms
step:585/1395 train_time:71803ms step_avg:124.87ms
step:586/1395 train_time:71932ms step_avg:124.88ms
step:587/1395 train_time:72061ms step_avg:124.89ms
step:588/1395 train_time:72189ms step_avg:124.89ms
step:589/1395 train_time:72317ms step_avg:124.90ms
step:590/1395 train_time:72445ms step_avg:124.90ms
step:591/1395 train_time:72573ms step_avg:124.91ms
step:592/1395 train_time:72701ms step_avg:124.92ms
step:593/1395 train_time:72830ms step_avg:124.92ms
step:594/1395 train_time:72958ms step_avg:124.93ms
step:595/1395 train_time:73087ms step_avg:124.93ms
step:596/1395 train_time:73215ms step_avg:124.94ms
step:597/1395 train_time:73344ms step_avg:124.95ms
step:598/1395 train_time:73473ms step_avg:124.95ms
step:599/1395 train_time:73601ms step_avg:124.96ms
step:600/1395 train_time:73732ms step_avg:124.97ms
step:601/1395 train_time:73862ms step_avg:124.98ms
step:602/1395 train_time:73989ms step_avg:124.98ms
step:603/1395 train_time:74117ms step_avg:124.99ms
step:604/1395 train_time:74246ms step_avg:124.99ms
step:605/1395 train_time:74374ms step_avg:125.00ms
step:606/1395 train_time:74502ms step_avg:125.00ms
step:607/1395 train_time:74630ms step_avg:125.01ms
step:608/1395 train_time:74759ms step_avg:125.01ms
step:609/1395 train_time:74888ms step_avg:125.02ms
step:610/1395 train_time:75017ms step_avg:125.03ms
step:611/1395 train_time:75145ms step_avg:125.03ms
step:612/1395 train_time:75274ms step_avg:125.04ms
step:613/1395 train_time:75402ms step_avg:125.04ms
step:614/1395 train_time:75531ms step_avg:125.05ms
step:615/1395 train_time:75659ms step_avg:125.06ms
step:616/1395 train_time:75787ms step_avg:125.06ms
step:617/1395 train_time:75916ms step_avg:125.07ms
step:618/1395 train_time:76044ms step_avg:125.07ms
step:619/1395 train_time:76173ms step_avg:125.08ms
step:620/1395 train_time:76302ms step_avg:125.09ms
step:621/1395 train_time:76431ms step_avg:125.09ms
step:622/1395 train_time:76560ms step_avg:125.10ms
step:623/1395 train_time:76689ms step_avg:125.10ms
step:624/1395 train_time:76819ms step_avg:125.11ms
step:625/1395 train_time:76949ms step_avg:125.12ms
step:625/1395 val_loss:3.5891 train_time:77076ms step_avg:125.33ms
step:626/1395 train_time:77097ms step_avg:125.16ms
step:627/1395 train_time:77218ms step_avg:125.15ms
step:628/1395 train_time:77347ms step_avg:125.16ms
step:629/1395 train_time:77475ms step_avg:125.16ms
step:630/1395 train_time:77603ms step_avg:125.17ms
step:631/1395 train_time:77731ms step_avg:125.17ms
step:632/1395 train_time:77859ms step_avg:125.17ms
step:633/1395 train_time:77987ms step_avg:125.18ms
step:634/1395 train_time:78117ms step_avg:125.19ms
step:635/1395 train_time:78248ms step_avg:125.20ms
step:636/1395 train_time:78376ms step_avg:125.20ms
step:637/1395 train_time:78505ms step_avg:125.21ms
step:638/1395 train_time:78634ms step_avg:125.21ms
step:639/1395 train_time:78761ms step_avg:125.22ms
step:640/1395 train_time:78889ms step_avg:125.22ms
step:641/1395 train_time:79018ms step_avg:125.23ms
step:642/1395 train_time:79148ms step_avg:125.23ms
step:643/1395 train_time:79279ms step_avg:125.24ms
step:644/1395 train_time:79408ms step_avg:125.25ms
step:645/1395 train_time:79537ms step_avg:125.26ms
step:646/1395 train_time:79666ms step_avg:125.26ms
step:647/1395 train_time:79794ms step_avg:125.27ms
step:648/1395 train_time:79924ms step_avg:125.27ms
step:649/1395 train_time:80052ms step_avg:125.28ms
step:650/1395 train_time:80182ms step_avg:125.28ms
step:651/1395 train_time:80310ms step_avg:125.29ms
step:652/1395 train_time:80440ms step_avg:125.30ms
step:653/1395 train_time:80569ms step_avg:125.30ms
step:654/1395 train_time:80697ms step_avg:125.31ms
step:655/1395 train_time:80826ms step_avg:125.31ms
step:656/1395 train_time:80955ms step_avg:125.32ms
step:657/1395 train_time:81085ms step_avg:125.32ms
step:658/1395 train_time:81214ms step_avg:125.33ms
step:659/1395 train_time:81342ms step_avg:125.33ms
step:660/1395 train_time:81471ms step_avg:125.34ms
step:661/1395 train_time:81601ms step_avg:125.35ms
step:662/1395 train_time:81730ms step_avg:125.35ms
step:663/1395 train_time:81860ms step_avg:125.36ms
step:664/1395 train_time:81989ms step_avg:125.37ms
step:665/1395 train_time:82120ms step_avg:125.37ms
step:666/1395 train_time:82248ms step_avg:125.38ms
step:667/1395 train_time:82377ms step_avg:125.38ms
step:668/1395 train_time:82506ms step_avg:125.39ms
step:669/1395 train_time:82636ms step_avg:125.40ms
step:670/1395 train_time:82765ms step_avg:125.40ms
step:671/1395 train_time:82893ms step_avg:125.41ms
step:672/1395 train_time:83022ms step_avg:125.41ms
step:673/1395 train_time:83151ms step_avg:125.42ms
step:674/1395 train_time:83281ms step_avg:125.42ms
step:675/1395 train_time:83410ms step_avg:125.43ms
step:676/1395 train_time:83540ms step_avg:125.44ms
step:677/1395 train_time:83669ms step_avg:125.44ms
step:678/1395 train_time:83797ms step_avg:125.44ms
step:679/1395 train_time:83926ms step_avg:125.45ms
step:680/1395 train_time:84055ms step_avg:125.45ms
step:681/1395 train_time:84185ms step_avg:125.46ms
step:682/1395 train_time:84314ms step_avg:125.47ms
step:683/1395 train_time:84442ms step_avg:125.47ms
step:684/1395 train_time:84571ms step_avg:125.48ms
step:685/1395 train_time:84700ms step_avg:125.48ms
step:686/1395 train_time:84828ms step_avg:125.49ms
step:687/1395 train_time:84957ms step_avg:125.49ms
step:688/1395 train_time:85087ms step_avg:125.50ms
step:689/1395 train_time:85216ms step_avg:125.50ms
step:690/1395 train_time:85346ms step_avg:125.51ms
step:691/1395 train_time:85474ms step_avg:125.51ms
step:692/1395 train_time:85603ms step_avg:125.52ms
step:693/1395 train_time:85732ms step_avg:125.52ms
step:694/1395 train_time:85861ms step_avg:125.53ms
step:695/1395 train_time:85990ms step_avg:125.53ms
step:696/1395 train_time:86119ms step_avg:125.54ms
step:697/1395 train_time:86247ms step_avg:125.54ms
step:698/1395 train_time:86377ms step_avg:125.55ms
step:699/1395 train_time:86506ms step_avg:125.55ms
step:700/1395 train_time:86636ms step_avg:125.56ms
step:701/1395 train_time:86764ms step_avg:125.56ms
step:702/1395 train_time:86893ms step_avg:125.57ms
step:703/1395 train_time:87023ms step_avg:125.57ms
step:704/1395 train_time:87152ms step_avg:125.58ms
step:705/1395 train_time:87281ms step_avg:125.58ms
step:706/1395 train_time:87410ms step_avg:125.59ms
step:707/1395 train_time:87539ms step_avg:125.59ms
step:708/1395 train_time:87668ms step_avg:125.60ms
step:709/1395 train_time:87798ms step_avg:125.61ms
step:710/1395 train_time:87928ms step_avg:125.61ms
step:711/1395 train_time:88058ms step_avg:125.62ms
step:712/1395 train_time:88187ms step_avg:125.62ms
step:713/1395 train_time:88316ms step_avg:125.63ms
step:714/1395 train_time:88444ms step_avg:125.63ms
step:715/1395 train_time:88573ms step_avg:125.64ms
step:716/1395 train_time:88702ms step_avg:125.64ms
step:717/1395 train_time:88832ms step_avg:125.65ms
step:718/1395 train_time:88961ms step_avg:125.65ms
step:719/1395 train_time:89091ms step_avg:125.66ms
step:720/1395 train_time:89220ms step_avg:125.66ms
step:721/1395 train_time:89349ms step_avg:125.67ms
step:722/1395 train_time:89478ms step_avg:125.67ms
step:723/1395 train_time:89607ms step_avg:125.68ms
step:724/1395 train_time:89735ms step_avg:125.68ms
step:725/1395 train_time:89864ms step_avg:125.68ms
step:726/1395 train_time:89994ms step_avg:125.69ms
step:727/1395 train_time:90126ms step_avg:125.70ms
step:728/1395 train_time:90258ms step_avg:125.71ms
step:729/1395 train_time:90389ms step_avg:125.71ms
step:730/1395 train_time:90520ms step_avg:125.72ms
step:731/1395 train_time:90651ms step_avg:125.73ms
step:732/1395 train_time:90782ms step_avg:125.74ms
step:733/1395 train_time:90912ms step_avg:125.74ms
step:734/1395 train_time:91043ms step_avg:125.75ms
step:735/1395 train_time:91174ms step_avg:125.76ms
step:736/1395 train_time:91304ms step_avg:125.76ms
step:737/1395 train_time:91434ms step_avg:125.77ms
step:738/1395 train_time:91564ms step_avg:125.78ms
step:739/1395 train_time:91695ms step_avg:125.78ms
step:740/1395 train_time:91826ms step_avg:125.79ms
step:741/1395 train_time:91959ms step_avg:125.80ms
step:742/1395 train_time:92089ms step_avg:125.80ms
step:743/1395 train_time:92220ms step_avg:125.81ms
step:744/1395 train_time:92350ms step_avg:125.82ms
step:745/1395 train_time:92481ms step_avg:125.83ms
step:746/1395 train_time:92613ms step_avg:125.83ms
step:747/1395 train_time:92744ms step_avg:125.84ms
step:748/1395 train_time:92875ms step_avg:125.85ms
step:749/1395 train_time:93006ms step_avg:125.85ms
step:750/1395 train_time:93137ms step_avg:125.86ms
step:750/1395 val_loss:3.5316 train_time:93266ms step_avg:126.04ms
step:751/1395 train_time:93287ms step_avg:125.89ms
step:752/1395 train_time:93410ms step_avg:125.89ms
step:753/1395 train_time:93540ms step_avg:125.90ms
step:754/1395 train_time:93670ms step_avg:125.90ms
step:755/1395 train_time:93800ms step_avg:125.91ms
step:756/1395 train_time:93929ms step_avg:125.91ms
step:757/1395 train_time:94060ms step_avg:125.92ms
step:758/1395 train_time:94191ms step_avg:125.92ms
step:759/1395 train_time:94322ms step_avg:125.93ms
step:760/1395 train_time:94454ms step_avg:125.94ms
step:761/1395 train_time:94584ms step_avg:125.94ms
step:762/1395 train_time:94716ms step_avg:125.95ms
step:763/1395 train_time:94847ms step_avg:125.96ms
step:764/1395 train_time:94977ms step_avg:125.96ms
step:765/1395 train_time:95108ms step_avg:125.97ms
step:766/1395 train_time:95238ms step_avg:125.98ms
step:767/1395 train_time:95370ms step_avg:125.98ms
step:768/1395 train_time:95500ms step_avg:125.99ms
step:769/1395 train_time:95631ms step_avg:126.00ms
step:770/1395 train_time:95762ms step_avg:126.00ms
step:771/1395 train_time:95894ms step_avg:126.01ms
step:772/1395 train_time:96025ms step_avg:126.02ms
step:773/1395 train_time:96155ms step_avg:126.02ms
step:774/1395 train_time:96285ms step_avg:126.03ms
step:775/1395 train_time:96417ms step_avg:126.03ms
step:776/1395 train_time:96547ms step_avg:126.04ms
step:777/1395 train_time:96678ms step_avg:126.05ms
step:778/1395 train_time:96810ms step_avg:126.05ms
step:779/1395 train_time:96940ms step_avg:126.06ms
step:780/1395 train_time:97072ms step_avg:126.07ms
step:781/1395 train_time:97202ms step_avg:126.07ms
step:782/1395 train_time:97333ms step_avg:126.08ms
step:783/1395 train_time:97464ms step_avg:126.09ms
step:784/1395 train_time:97594ms step_avg:126.09ms
step:785/1395 train_time:97724ms step_avg:126.10ms
step:786/1395 train_time:97856ms step_avg:126.10ms
step:787/1395 train_time:97987ms step_avg:126.11ms
step:788/1395 train_time:98118ms step_avg:126.12ms
step:789/1395 train_time:98248ms step_avg:126.12ms
step:790/1395 train_time:98378ms step_avg:126.13ms
step:791/1395 train_time:98508ms step_avg:126.13ms
step:792/1395 train_time:98638ms step_avg:126.14ms
step:793/1395 train_time:98770ms step_avg:126.14ms
step:794/1395 train_time:98901ms step_avg:126.15ms
step:795/1395 train_time:99032ms step_avg:126.16ms
step:796/1395 train_time:99164ms step_avg:126.16ms
step:797/1395 train_time:99293ms step_avg:126.17ms
step:798/1395 train_time:99424ms step_avg:126.17ms
step:799/1395 train_time:99556ms step_avg:126.18ms
step:800/1395 train_time:99686ms step_avg:126.19ms
step:801/1395 train_time:99817ms step_avg:126.19ms
step:802/1395 train_time:99947ms step_avg:126.20ms
step:803/1395 train_time:100077ms step_avg:126.20ms
step:804/1395 train_time:100209ms step_avg:126.21ms
step:805/1395 train_time:100341ms step_avg:126.21ms
step:806/1395 train_time:100471ms step_avg:126.22ms
step:807/1395 train_time:100601ms step_avg:126.23ms
step:808/1395 train_time:100733ms step_avg:126.23ms
step:809/1395 train_time:100863ms step_avg:126.24ms
step:810/1395 train_time:100994ms step_avg:126.24ms
step:811/1395 train_time:101125ms step_avg:126.25ms
step:812/1395 train_time:101256ms step_avg:126.25ms
step:813/1395 train_time:101386ms step_avg:126.26ms
step:814/1395 train_time:101517ms step_avg:126.26ms
step:815/1395 train_time:101647ms step_avg:126.27ms
step:816/1395 train_time:101778ms step_avg:126.28ms
step:817/1395 train_time:101909ms step_avg:126.28ms
step:818/1395 train_time:102040ms step_avg:126.29ms
step:819/1395 train_time:102172ms step_avg:126.29ms
step:820/1395 train_time:102302ms step_avg:126.30ms
step:821/1395 train_time:102432ms step_avg:126.30ms
step:822/1395 train_time:102563ms step_avg:126.31ms
step:823/1395 train_time:102694ms step_avg:126.32ms
step:824/1395 train_time:102825ms step_avg:126.32ms
step:825/1395 train_time:102955ms step_avg:126.33ms
step:826/1395 train_time:103085ms step_avg:126.33ms
step:827/1395 train_time:103217ms step_avg:126.34ms
step:828/1395 train_time:103348ms step_avg:126.34ms
step:829/1395 train_time:103479ms step_avg:126.35ms
step:830/1395 train_time:103610ms step_avg:126.35ms
step:831/1395 train_time:103741ms step_avg:126.36ms
step:832/1395 train_time:103872ms step_avg:126.37ms
step:833/1395 train_time:104004ms step_avg:126.37ms
step:834/1395 train_time:104135ms step_avg:126.38ms
step:835/1395 train_time:104266ms step_avg:126.38ms
step:836/1395 train_time:104398ms step_avg:126.39ms
step:837/1395 train_time:104529ms step_avg:126.40ms
step:838/1395 train_time:104660ms step_avg:126.40ms
step:839/1395 train_time:104792ms step_avg:126.41ms
step:840/1395 train_time:104923ms step_avg:126.41ms
step:841/1395 train_time:105055ms step_avg:126.42ms
step:842/1395 train_time:105186ms step_avg:126.43ms
step:843/1395 train_time:105318ms step_avg:126.43ms
step:844/1395 train_time:105449ms step_avg:126.44ms
step:845/1395 train_time:105579ms step_avg:126.44ms
step:846/1395 train_time:105711ms step_avg:126.45ms
step:847/1395 train_time:105841ms step_avg:126.45ms
step:848/1395 train_time:105972ms step_avg:126.46ms
step:849/1395 train_time:106103ms step_avg:126.46ms
step:850/1395 train_time:106235ms step_avg:126.47ms
step:851/1395 train_time:106366ms step_avg:126.48ms
step:852/1395 train_time:106497ms step_avg:126.48ms
step:853/1395 train_time:106628ms step_avg:126.49ms
step:854/1395 train_time:106758ms step_avg:126.49ms
step:855/1395 train_time:106889ms step_avg:126.50ms
step:856/1395 train_time:107019ms step_avg:126.50ms
step:857/1395 train_time:107151ms step_avg:126.51ms
step:858/1395 train_time:107282ms step_avg:126.51ms
step:859/1395 train_time:107413ms step_avg:126.52ms
step:860/1395 train_time:107545ms step_avg:126.52ms
step:861/1395 train_time:107675ms step_avg:126.53ms
step:862/1395 train_time:107808ms step_avg:126.53ms
step:863/1395 train_time:107939ms step_avg:126.54ms
step:864/1395 train_time:108070ms step_avg:126.55ms
step:865/1395 train_time:108200ms step_avg:126.55ms
step:866/1395 train_time:108333ms step_avg:126.56ms
step:867/1395 train_time:108466ms step_avg:126.56ms
step:868/1395 train_time:108596ms step_avg:126.57ms
step:869/1395 train_time:108727ms step_avg:126.57ms
step:870/1395 train_time:108859ms step_avg:126.58ms
step:871/1395 train_time:108990ms step_avg:126.59ms
step:872/1395 train_time:109120ms step_avg:126.59ms
step:873/1395 train_time:109250ms step_avg:126.59ms
step:874/1395 train_time:109380ms step_avg:126.60ms
step:875/1395 train_time:109512ms step_avg:126.60ms
step:875/1395 val_loss:3.4828 train_time:109642ms step_avg:126.75ms
step:876/1395 train_time:109665ms step_avg:126.63ms
step:877/1395 train_time:109784ms step_avg:126.63ms
step:878/1395 train_time:109918ms step_avg:126.63ms
step:879/1395 train_time:110048ms step_avg:126.64ms
step:880/1395 train_time:110179ms step_avg:126.64ms
step:881/1395 train_time:110309ms step_avg:126.65ms
step:882/1395 train_time:110439ms step_avg:126.65ms
step:883/1395 train_time:110570ms step_avg:126.65ms
step:884/1395 train_time:110701ms step_avg:126.66ms
step:885/1395 train_time:110833ms step_avg:126.67ms
step:886/1395 train_time:110965ms step_avg:126.67ms
step:887/1395 train_time:111095ms step_avg:126.68ms
step:888/1395 train_time:111226ms step_avg:126.68ms
step:889/1395 train_time:111358ms step_avg:126.69ms
step:890/1395 train_time:111488ms step_avg:126.69ms
step:891/1395 train_time:111618ms step_avg:126.69ms
step:892/1395 train_time:111749ms step_avg:126.70ms
step:893/1395 train_time:111880ms step_avg:126.70ms
step:894/1395 train_time:112011ms step_avg:126.71ms
step:895/1395 train_time:112142ms step_avg:126.71ms
step:896/1395 train_time:112274ms step_avg:126.72ms
step:897/1395 train_time:112405ms step_avg:126.72ms
step:898/1395 train_time:112536ms step_avg:126.73ms
step:899/1395 train_time:112668ms step_avg:126.74ms
step:900/1395 train_time:112798ms step_avg:126.74ms
step:901/1395 train_time:112930ms step_avg:126.75ms
step:902/1395 train_time:113061ms step_avg:126.75ms
step:903/1395 train_time:113191ms step_avg:126.75ms
step:904/1395 train_time:113322ms step_avg:126.76ms
step:905/1395 train_time:113453ms step_avg:126.76ms
step:906/1395 train_time:113585ms step_avg:126.77ms
step:907/1395 train_time:113717ms step_avg:126.77ms
step:908/1395 train_time:113847ms step_avg:126.78ms
step:909/1395 train_time:113978ms step_avg:126.78ms
step:910/1395 train_time:114110ms step_avg:126.79ms
step:911/1395 train_time:114240ms step_avg:126.79ms
step:912/1395 train_time:114370ms step_avg:126.80ms
step:913/1395 train_time:114501ms step_avg:126.80ms
step:914/1395 train_time:114633ms step_avg:126.81ms
step:915/1395 train_time:114765ms step_avg:126.81ms
step:916/1395 train_time:114897ms step_avg:126.82ms
step:917/1395 train_time:115029ms step_avg:126.82ms
step:918/1395 train_time:115161ms step_avg:126.83ms
step:919/1395 train_time:115295ms step_avg:126.84ms
step:920/1395 train_time:115426ms step_avg:126.84ms
step:921/1395 train_time:115558ms step_avg:126.85ms
step:922/1395 train_time:115689ms step_avg:126.85ms
step:923/1395 train_time:115820ms step_avg:126.86ms
step:924/1395 train_time:115951ms step_avg:126.86ms
step:925/1395 train_time:116083ms step_avg:126.87ms
step:926/1395 train_time:116215ms step_avg:126.87ms
step:927/1395 train_time:116346ms step_avg:126.88ms
step:928/1395 train_time:116478ms step_avg:126.88ms
step:929/1395 train_time:116609ms step_avg:126.89ms
step:930/1395 train_time:116740ms step_avg:126.89ms
step:931/1395 train_time:116871ms step_avg:126.90ms
step:932/1395 train_time:117003ms step_avg:126.90ms
step:933/1395 train_time:117136ms step_avg:126.91ms
step:934/1395 train_time:117269ms step_avg:126.91ms
step:935/1395 train_time:117402ms step_avg:126.92ms
step:936/1395 train_time:117534ms step_avg:126.93ms
step:937/1395 train_time:117669ms step_avg:126.94ms
step:938/1395 train_time:117804ms step_avg:126.94ms
step:939/1395 train_time:117936ms step_avg:126.95ms
step:940/1395 train_time:118070ms step_avg:126.96ms
step:941/1395 train_time:118203ms step_avg:126.96ms
step:942/1395 train_time:118335ms step_avg:126.97ms
step:943/1395 train_time:118468ms step_avg:126.98ms
step:944/1395 train_time:118601ms step_avg:126.98ms
step:945/1395 train_time:118736ms step_avg:126.99ms
step:946/1395 train_time:118869ms step_avg:127.00ms
step:947/1395 train_time:119002ms step_avg:127.00ms
step:948/1395 train_time:119135ms step_avg:127.01ms
step:949/1395 train_time:119268ms step_avg:127.02ms
step:950/1395 train_time:119400ms step_avg:127.02ms
step:951/1395 train_time:119535ms step_avg:127.03ms
step:952/1395 train_time:119666ms step_avg:127.03ms
step:953/1395 train_time:119799ms step_avg:127.04ms
step:954/1395 train_time:119933ms step_avg:127.05ms
step:955/1395 train_time:120064ms step_avg:127.05ms
step:956/1395 train_time:120197ms step_avg:127.06ms
step:957/1395 train_time:120329ms step_avg:127.06ms
step:958/1395 train_time:120462ms step_avg:127.07ms
step:959/1395 train_time:120594ms step_avg:127.08ms
step:960/1395 train_time:120727ms step_avg:127.08ms
step:961/1395 train_time:120860ms step_avg:127.09ms
step:962/1395 train_time:120993ms step_avg:127.09ms
step:963/1395 train_time:121127ms step_avg:127.10ms
step:964/1395 train_time:121260ms step_avg:127.11ms
step:965/1395 train_time:121394ms step_avg:127.11ms
step:966/1395 train_time:121526ms step_avg:127.12ms
step:967/1395 train_time:121659ms step_avg:127.13ms
step:968/1395 train_time:121791ms step_avg:127.13ms
step:969/1395 train_time:121925ms step_avg:127.14ms
step:970/1395 train_time:122059ms step_avg:127.15ms
step:971/1395 train_time:122192ms step_avg:127.15ms
step:972/1395 train_time:122328ms step_avg:127.16ms
step:973/1395 train_time:122460ms step_avg:127.17ms
step:974/1395 train_time:122592ms step_avg:127.17ms
step:975/1395 train_time:122725ms step_avg:127.18ms
step:976/1395 train_time:122858ms step_avg:127.18ms
step:977/1395 train_time:122990ms step_avg:127.19ms
step:978/1395 train_time:123123ms step_avg:127.19ms
step:979/1395 train_time:123258ms step_avg:127.20ms
step:980/1395 train_time:123391ms step_avg:127.21ms
step:981/1395 train_time:123525ms step_avg:127.21ms
step:982/1395 train_time:123657ms step_avg:127.22ms
step:983/1395 train_time:123788ms step_avg:127.22ms
step:984/1395 train_time:123920ms step_avg:127.23ms
step:985/1395 train_time:124053ms step_avg:127.23ms
step:986/1395 train_time:124189ms step_avg:127.24ms
step:987/1395 train_time:124323ms step_avg:127.25ms
step:988/1395 train_time:124454ms step_avg:127.25ms
step:989/1395 train_time:124588ms step_avg:127.26ms
step:990/1395 train_time:124721ms step_avg:127.27ms
step:991/1395 train_time:124856ms step_avg:127.27ms
step:992/1395 train_time:124990ms step_avg:127.28ms
step:993/1395 train_time:125124ms step_avg:127.29ms
step:994/1395 train_time:125256ms step_avg:127.29ms
step:995/1395 train_time:125388ms step_avg:127.30ms
step:996/1395 train_time:125521ms step_avg:127.30ms
step:997/1395 train_time:125653ms step_avg:127.31ms
step:998/1395 train_time:125786ms step_avg:127.31ms
step:999/1395 train_time:125920ms step_avg:127.32ms
step:1000/1395 train_time:126052ms step_avg:127.33ms
step:1000/1395 val_loss:3.4189 train_time:126183ms step_avg:127.46ms
step:1001/1395 train_time:126205ms step_avg:127.35ms
step:1002/1395 train_time:126328ms step_avg:127.35ms
step:1003/1395 train_time:126464ms step_avg:127.36ms
step:1004/1395 train_time:126596ms step_avg:127.36ms
step:1005/1395 train_time:126728ms step_avg:127.36ms
step:1006/1395 train_time:126859ms step_avg:127.37ms
step:1007/1395 train_time:126992ms step_avg:127.37ms
step:1008/1395 train_time:127125ms step_avg:127.38ms
step:1009/1395 train_time:127260ms step_avg:127.39ms
step:1010/1395 train_time:127396ms step_avg:127.40ms
step:1011/1395 train_time:127527ms step_avg:127.40ms
step:1012/1395 train_time:127660ms step_avg:127.41ms
step:1013/1395 train_time:127792ms step_avg:127.41ms
step:1014/1395 train_time:127924ms step_avg:127.41ms
step:1015/1395 train_time:128056ms step_avg:127.42ms
step:1016/1395 train_time:128188ms step_avg:127.42ms
step:1017/1395 train_time:128323ms step_avg:127.43ms
step:1018/1395 train_time:128457ms step_avg:127.44ms
step:1019/1395 train_time:128590ms step_avg:127.44ms
step:1020/1395 train_time:128723ms step_avg:127.45ms
step:1021/1395 train_time:128856ms step_avg:127.45ms
step:1022/1395 train_time:128988ms step_avg:127.46ms
step:1023/1395 train_time:129121ms step_avg:127.46ms
step:1024/1395 train_time:129253ms step_avg:127.47ms
step:1025/1395 train_time:129387ms step_avg:127.47ms
step:1026/1395 train_time:129521ms step_avg:127.48ms
step:1027/1395 train_time:129654ms step_avg:127.49ms
step:1028/1395 train_time:129787ms step_avg:127.49ms
step:1029/1395 train_time:129922ms step_avg:127.50ms
step:1030/1395 train_time:130054ms step_avg:127.50ms
step:1031/1395 train_time:130186ms step_avg:127.51ms
step:1032/1395 train_time:130319ms step_avg:127.51ms
step:1033/1395 train_time:130452ms step_avg:127.52ms
step:1034/1395 train_time:130585ms step_avg:127.52ms
step:1035/1395 train_time:130719ms step_avg:127.53ms
step:1036/1395 train_time:130853ms step_avg:127.54ms
step:1037/1395 train_time:130986ms step_avg:127.54ms
step:1038/1395 train_time:131118ms step_avg:127.55ms
step:1039/1395 train_time:131251ms step_avg:127.55ms
step:1040/1395 train_time:131385ms step_avg:127.56ms
step:1041/1395 train_time:131519ms step_avg:127.56ms
step:1042/1395 train_time:131652ms step_avg:127.57ms
step:1043/1395 train_time:131786ms step_avg:127.58ms
step:1044/1395 train_time:131921ms step_avg:127.58ms
step:1045/1395 train_time:132054ms step_avg:127.59ms
step:1046/1395 train_time:132187ms step_avg:127.59ms
step:1047/1395 train_time:132319ms step_avg:127.60ms
step:1048/1395 train_time:132453ms step_avg:127.60ms
step:1049/1395 train_time:132587ms step_avg:127.61ms
step:1050/1395 train_time:132721ms step_avg:127.62ms
step:1051/1395 train_time:132855ms step_avg:127.62ms
step:1052/1395 train_time:132989ms step_avg:127.63ms
step:1053/1395 train_time:133122ms step_avg:127.63ms
step:1054/1395 train_time:133256ms step_avg:127.64ms
step:1055/1395 train_time:133389ms step_avg:127.64ms
step:1056/1395 train_time:133521ms step_avg:127.65ms
step:1057/1395 train_time:133656ms step_avg:127.66ms
step:1058/1395 train_time:133789ms step_avg:127.66ms
step:1059/1395 train_time:133923ms step_avg:127.67ms
step:1060/1395 train_time:134058ms step_avg:127.67ms
step:1061/1395 train_time:134191ms step_avg:127.68ms
step:1062/1395 train_time:134325ms step_avg:127.69ms
step:1063/1395 train_time:134457ms step_avg:127.69ms
step:1064/1395 train_time:134589ms step_avg:127.69ms
step:1065/1395 train_time:134722ms step_avg:127.70ms
step:1066/1395 train_time:134856ms step_avg:127.70ms
step:1067/1395 train_time:134989ms step_avg:127.71ms
step:1068/1395 train_time:135123ms step_avg:127.72ms
step:1069/1395 train_time:135258ms step_avg:127.72ms
step:1070/1395 train_time:135391ms step_avg:127.73ms
step:1071/1395 train_time:135525ms step_avg:127.73ms
step:1072/1395 train_time:135657ms step_avg:127.74ms
step:1073/1395 train_time:135790ms step_avg:127.74ms
step:1074/1395 train_time:135922ms step_avg:127.75ms
step:1075/1395 train_time:136055ms step_avg:127.75ms
step:1076/1395 train_time:136188ms step_avg:127.76ms
step:1077/1395 train_time:136321ms step_avg:127.76ms
step:1078/1395 train_time:136455ms step_avg:127.77ms
step:1079/1395 train_time:136592ms step_avg:127.78ms
step:1080/1395 train_time:136725ms step_avg:127.78ms
step:1081/1395 train_time:136858ms step_avg:127.79ms
step:1082/1395 train_time:136991ms step_avg:127.79ms
step:1083/1395 train_time:137123ms step_avg:127.79ms
step:1084/1395 train_time:137256ms step_avg:127.80ms
step:1085/1395 train_time:137389ms step_avg:127.80ms
step:1086/1395 train_time:137522ms step_avg:127.81ms
step:1087/1395 train_time:137656ms step_avg:127.81ms
step:1088/1395 train_time:137789ms step_avg:127.82ms
step:1089/1395 train_time:137924ms step_avg:127.83ms
step:1090/1395 train_time:138059ms step_avg:127.83ms
step:1091/1395 train_time:138192ms step_avg:127.84ms
step:1092/1395 train_time:138324ms step_avg:127.84ms
step:1093/1395 train_time:138457ms step_avg:127.85ms
step:1094/1395 train_time:138589ms step_avg:127.85ms
step:1095/1395 train_time:138722ms step_avg:127.85ms
step:1096/1395 train_time:138856ms step_avg:127.86ms
step:1097/1395 train_time:138989ms step_avg:127.87ms
step:1098/1395 train_time:139123ms step_avg:127.87ms
step:1099/1395 train_time:139256ms step_avg:127.88ms
step:1100/1395 train_time:139388ms step_avg:127.88ms
step:1101/1395 train_time:139522ms step_avg:127.88ms
step:1102/1395 train_time:139654ms step_avg:127.89ms
step:1103/1395 train_time:139788ms step_avg:127.89ms
step:1104/1395 train_time:139921ms step_avg:127.90ms
step:1105/1395 train_time:140054ms step_avg:127.90ms
step:1106/1395 train_time:140187ms step_avg:127.91ms
step:1107/1395 train_time:140320ms step_avg:127.91ms
step:1108/1395 train_time:140454ms step_avg:127.92ms
step:1109/1395 train_time:140586ms step_avg:127.92ms
step:1110/1395 train_time:140720ms step_avg:127.93ms
step:1111/1395 train_time:140853ms step_avg:127.93ms
step:1112/1395 train_time:140986ms step_avg:127.94ms
step:1113/1395 train_time:141119ms step_avg:127.94ms
step:1114/1395 train_time:141252ms step_avg:127.95ms
step:1115/1395 train_time:141386ms step_avg:127.95ms
step:1116/1395 train_time:141518ms step_avg:127.96ms
step:1117/1395 train_time:141653ms step_avg:127.96ms
step:1118/1395 train_time:141788ms step_avg:127.97ms
step:1119/1395 train_time:141920ms step_avg:127.97ms
step:1120/1395 train_time:142052ms step_avg:127.98ms
step:1121/1395 train_time:142186ms step_avg:127.98ms
step:1122/1395 train_time:142318ms step_avg:127.98ms
step:1123/1395 train_time:142450ms step_avg:127.99ms
step:1124/1395 train_time:142583ms step_avg:127.99ms
step:1125/1395 train_time:142715ms step_avg:128.00ms
step:1125/1395 val_loss:3.3678 train_time:142847ms step_avg:128.11ms
step:1126/1395 train_time:142871ms step_avg:128.02ms
step:1127/1395 train_time:142990ms step_avg:128.01ms
step:1128/1395 train_time:143123ms step_avg:128.02ms
step:1129/1395 train_time:143255ms step_avg:128.02ms
step:1130/1395 train_time:143387ms step_avg:128.02ms
step:1131/1395 train_time:143519ms step_avg:128.03ms
step:1132/1395 train_time:143652ms step_avg:128.03ms
step:1133/1395 train_time:143783ms step_avg:128.04ms
step:1134/1395 train_time:143919ms step_avg:128.04ms
step:1135/1395 train_time:144053ms step_avg:128.05ms
step:1136/1395 train_time:144188ms step_avg:128.05ms
step:1137/1395 train_time:144319ms step_avg:128.06ms
step:1138/1395 train_time:144452ms step_avg:128.06ms
step:1139/1395 train_time:144587ms step_avg:128.07ms
step:1140/1395 train_time:144722ms step_avg:128.07ms
step:1141/1395 train_time:144857ms step_avg:128.08ms
step:1142/1395 train_time:144990ms step_avg:128.08ms
step:1143/1395 train_time:145128ms step_avg:128.09ms
step:1144/1395 train_time:145262ms step_avg:128.10ms
step:1145/1395 train_time:145396ms step_avg:128.10ms
step:1146/1395 train_time:145530ms step_avg:128.11ms
step:1147/1395 train_time:145666ms step_avg:128.11ms
step:1148/1395 train_time:145801ms step_avg:128.12ms
step:1149/1395 train_time:145934ms step_avg:128.13ms
step:1150/1395 train_time:146069ms step_avg:128.13ms
step:1151/1395 train_time:146204ms step_avg:128.14ms
step:1152/1395 train_time:146338ms step_avg:128.14ms
step:1153/1395 train_time:146473ms step_avg:128.15ms
step:1154/1395 train_time:146607ms step_avg:128.15ms
step:1155/1395 train_time:146742ms step_avg:128.16ms
step:1156/1395 train_time:146880ms step_avg:128.17ms
step:1157/1395 train_time:147016ms step_avg:128.17ms
step:1158/1395 train_time:147151ms step_avg:128.18ms
step:1159/1395 train_time:147284ms step_avg:128.18ms
step:1160/1395 train_time:147418ms step_avg:128.19ms
step:1161/1395 train_time:147552ms step_avg:128.19ms
step:1162/1395 train_time:147687ms step_avg:128.20ms
step:1163/1395 train_time:147821ms step_avg:128.21ms
step:1164/1395 train_time:147956ms step_avg:128.21ms
step:1165/1395 train_time:148091ms step_avg:128.22ms
step:1166/1395 train_time:148224ms step_avg:128.22ms
step:1167/1395 train_time:148358ms step_avg:128.23ms
step:1168/1395 train_time:148492ms step_avg:128.23ms
step:1169/1395 train_time:148626ms step_avg:128.24ms
step:1170/1395 train_time:148760ms step_avg:128.24ms
step:1171/1395 train_time:148894ms step_avg:128.25ms
step:1172/1395 train_time:149030ms step_avg:128.25ms
step:1173/1395 train_time:149164ms step_avg:128.26ms
step:1174/1395 train_time:149304ms step_avg:128.27ms
step:1175/1395 train_time:149438ms step_avg:128.27ms
step:1176/1395 train_time:149572ms step_avg:128.28ms
step:1177/1395 train_time:149709ms step_avg:128.29ms
step:1178/1395 train_time:149843ms step_avg:128.29ms
step:1179/1395 train_time:149977ms step_avg:128.29ms
step:1180/1395 train_time:150113ms step_avg:128.30ms
step:1181/1395 train_time:150250ms step_avg:128.31ms
step:1182/1395 train_time:150384ms step_avg:128.31ms
step:1183/1395 train_time:150518ms step_avg:128.32ms
step:1184/1395 train_time:150652ms step_avg:128.32ms
step:1185/1395 train_time:150787ms step_avg:128.33ms
step:1186/1395 train_time:150920ms step_avg:128.33ms
step:1187/1395 train_time:151059ms step_avg:128.34ms
step:1188/1395 train_time:151193ms step_avg:128.35ms
step:1189/1395 train_time:151327ms step_avg:128.35ms
step:1190/1395 train_time:151462ms step_avg:128.36ms
step:1191/1395 train_time:151597ms step_avg:128.36ms
step:1192/1395 train_time:151732ms step_avg:128.37ms
step:1193/1395 train_time:151865ms step_avg:128.37ms
step:1194/1395 train_time:151999ms step_avg:128.38ms
step:1195/1395 train_time:152133ms step_avg:128.38ms
step:1196/1395 train_time:152268ms step_avg:128.39ms
step:1197/1395 train_time:152403ms step_avg:128.39ms
step:1198/1395 train_time:152540ms step_avg:128.40ms
step:1199/1395 train_time:152674ms step_avg:128.41ms
step:1200/1395 train_time:152809ms step_avg:128.41ms
step:1201/1395 train_time:152942ms step_avg:128.41ms
step:1202/1395 train_time:153081ms step_avg:128.42ms
step:1203/1395 train_time:153218ms step_avg:128.43ms
step:1204/1395 train_time:153352ms step_avg:128.44ms
step:1205/1395 train_time:153488ms step_avg:128.44ms
step:1206/1395 train_time:153623ms step_avg:128.45ms
step:1207/1395 train_time:153756ms step_avg:128.45ms
step:1208/1395 train_time:153892ms step_avg:128.46ms
step:1209/1395 train_time:154025ms step_avg:128.46ms
step:1210/1395 train_time:154163ms step_avg:128.47ms
step:1211/1395 train_time:154298ms step_avg:128.47ms
step:1212/1395 train_time:154431ms step_avg:128.48ms
step:1213/1395 train_time:154566ms step_avg:128.48ms
step:1214/1395 train_time:154702ms step_avg:128.49ms
step:1215/1395 train_time:154838ms step_avg:128.50ms
step:1216/1395 train_time:154971ms step_avg:128.50ms
step:1217/1395 train_time:155105ms step_avg:128.50ms
step:1218/1395 train_time:155239ms step_avg:128.51ms
step:1219/1395 train_time:155373ms step_avg:128.51ms
step:1220/1395 train_time:155506ms step_avg:128.52ms
step:1221/1395 train_time:155640ms step_avg:128.52ms
step:1222/1395 train_time:155775ms step_avg:128.53ms
step:1223/1395 train_time:155910ms step_avg:128.53ms
step:1224/1395 train_time:156044ms step_avg:128.54ms
step:1225/1395 train_time:156182ms step_avg:128.54ms
step:1226/1395 train_time:156316ms step_avg:128.55ms
step:1227/1395 train_time:156450ms step_avg:128.55ms
step:1228/1395 train_time:156585ms step_avg:128.56ms
step:1229/1395 train_time:156719ms step_avg:128.56ms
step:1230/1395 train_time:156855ms step_avg:128.57ms
step:1231/1395 train_time:156993ms step_avg:128.58ms
step:1232/1395 train_time:157130ms step_avg:128.58ms
step:1233/1395 train_time:157263ms step_avg:128.59ms
step:1234/1395 train_time:157397ms step_avg:128.59ms
step:1235/1395 train_time:157531ms step_avg:128.60ms
step:1236/1395 train_time:157665ms step_avg:128.60ms
step:1237/1395 train_time:157799ms step_avg:128.61ms
step:1238/1395 train_time:157937ms step_avg:128.61ms
step:1239/1395 train_time:158070ms step_avg:128.62ms
step:1240/1395 train_time:158205ms step_avg:128.62ms
step:1241/1395 train_time:158341ms step_avg:128.63ms
step:1242/1395 train_time:158474ms step_avg:128.63ms
step:1243/1395 train_time:158609ms step_avg:128.64ms
step:1244/1395 train_time:158743ms step_avg:128.64ms
step:1245/1395 train_time:158879ms step_avg:128.65ms
step:1246/1395 train_time:159014ms step_avg:128.65ms
step:1247/1395 train_time:159149ms step_avg:128.66ms
step:1248/1395 train_time:159283ms step_avg:128.66ms
step:1249/1395 train_time:159416ms step_avg:128.66ms
step:1250/1395 train_time:159550ms step_avg:128.67ms
step:1250/1395 val_loss:3.3204 train_time:159684ms step_avg:128.78ms
step:1251/1395 train_time:159705ms step_avg:128.69ms
step:1252/1395 train_time:159830ms step_avg:128.69ms
step:1253/1395 train_time:159964ms step_avg:128.69ms
step:1254/1395 train_time:160098ms step_avg:128.70ms
step:1255/1395 train_time:160237ms step_avg:128.70ms
step:1256/1395 train_time:160371ms step_avg:128.71ms
step:1257/1395 train_time:160506ms step_avg:128.71ms
step:1258/1395 train_time:160639ms step_avg:128.72ms
step:1259/1395 train_time:160776ms step_avg:128.72ms
step:1260/1395 train_time:160910ms step_avg:128.73ms
step:1261/1395 train_time:161043ms step_avg:128.73ms
step:1262/1395 train_time:161179ms step_avg:128.74ms
step:1263/1395 train_time:161314ms step_avg:128.74ms
step:1264/1395 train_time:161448ms step_avg:128.75ms
step:1265/1395 train_time:161583ms step_avg:128.75ms
step:1266/1395 train_time:161717ms step_avg:128.76ms
step:1267/1395 train_time:161852ms step_avg:128.76ms
step:1268/1395 train_time:161987ms step_avg:128.77ms
step:1269/1395 train_time:162124ms step_avg:128.77ms
step:1270/1395 train_time:162258ms step_avg:128.78ms
step:1271/1395 train_time:162393ms step_avg:128.78ms
step:1272/1395 train_time:162527ms step_avg:128.79ms
step:1273/1395 train_time:162660ms step_avg:128.79ms
step:1274/1395 train_time:162793ms step_avg:128.79ms
step:1275/1395 train_time:162930ms step_avg:128.80ms
step:1276/1395 train_time:163063ms step_avg:128.80ms
step:1277/1395 train_time:163197ms step_avg:128.81ms
step:1278/1395 train_time:163331ms step_avg:128.81ms
step:1279/1395 train_time:163464ms step_avg:128.81ms
step:1280/1395 train_time:163599ms step_avg:128.82ms
step:1281/1395 train_time:163734ms step_avg:128.82ms
step:1282/1395 train_time:163868ms step_avg:128.83ms
step:1283/1395 train_time:164001ms step_avg:128.83ms
step:1284/1395 train_time:164138ms step_avg:128.84ms
step:1285/1395 train_time:164272ms step_avg:128.84ms
step:1286/1395 train_time:164406ms step_avg:128.84ms
step:1287/1395 train_time:164541ms step_avg:128.85ms
step:1288/1395 train_time:164674ms step_avg:128.85ms
step:1289/1395 train_time:164811ms step_avg:128.86ms
step:1290/1395 train_time:164947ms step_avg:128.87ms
step:1291/1395 train_time:165086ms step_avg:128.87ms
step:1292/1395 train_time:165220ms step_avg:128.88ms
step:1293/1395 train_time:165356ms step_avg:128.88ms
step:1294/1395 train_time:165489ms step_avg:128.89ms
step:1295/1395 train_time:165625ms step_avg:128.89ms
step:1296/1395 train_time:165759ms step_avg:128.90ms
step:1297/1395 train_time:165895ms step_avg:128.90ms
step:1298/1395 train_time:166029ms step_avg:128.90ms
step:1299/1395 train_time:166164ms step_avg:128.91ms
step:1300/1395 train_time:166299ms step_avg:128.91ms
step:1301/1395 train_time:166433ms step_avg:128.92ms
step:1302/1395 train_time:166567ms step_avg:128.92ms
step:1303/1395 train_time:166701ms step_avg:128.93ms
step:1304/1395 train_time:166837ms step_avg:128.93ms
step:1305/1395 train_time:166974ms step_avg:128.94ms
step:1306/1395 train_time:167108ms step_avg:128.94ms
step:1307/1395 train_time:167244ms step_avg:128.95ms
step:1308/1395 train_time:167377ms step_avg:128.95ms
step:1309/1395 train_time:167513ms step_avg:128.96ms
step:1310/1395 train_time:167648ms step_avg:128.96ms
step:1311/1395 train_time:167782ms step_avg:128.96ms
step:1312/1395 train_time:167915ms step_avg:128.97ms
step:1313/1395 train_time:168050ms step_avg:128.97ms
step:1314/1395 train_time:168184ms step_avg:128.98ms
step:1315/1395 train_time:168319ms step_avg:128.98ms
step:1316/1395 train_time:168453ms step_avg:128.98ms
step:1317/1395 train_time:168587ms step_avg:128.99ms
step:1318/1395 train_time:168722ms step_avg:128.99ms
step:1319/1395 train_time:168859ms step_avg:129.00ms
step:1320/1395 train_time:168994ms step_avg:129.00ms
step:1321/1395 train_time:169128ms step_avg:129.01ms
step:1322/1395 train_time:169266ms step_avg:129.01ms
step:1323/1395 train_time:169400ms step_avg:129.02ms
step:1324/1395 train_time:169533ms step_avg:129.02ms
step:1325/1395 train_time:169668ms step_avg:129.03ms
step:1326/1395 train_time:169803ms step_avg:129.03ms
step:1327/1395 train_time:169938ms step_avg:129.03ms
step:1328/1395 train_time:170073ms step_avg:129.04ms
step:1329/1395 train_time:170211ms step_avg:129.05ms
step:1330/1395 train_time:170347ms step_avg:129.05ms
step:1331/1395 train_time:170485ms step_avg:129.06ms
step:1332/1395 train_time:170623ms step_avg:129.06ms
step:1333/1395 train_time:170758ms step_avg:129.07ms
step:1334/1395 train_time:170892ms step_avg:129.07ms
step:1335/1395 train_time:171026ms step_avg:129.08ms
step:1336/1395 train_time:171163ms step_avg:129.08ms
step:1337/1395 train_time:171297ms step_avg:129.09ms
step:1338/1395 train_time:171432ms step_avg:129.09ms
step:1339/1395 train_time:171568ms step_avg:129.10ms
step:1340/1395 train_time:171704ms step_avg:129.10ms
step:1341/1395 train_time:171838ms step_avg:129.10ms
step:1342/1395 train_time:171972ms step_avg:129.11ms
step:1343/1395 train_time:172106ms step_avg:129.11ms
step:1344/1395 train_time:172240ms step_avg:129.12ms
step:1345/1395 train_time:172374ms step_avg:129.12ms
step:1346/1395 train_time:172511ms step_avg:129.12ms
step:1347/1395 train_time:172647ms step_avg:129.13ms
step:1348/1395 train_time:172782ms step_avg:129.13ms
step:1349/1395 train_time:172919ms step_avg:129.14ms
step:1350/1395 train_time:173053ms step_avg:129.14ms
step:1351/1395 train_time:173188ms step_avg:129.15ms
step:1352/1395 train_time:173326ms step_avg:129.16ms
step:1353/1395 train_time:173465ms step_avg:129.16ms
step:1354/1395 train_time:173601ms step_avg:129.17ms
step:1355/1395 train_time:173737ms step_avg:129.17ms
step:1356/1395 train_time:173871ms step_avg:129.18ms
step:1357/1395 train_time:174007ms step_avg:129.18ms
step:1358/1395 train_time:174145ms step_avg:129.19ms
step:1359/1395 train_time:174280ms step_avg:129.19ms
step:1360/1395 train_time:174418ms step_avg:129.20ms
step:1361/1395 train_time:174554ms step_avg:129.20ms
step:1362/1395 train_time:174692ms step_avg:129.21ms
step:1363/1395 train_time:174829ms step_avg:129.22ms
step:1364/1395 train_time:174966ms step_avg:129.22ms
step:1365/1395 train_time:175100ms step_avg:129.22ms
step:1366/1395 train_time:175236ms step_avg:129.23ms
step:1367/1395 train_time:175373ms step_avg:129.24ms
step:1368/1395 train_time:175509ms step_avg:129.24ms
step:1369/1395 train_time:175647ms step_avg:129.25ms
step:1370/1395 train_time:175785ms step_avg:129.25ms
step:1371/1395 train_time:175920ms step_avg:129.26ms
step:1372/1395 train_time:176058ms step_avg:129.26ms
step:1373/1395 train_time:176194ms step_avg:129.27ms
step:1374/1395 train_time:176332ms step_avg:129.28ms
step:1375/1395 train_time:176465ms step_avg:129.28ms
step:1375/1395 val_loss:3.2856 train_time:176600ms step_avg:129.38ms
step:1376/1395 train_time:176622ms step_avg:129.30ms
step:1377/1395 train_time:176745ms step_avg:129.29ms
step:1378/1395 train_time:176879ms step_avg:129.30ms
step:1379/1395 train_time:177015ms step_avg:129.30ms
step:1380/1395 train_time:177152ms step_avg:129.31ms
step:1381/1395 train_time:177288ms step_avg:129.31ms
step:1382/1395 train_time:177425ms step_avg:129.32ms
step:1383/1395 train_time:177560ms step_avg:129.32ms
step:1384/1395 train_time:177699ms step_avg:129.33ms
step:1385/1395 train_time:177834ms step_avg:129.33ms
step:1386/1395 train_time:177969ms step_avg:129.34ms
step:1387/1395 train_time:178107ms step_avg:129.34ms
step:1388/1395 train_time:178242ms step_avg:129.35ms
step:1389/1395 train_time:178377ms step_avg:129.35ms
step:1390/1395 train_time:178514ms step_avg:129.36ms
step:1391/1395 train_time:178649ms step_avg:129.36ms
step:1392/1395 train_time:178785ms step_avg:129.37ms
step:1393/1395 train_time:178920ms step_avg:129.37ms
step:1394/1395 train_time:179054ms step_avg:129.37ms
step:1395/1395 train_time:179190ms step_avg:129.38ms
step:1395/1395 val_loss:3.2814 train_time:179325ms step_avg:129.48ms
peak memory allocated: 37653 MiB reserved: 39156 MiB
