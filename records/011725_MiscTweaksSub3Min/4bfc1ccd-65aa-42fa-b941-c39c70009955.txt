import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True

# -----------------------------------------------------------------------------
# Custom operators

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

def lm_head_fp8(x: Tensor, w: Tensor) -> Tensor:
    _x = x.flatten(0, -2)
    out: Tensor = torch.ops.nanogpt.mm(_x, w, x_s=2.0, w_s=32.0, grad_s=2.0**29)[0]
    return out.reshape(*x.shape[:-1], -1)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev():
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int):
        super().__init__(in_features, out_features, bias=False)

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x):
        return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len=65536):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        # self.c_q = CastedLinear(dim, dim)
        # self.c_k = CastedLinear(dim, dim)
        # self.c_v = CastedLinear(dim, dim)
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std
        self.qkv_w = nn.Parameter(torch.empty(3, dim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads) # dim // num_heads = head_dim
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # Set attention scale such that the minimum attainable attention entropy
        # (but not necessary the attention entropy itself) is close to 0. By @leloykun
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        # q = self.c_q(x).view(B, T, self.num_heads, -1)
        # k = self.c_k(x).view(B, T, self.num_heads, -1)
        # v = self.c_v(x).view(B, T, self.num_heads, -1)
        # qkv_weight = torch.cat([self.c_q.weight, self.c_k.weight, self.c_v.weight], dim=0).type_as(x)
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3*self.num_heads, -1).chunk(3, dim=-2)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, model_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(model_dim, num_heads, layer_idx) if layer_idx != 7 else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, ve, x0, block_mask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, num_embeddings: int, embedding_dim: int):
        super().__init__()
        self.embed = nn.ModuleList([nn.Embedding(num_embeddings, embedding_dim) for _ in range(3)])

    def forward(self, input_seq) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2], None, None, None, None, None, None, ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual learning
        self.value_embeds = ValueEmbedding(vocab_size, model_dim)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128))
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        assert input_seq.ndim == 1
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        docs = (input_seq == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        def create_doc_swc_block_masks(sliding_window_num_blocks: Tensor):
            kv_idx = block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & document_bm
            full_bm  = causal_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            def build_bm(sw_num_blocks: Tensor) -> BlockMask:
                return BlockMask.from_kv_blocks(
                    torch.clamp_max(kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                    kv_indices,
                    torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                    full_kv_indices,
                    BLOCK_SIZE=BLOCK_SIZE,
                    mask_mod=document_causal,
                )
            return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

        # Long-short SWA block masks by @leloykun & @YouJiacheng
        long_bm, short_bm = create_doc_swc_block_masks(sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = lm_head_fp8(x, self.lm_head.weight) if self.training else self.lm_head(x)
        # @Grad62304977 added tanh softcapping, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use cycle(files) if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    batch_size = 8*64*1024 # batch size in tokens
    num_iterations = 1395 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 64*1024 # FlexAttention sequence length
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
             if console:
                 print(s)
             print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_loader = distributed_data_generator(args.train_files, args.batch_size, rank, world_size)

model = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim >= 2]
embed_params = [model.embed.weight, *model.value_embeds.parameters()]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), fused=True, eps=1e-10)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(it: int):
    t = 1 - it / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_bs = world_size * args.seq_len
        assert args.val_tokens % val_bs == 0
        val_steps = args.val_tokens // val_bs
        val_loader = distributed_data_generator(args.val_files, val_bs, rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION BEGIN -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB"
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250110+cu126 compiled for CUDA 12.6
Thu Jan 16 20:42:58 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   39C    P0             121W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   30C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   29C    P0             115W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0             118W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   39C    P0             120W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   37C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1395 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1395 train_time:25658ms step_avg:nanms
step:2/1395 train_time:26171ms step_avg:nanms
step:3/1395 train_time:26291ms step_avg:nanms
step:4/1395 train_time:26411ms step_avg:nanms
step:5/1395 train_time:26533ms step_avg:nanms
step:6/1395 train_time:26655ms step_avg:nanms
step:7/1395 train_time:26775ms step_avg:nanms
step:8/1395 train_time:26896ms step_avg:nanms
step:9/1395 train_time:27017ms step_avg:nanms
step:10/1395 train_time:27140ms step_avg:nanms
step:11/1395 train_time:123ms step_avg:nanms
step:12/1395 train_time:246ms step_avg:nanms
step:13/1395 train_time:368ms step_avg:122.54ms
step:14/1395 train_time:490ms step_avg:122.51ms
step:15/1395 train_time:611ms step_avg:122.26ms
step:16/1395 train_time:733ms step_avg:122.25ms
step:17/1395 train_time:855ms step_avg:122.20ms
step:18/1395 train_time:977ms step_avg:122.12ms
step:19/1395 train_time:1100ms step_avg:122.20ms
step:20/1395 train_time:1223ms step_avg:122.33ms
step:21/1395 train_time:1345ms step_avg:122.32ms
step:22/1395 train_time:1468ms step_avg:122.31ms
step:23/1395 train_time:1590ms step_avg:122.33ms
step:24/1395 train_time:1711ms step_avg:122.24ms
step:25/1395 train_time:1834ms step_avg:122.23ms
step:26/1395 train_time:1955ms step_avg:122.22ms
step:27/1395 train_time:2078ms step_avg:122.22ms
step:28/1395 train_time:2200ms step_avg:122.19ms
step:29/1395 train_time:2321ms step_avg:122.15ms
step:30/1395 train_time:2444ms step_avg:122.19ms
step:31/1395 train_time:2567ms step_avg:122.23ms
step:32/1395 train_time:2689ms step_avg:122.23ms
step:33/1395 train_time:2811ms step_avg:122.22ms
step:34/1395 train_time:2933ms step_avg:122.20ms
step:35/1395 train_time:3055ms step_avg:122.20ms
step:36/1395 train_time:3177ms step_avg:122.18ms
step:37/1395 train_time:3299ms step_avg:122.17ms
step:38/1395 train_time:3422ms step_avg:122.21ms
step:39/1395 train_time:3546ms step_avg:122.26ms
step:40/1395 train_time:3667ms step_avg:122.22ms
step:41/1395 train_time:3788ms step_avg:122.21ms
step:42/1395 train_time:3910ms step_avg:122.20ms
step:43/1395 train_time:4032ms step_avg:122.18ms
step:44/1395 train_time:4155ms step_avg:122.20ms
step:45/1395 train_time:4276ms step_avg:122.17ms
step:46/1395 train_time:4400ms step_avg:122.22ms
step:47/1395 train_time:4521ms step_avg:122.20ms
step:48/1395 train_time:4646ms step_avg:122.26ms
step:49/1395 train_time:4769ms step_avg:122.27ms
step:50/1395 train_time:4891ms step_avg:122.28ms
step:51/1395 train_time:5013ms step_avg:122.28ms
step:52/1395 train_time:5135ms step_avg:122.27ms
step:53/1395 train_time:5256ms step_avg:122.24ms
step:54/1395 train_time:5378ms step_avg:122.23ms
step:55/1395 train_time:5501ms step_avg:122.24ms
step:56/1395 train_time:5623ms step_avg:122.24ms
step:57/1395 train_time:5745ms step_avg:122.24ms
step:58/1395 train_time:5867ms step_avg:122.23ms
step:59/1395 train_time:5990ms step_avg:122.25ms
step:60/1395 train_time:6113ms step_avg:122.26ms
step:61/1395 train_time:6235ms step_avg:122.26ms
step:62/1395 train_time:6356ms step_avg:122.23ms
step:63/1395 train_time:6480ms step_avg:122.27ms
step:64/1395 train_time:6601ms step_avg:122.25ms
step:65/1395 train_time:6724ms step_avg:122.25ms
step:66/1395 train_time:6848ms step_avg:122.28ms
step:67/1395 train_time:6971ms step_avg:122.30ms
step:68/1395 train_time:7094ms step_avg:122.30ms
step:69/1395 train_time:7217ms step_avg:122.32ms
step:70/1395 train_time:7338ms step_avg:122.29ms
step:71/1395 train_time:7461ms step_avg:122.31ms
step:72/1395 train_time:7586ms step_avg:122.35ms
step:73/1395 train_time:7709ms step_avg:122.37ms
step:74/1395 train_time:7832ms step_avg:122.37ms
step:75/1395 train_time:7954ms step_avg:122.37ms
step:76/1395 train_time:8076ms step_avg:122.36ms
step:77/1395 train_time:8198ms step_avg:122.36ms
step:78/1395 train_time:8320ms step_avg:122.35ms
step:79/1395 train_time:8441ms step_avg:122.33ms
step:80/1395 train_time:8563ms step_avg:122.33ms
step:81/1395 train_time:8685ms step_avg:122.33ms
step:82/1395 train_time:8808ms step_avg:122.33ms
step:83/1395 train_time:8930ms step_avg:122.33ms
step:84/1395 train_time:9053ms step_avg:122.33ms
step:85/1395 train_time:9175ms step_avg:122.33ms
step:86/1395 train_time:9299ms step_avg:122.35ms
step:87/1395 train_time:9422ms step_avg:122.36ms
step:88/1395 train_time:9543ms step_avg:122.34ms
step:89/1395 train_time:9665ms step_avg:122.34ms
step:90/1395 train_time:9788ms step_avg:122.35ms
step:91/1395 train_time:9909ms step_avg:122.34ms
step:92/1395 train_time:10031ms step_avg:122.33ms
step:93/1395 train_time:10152ms step_avg:122.31ms
step:94/1395 train_time:10274ms step_avg:122.31ms
step:95/1395 train_time:10398ms step_avg:122.33ms
step:96/1395 train_time:10520ms step_avg:122.33ms
step:97/1395 train_time:10643ms step_avg:122.33ms
step:98/1395 train_time:10765ms step_avg:122.33ms
step:99/1395 train_time:10887ms step_avg:122.33ms
step:100/1395 train_time:11010ms step_avg:122.33ms
step:101/1395 train_time:11132ms step_avg:122.33ms
step:102/1395 train_time:11254ms step_avg:122.33ms
step:103/1395 train_time:11376ms step_avg:122.33ms
step:104/1395 train_time:11499ms step_avg:122.33ms
step:105/1395 train_time:11620ms step_avg:122.32ms
step:106/1395 train_time:11743ms step_avg:122.33ms
step:107/1395 train_time:11866ms step_avg:122.33ms
step:108/1395 train_time:11989ms step_avg:122.33ms
step:109/1395 train_time:12111ms step_avg:122.33ms
step:110/1395 train_time:12234ms step_avg:122.34ms
step:111/1395 train_time:12356ms step_avg:122.34ms
step:112/1395 train_time:12479ms step_avg:122.34ms
step:113/1395 train_time:12601ms step_avg:122.34ms
step:114/1395 train_time:12725ms step_avg:122.36ms
step:115/1395 train_time:12848ms step_avg:122.37ms
step:116/1395 train_time:12972ms step_avg:122.37ms
step:117/1395 train_time:13095ms step_avg:122.39ms
step:118/1395 train_time:13218ms step_avg:122.39ms
step:119/1395 train_time:13341ms step_avg:122.40ms
step:120/1395 train_time:13464ms step_avg:122.40ms
step:121/1395 train_time:13586ms step_avg:122.40ms
step:122/1395 train_time:13708ms step_avg:122.39ms
step:123/1395 train_time:13831ms step_avg:122.40ms
step:124/1395 train_time:13954ms step_avg:122.40ms
step:125/1395 train_time:14076ms step_avg:122.40ms
step:125/1395 val_loss:4.3953 train_time:14197ms step_avg:123.46ms
step:126/1395 train_time:14220ms step_avg:122.59ms
step:127/1395 train_time:14339ms step_avg:122.55ms
step:128/1395 train_time:14466ms step_avg:122.59ms
step:129/1395 train_time:14588ms step_avg:122.59ms
step:130/1395 train_time:14711ms step_avg:122.59ms
step:131/1395 train_time:14833ms step_avg:122.59ms
step:132/1395 train_time:14956ms step_avg:122.59ms
step:133/1395 train_time:15079ms step_avg:122.59ms
step:134/1395 train_time:15201ms step_avg:122.59ms
step:135/1395 train_time:15326ms step_avg:122.61ms
step:136/1395 train_time:15449ms step_avg:122.61ms
step:137/1395 train_time:15573ms step_avg:122.62ms
step:138/1395 train_time:15697ms step_avg:122.63ms
step:139/1395 train_time:15819ms step_avg:122.63ms
step:140/1395 train_time:15942ms step_avg:122.63ms
step:141/1395 train_time:16065ms step_avg:122.63ms
step:142/1395 train_time:16187ms step_avg:122.63ms
step:143/1395 train_time:16310ms step_avg:122.63ms
step:144/1395 train_time:16434ms step_avg:122.64ms
step:145/1395 train_time:16558ms step_avg:122.65ms
step:146/1395 train_time:16681ms step_avg:122.65ms
step:147/1395 train_time:16803ms step_avg:122.65ms
step:148/1395 train_time:16925ms step_avg:122.65ms
step:149/1395 train_time:17047ms step_avg:122.64ms
step:150/1395 train_time:17172ms step_avg:122.65ms
step:151/1395 train_time:17296ms step_avg:122.67ms
step:152/1395 train_time:17421ms step_avg:122.69ms
step:153/1395 train_time:17546ms step_avg:122.70ms
step:154/1395 train_time:17670ms step_avg:122.71ms
step:155/1395 train_time:17792ms step_avg:122.70ms
step:156/1395 train_time:17914ms step_avg:122.70ms
step:157/1395 train_time:18036ms step_avg:122.70ms
step:158/1395 train_time:18159ms step_avg:122.70ms
step:159/1395 train_time:18282ms step_avg:122.70ms
step:160/1395 train_time:18405ms step_avg:122.70ms
step:161/1395 train_time:18528ms step_avg:122.70ms
step:162/1395 train_time:18651ms step_avg:122.71ms
step:163/1395 train_time:18775ms step_avg:122.71ms
step:164/1395 train_time:18897ms step_avg:122.71ms
step:165/1395 train_time:19020ms step_avg:122.71ms
step:166/1395 train_time:19142ms step_avg:122.71ms
step:167/1395 train_time:19266ms step_avg:122.71ms
step:168/1395 train_time:19390ms step_avg:122.72ms
step:169/1395 train_time:19513ms step_avg:122.73ms
step:170/1395 train_time:19637ms step_avg:122.73ms
step:171/1395 train_time:19760ms step_avg:122.73ms
step:172/1395 train_time:19883ms step_avg:122.73ms
step:173/1395 train_time:20005ms step_avg:122.73ms
step:174/1395 train_time:20127ms step_avg:122.73ms
step:175/1395 train_time:20250ms step_avg:122.73ms
step:176/1395 train_time:20373ms step_avg:122.73ms
step:177/1395 train_time:20495ms step_avg:122.73ms
step:178/1395 train_time:20619ms step_avg:122.73ms
step:179/1395 train_time:20743ms step_avg:122.74ms
step:180/1395 train_time:20866ms step_avg:122.74ms
step:181/1395 train_time:20987ms step_avg:122.73ms
step:182/1395 train_time:21111ms step_avg:122.74ms
step:183/1395 train_time:21235ms step_avg:122.75ms
step:184/1395 train_time:21358ms step_avg:122.75ms
step:185/1395 train_time:21480ms step_avg:122.75ms
step:186/1395 train_time:21603ms step_avg:122.74ms
step:187/1395 train_time:21725ms step_avg:122.74ms
step:188/1395 train_time:21847ms step_avg:122.74ms
step:189/1395 train_time:21971ms step_avg:122.74ms
step:190/1395 train_time:22094ms step_avg:122.74ms
step:191/1395 train_time:22217ms step_avg:122.75ms
step:192/1395 train_time:22340ms step_avg:122.75ms
step:193/1395 train_time:22463ms step_avg:122.75ms
step:194/1395 train_time:22584ms step_avg:122.74ms
step:195/1395 train_time:22706ms step_avg:122.74ms
step:196/1395 train_time:22829ms step_avg:122.74ms
step:197/1395 train_time:22952ms step_avg:122.74ms
step:198/1395 train_time:23076ms step_avg:122.75ms
step:199/1395 train_time:23200ms step_avg:122.75ms
step:200/1395 train_time:23323ms step_avg:122.75ms
step:201/1395 train_time:23446ms step_avg:122.75ms
step:202/1395 train_time:23569ms step_avg:122.76ms
step:203/1395 train_time:23691ms step_avg:122.75ms
step:204/1395 train_time:23815ms step_avg:122.76ms
step:205/1395 train_time:23938ms step_avg:122.76ms
step:206/1395 train_time:24061ms step_avg:122.76ms
step:207/1395 train_time:24184ms step_avg:122.76ms
step:208/1395 train_time:24306ms step_avg:122.76ms
step:209/1395 train_time:24430ms step_avg:122.76ms
step:210/1395 train_time:24554ms step_avg:122.77ms
step:211/1395 train_time:24678ms step_avg:122.77ms
step:212/1395 train_time:24801ms step_avg:122.77ms
step:213/1395 train_time:24923ms step_avg:122.78ms
step:214/1395 train_time:25047ms step_avg:122.78ms
step:215/1395 train_time:25170ms step_avg:122.78ms
step:216/1395 train_time:25294ms step_avg:122.79ms
step:217/1395 train_time:25418ms step_avg:122.79ms
step:218/1395 train_time:25541ms step_avg:122.79ms
step:219/1395 train_time:25665ms step_avg:122.80ms
step:220/1395 train_time:25787ms step_avg:122.80ms
step:221/1395 train_time:25910ms step_avg:122.80ms
step:222/1395 train_time:26034ms step_avg:122.80ms
step:223/1395 train_time:26158ms step_avg:122.81ms
step:224/1395 train_time:26283ms step_avg:122.82ms
step:225/1395 train_time:26407ms step_avg:122.82ms
step:226/1395 train_time:26530ms step_avg:122.83ms
step:227/1395 train_time:26655ms step_avg:122.83ms
step:228/1395 train_time:26778ms step_avg:122.83ms
step:229/1395 train_time:26902ms step_avg:122.84ms
step:230/1395 train_time:27024ms step_avg:122.84ms
step:231/1395 train_time:27146ms step_avg:122.83ms
step:232/1395 train_time:27270ms step_avg:122.84ms
step:233/1395 train_time:27394ms step_avg:122.84ms
step:234/1395 train_time:27518ms step_avg:122.85ms
step:235/1395 train_time:27641ms step_avg:122.85ms
step:236/1395 train_time:27765ms step_avg:122.85ms
step:237/1395 train_time:27888ms step_avg:122.86ms
step:238/1395 train_time:28013ms step_avg:122.86ms
step:239/1395 train_time:28135ms step_avg:122.86ms
step:240/1395 train_time:28258ms step_avg:122.86ms
step:241/1395 train_time:28382ms step_avg:122.86ms
step:242/1395 train_time:28505ms step_avg:122.86ms
step:243/1395 train_time:28628ms step_avg:122.87ms
step:244/1395 train_time:28752ms step_avg:122.87ms
step:245/1395 train_time:28875ms step_avg:122.87ms
step:246/1395 train_time:29000ms step_avg:122.88ms
step:247/1395 train_time:29123ms step_avg:122.88ms
step:248/1395 train_time:29246ms step_avg:122.88ms
step:249/1395 train_time:29370ms step_avg:122.89ms
step:250/1395 train_time:29493ms step_avg:122.89ms
step:250/1395 val_loss:3.9873 train_time:29615ms step_avg:123.39ms
step:251/1395 train_time:29636ms step_avg:122.97ms
step:252/1395 train_time:29753ms step_avg:122.95ms
step:253/1395 train_time:29880ms step_avg:122.96ms
step:254/1395 train_time:30003ms step_avg:122.96ms
step:255/1395 train_time:30125ms step_avg:122.96ms
step:256/1395 train_time:30248ms step_avg:122.96ms
step:257/1395 train_time:30371ms step_avg:122.96ms
step:258/1395 train_time:30493ms step_avg:122.96ms
step:259/1395 train_time:30616ms step_avg:122.95ms
step:260/1395 train_time:30740ms step_avg:122.96ms
step:261/1395 train_time:30864ms step_avg:122.96ms
step:262/1395 train_time:30988ms step_avg:122.97ms
step:263/1395 train_time:31112ms step_avg:122.97ms
step:264/1395 train_time:31235ms step_avg:122.97ms
step:265/1395 train_time:31359ms step_avg:122.97ms
step:266/1395 train_time:31482ms step_avg:122.98ms
step:267/1395 train_time:31606ms step_avg:122.98ms
step:268/1395 train_time:31730ms step_avg:122.98ms
step:269/1395 train_time:31854ms step_avg:122.99ms
step:270/1395 train_time:31977ms step_avg:122.99ms
step:271/1395 train_time:32100ms step_avg:122.99ms
step:272/1395 train_time:32223ms step_avg:122.99ms
step:273/1395 train_time:32347ms step_avg:122.99ms
step:274/1395 train_time:32470ms step_avg:122.99ms
step:275/1395 train_time:32594ms step_avg:122.99ms
step:276/1395 train_time:32717ms step_avg:123.00ms
step:277/1395 train_time:32840ms step_avg:123.00ms
step:278/1395 train_time:32964ms step_avg:123.00ms
step:279/1395 train_time:33087ms step_avg:123.00ms
step:280/1395 train_time:33212ms step_avg:123.01ms
step:281/1395 train_time:33335ms step_avg:123.01ms
step:282/1395 train_time:33457ms step_avg:123.00ms
step:283/1395 train_time:33581ms step_avg:123.01ms
step:284/1395 train_time:33705ms step_avg:123.01ms
step:285/1395 train_time:33828ms step_avg:123.01ms
step:286/1395 train_time:33952ms step_avg:123.02ms
step:287/1395 train_time:34075ms step_avg:123.02ms
step:288/1395 train_time:34199ms step_avg:123.02ms
step:289/1395 train_time:34323ms step_avg:123.02ms
step:290/1395 train_time:34445ms step_avg:123.02ms
step:291/1395 train_time:34568ms step_avg:123.02ms
step:292/1395 train_time:34692ms step_avg:123.02ms
step:293/1395 train_time:34815ms step_avg:123.02ms
step:294/1395 train_time:34938ms step_avg:123.02ms
step:295/1395 train_time:35063ms step_avg:123.03ms
step:296/1395 train_time:35187ms step_avg:123.03ms
step:297/1395 train_time:35311ms step_avg:123.04ms
step:298/1395 train_time:35434ms step_avg:123.04ms
step:299/1395 train_time:35558ms step_avg:123.04ms
step:300/1395 train_time:35681ms step_avg:123.04ms
step:301/1395 train_time:35804ms step_avg:123.04ms
step:302/1395 train_time:35926ms step_avg:123.04ms
step:303/1395 train_time:36049ms step_avg:123.03ms
step:304/1395 train_time:36174ms step_avg:123.04ms
step:305/1395 train_time:36298ms step_avg:123.04ms
step:306/1395 train_time:36420ms step_avg:123.04ms
step:307/1395 train_time:36544ms step_avg:123.04ms
step:308/1395 train_time:36668ms step_avg:123.05ms
step:309/1395 train_time:36791ms step_avg:123.05ms
step:310/1395 train_time:36914ms step_avg:123.05ms
step:311/1395 train_time:37037ms step_avg:123.05ms
step:312/1395 train_time:37160ms step_avg:123.05ms
step:313/1395 train_time:37286ms step_avg:123.06ms
step:314/1395 train_time:37413ms step_avg:123.07ms
step:315/1395 train_time:37539ms step_avg:123.08ms
step:316/1395 train_time:37665ms step_avg:123.09ms
step:317/1395 train_time:37793ms step_avg:123.10ms
step:318/1395 train_time:37918ms step_avg:123.11ms
step:319/1395 train_time:38044ms step_avg:123.12ms
step:320/1395 train_time:38170ms step_avg:123.13ms
step:321/1395 train_time:38296ms step_avg:123.14ms
step:322/1395 train_time:38421ms step_avg:123.14ms
step:323/1395 train_time:38547ms step_avg:123.15ms
step:324/1395 train_time:38673ms step_avg:123.16ms
step:325/1395 train_time:38799ms step_avg:123.17ms
step:326/1395 train_time:38925ms step_avg:123.18ms
step:327/1395 train_time:39052ms step_avg:123.19ms
step:328/1395 train_time:39177ms step_avg:123.20ms
step:329/1395 train_time:39303ms step_avg:123.21ms
step:330/1395 train_time:39428ms step_avg:123.21ms
step:331/1395 train_time:39554ms step_avg:123.22ms
step:332/1395 train_time:39680ms step_avg:123.23ms
step:333/1395 train_time:39805ms step_avg:123.24ms
step:334/1395 train_time:39931ms step_avg:123.24ms
step:335/1395 train_time:40056ms step_avg:123.25ms
step:336/1395 train_time:40181ms step_avg:123.26ms
step:337/1395 train_time:40306ms step_avg:123.26ms
step:338/1395 train_time:40432ms step_avg:123.27ms
step:339/1395 train_time:40557ms step_avg:123.27ms
step:340/1395 train_time:40683ms step_avg:123.28ms
step:341/1395 train_time:40808ms step_avg:123.29ms
step:342/1395 train_time:40934ms step_avg:123.30ms
step:343/1395 train_time:41060ms step_avg:123.30ms
step:344/1395 train_time:41188ms step_avg:123.32ms
step:345/1395 train_time:41314ms step_avg:123.33ms
step:346/1395 train_time:41440ms step_avg:123.33ms
step:347/1395 train_time:41565ms step_avg:123.34ms
step:348/1395 train_time:41693ms step_avg:123.35ms
step:349/1395 train_time:41818ms step_avg:123.36ms
step:350/1395 train_time:41944ms step_avg:123.36ms
step:351/1395 train_time:42069ms step_avg:123.37ms
step:352/1395 train_time:42194ms step_avg:123.38ms
step:353/1395 train_time:42320ms step_avg:123.38ms
step:354/1395 train_time:42446ms step_avg:123.39ms
step:355/1395 train_time:42572ms step_avg:123.40ms
step:356/1395 train_time:42698ms step_avg:123.41ms
step:357/1395 train_time:42824ms step_avg:123.41ms
step:358/1395 train_time:42951ms step_avg:123.42ms
step:359/1395 train_time:43078ms step_avg:123.43ms
step:360/1395 train_time:43204ms step_avg:123.44ms
step:361/1395 train_time:43331ms step_avg:123.45ms
step:362/1395 train_time:43457ms step_avg:123.46ms
step:363/1395 train_time:43582ms step_avg:123.46ms
step:364/1395 train_time:43711ms step_avg:123.48ms
step:365/1395 train_time:43836ms step_avg:123.48ms
step:366/1395 train_time:43962ms step_avg:123.49ms
step:367/1395 train_time:44088ms step_avg:123.50ms
step:368/1395 train_time:44215ms step_avg:123.50ms
step:369/1395 train_time:44340ms step_avg:123.51ms
step:370/1395 train_time:44465ms step_avg:123.51ms
step:371/1395 train_time:44591ms step_avg:123.52ms
step:372/1395 train_time:44717ms step_avg:123.53ms
step:373/1395 train_time:44843ms step_avg:123.53ms
step:374/1395 train_time:44969ms step_avg:123.54ms
step:375/1395 train_time:45094ms step_avg:123.54ms
step:375/1395 val_loss:3.7874 train_time:45218ms step_avg:123.89ms
step:376/1395 train_time:45240ms step_avg:123.61ms
step:377/1395 train_time:45358ms step_avg:123.59ms
step:378/1395 train_time:45486ms step_avg:123.60ms
step:379/1395 train_time:45612ms step_avg:123.61ms
step:380/1395 train_time:45737ms step_avg:123.61ms
step:381/1395 train_time:45863ms step_avg:123.62ms
step:382/1395 train_time:45988ms step_avg:123.62ms
step:383/1395 train_time:46114ms step_avg:123.63ms
step:384/1395 train_time:46239ms step_avg:123.63ms
step:385/1395 train_time:46365ms step_avg:123.64ms
step:386/1395 train_time:46492ms step_avg:123.65ms
step:387/1395 train_time:46619ms step_avg:123.66ms
step:388/1395 train_time:46744ms step_avg:123.66ms
step:389/1395 train_time:46870ms step_avg:123.67ms
step:390/1395 train_time:46995ms step_avg:123.67ms
step:391/1395 train_time:47120ms step_avg:123.68ms
step:392/1395 train_time:47246ms step_avg:123.68ms
step:393/1395 train_time:47372ms step_avg:123.69ms
step:394/1395 train_time:47499ms step_avg:123.70ms
step:395/1395 train_time:47625ms step_avg:123.70ms
step:396/1395 train_time:47751ms step_avg:123.71ms
step:397/1395 train_time:47876ms step_avg:123.71ms
step:398/1395 train_time:48003ms step_avg:123.72ms
step:399/1395 train_time:48129ms step_avg:123.72ms
step:400/1395 train_time:48254ms step_avg:123.73ms
step:401/1395 train_time:48380ms step_avg:123.74ms
step:402/1395 train_time:48507ms step_avg:123.74ms
step:403/1395 train_time:48635ms step_avg:123.75ms
step:404/1395 train_time:48762ms step_avg:123.76ms
step:405/1395 train_time:48888ms step_avg:123.77ms
step:406/1395 train_time:49012ms step_avg:123.77ms
step:407/1395 train_time:49138ms step_avg:123.77ms
step:408/1395 train_time:49263ms step_avg:123.78ms
step:409/1395 train_time:49390ms step_avg:123.79ms
step:410/1395 train_time:49516ms step_avg:123.79ms
step:411/1395 train_time:49642ms step_avg:123.79ms
step:412/1395 train_time:49767ms step_avg:123.80ms
step:413/1395 train_time:49894ms step_avg:123.81ms
step:414/1395 train_time:50019ms step_avg:123.81ms
step:415/1395 train_time:50145ms step_avg:123.82ms
step:416/1395 train_time:50271ms step_avg:123.82ms
step:417/1395 train_time:50399ms step_avg:123.83ms
step:418/1395 train_time:50525ms step_avg:123.84ms
step:419/1395 train_time:50651ms step_avg:123.84ms
step:420/1395 train_time:50777ms step_avg:123.85ms
step:421/1395 train_time:50903ms step_avg:123.85ms
step:422/1395 train_time:51031ms step_avg:123.86ms
step:423/1395 train_time:51157ms step_avg:123.87ms
step:424/1395 train_time:51283ms step_avg:123.87ms
step:425/1395 train_time:51411ms step_avg:123.88ms
step:426/1395 train_time:51537ms step_avg:123.89ms
step:427/1395 train_time:51664ms step_avg:123.89ms
step:428/1395 train_time:51790ms step_avg:123.90ms
step:429/1395 train_time:51916ms step_avg:123.90ms
step:430/1395 train_time:52042ms step_avg:123.91ms
step:431/1395 train_time:52169ms step_avg:123.92ms
step:432/1395 train_time:52294ms step_avg:123.92ms
step:433/1395 train_time:52422ms step_avg:123.93ms
step:434/1395 train_time:52549ms step_avg:123.94ms
step:435/1395 train_time:52675ms step_avg:123.94ms
step:436/1395 train_time:52801ms step_avg:123.94ms
step:437/1395 train_time:52927ms step_avg:123.95ms
step:438/1395 train_time:53052ms step_avg:123.95ms
step:439/1395 train_time:53178ms step_avg:123.96ms
step:440/1395 train_time:53304ms step_avg:123.96ms
step:441/1395 train_time:53431ms step_avg:123.97ms
step:442/1395 train_time:53558ms step_avg:123.98ms
step:443/1395 train_time:53685ms step_avg:123.98ms
step:444/1395 train_time:53811ms step_avg:123.99ms
step:445/1395 train_time:53937ms step_avg:123.99ms
step:446/1395 train_time:54063ms step_avg:124.00ms
step:447/1395 train_time:54189ms step_avg:124.00ms
step:448/1395 train_time:54314ms step_avg:124.01ms
step:449/1395 train_time:54441ms step_avg:124.01ms
step:450/1395 train_time:54568ms step_avg:124.02ms
step:451/1395 train_time:54694ms step_avg:124.02ms
step:452/1395 train_time:54821ms step_avg:124.03ms
step:453/1395 train_time:54946ms step_avg:124.03ms
step:454/1395 train_time:55073ms step_avg:124.04ms
step:455/1395 train_time:55199ms step_avg:124.04ms
step:456/1395 train_time:55326ms step_avg:124.05ms
step:457/1395 train_time:55453ms step_avg:124.06ms
step:458/1395 train_time:55580ms step_avg:124.06ms
step:459/1395 train_time:55706ms step_avg:124.07ms
step:460/1395 train_time:55833ms step_avg:124.07ms
step:461/1395 train_time:55959ms step_avg:124.08ms
step:462/1395 train_time:56085ms step_avg:124.08ms
step:463/1395 train_time:56212ms step_avg:124.09ms
step:464/1395 train_time:56338ms step_avg:124.09ms
step:465/1395 train_time:56465ms step_avg:124.10ms
step:466/1395 train_time:56591ms step_avg:124.10ms
step:467/1395 train_time:56718ms step_avg:124.11ms
step:468/1395 train_time:56844ms step_avg:124.11ms
step:469/1395 train_time:56970ms step_avg:124.12ms
step:470/1395 train_time:57096ms step_avg:124.12ms
step:471/1395 train_time:57223ms step_avg:124.13ms
step:472/1395 train_time:57349ms step_avg:124.13ms
step:473/1395 train_time:57475ms step_avg:124.14ms
step:474/1395 train_time:57601ms step_avg:124.14ms
step:475/1395 train_time:57728ms step_avg:124.15ms
step:476/1395 train_time:57855ms step_avg:124.15ms
step:477/1395 train_time:57982ms step_avg:124.16ms
step:478/1395 train_time:58109ms step_avg:124.16ms
step:479/1395 train_time:58234ms step_avg:124.17ms
step:480/1395 train_time:58361ms step_avg:124.17ms
step:481/1395 train_time:58489ms step_avg:124.18ms
step:482/1395 train_time:58614ms step_avg:124.18ms
step:483/1395 train_time:58740ms step_avg:124.19ms
step:484/1395 train_time:58866ms step_avg:124.19ms
step:485/1395 train_time:58993ms step_avg:124.20ms
step:486/1395 train_time:59120ms step_avg:124.20ms
step:487/1395 train_time:59245ms step_avg:124.20ms
step:488/1395 train_time:59371ms step_avg:124.21ms
step:489/1395 train_time:59498ms step_avg:124.21ms
step:490/1395 train_time:59624ms step_avg:124.22ms
step:491/1395 train_time:59751ms step_avg:124.22ms
step:492/1395 train_time:59877ms step_avg:124.23ms
step:493/1395 train_time:60003ms step_avg:124.23ms
step:494/1395 train_time:60130ms step_avg:124.24ms
step:495/1395 train_time:60257ms step_avg:124.24ms
step:496/1395 train_time:60383ms step_avg:124.24ms
step:497/1395 train_time:60509ms step_avg:124.25ms
step:498/1395 train_time:60635ms step_avg:124.25ms
step:499/1395 train_time:60762ms step_avg:124.26ms
step:500/1395 train_time:60889ms step_avg:124.26ms
step:500/1395 val_loss:3.6639 train_time:61013ms step_avg:124.52ms
step:501/1395 train_time:61034ms step_avg:124.31ms
step:502/1395 train_time:61155ms step_avg:124.30ms
step:503/1395 train_time:61284ms step_avg:124.31ms
step:504/1395 train_time:61410ms step_avg:124.31ms
step:505/1395 train_time:61535ms step_avg:124.31ms
step:506/1395 train_time:61661ms step_avg:124.32ms
step:507/1395 train_time:61787ms step_avg:124.32ms
step:508/1395 train_time:61913ms step_avg:124.32ms
step:509/1395 train_time:62040ms step_avg:124.33ms
step:510/1395 train_time:62168ms step_avg:124.34ms
step:511/1395 train_time:62295ms step_avg:124.34ms
step:512/1395 train_time:62423ms step_avg:124.35ms
step:513/1395 train_time:62549ms step_avg:124.35ms
step:514/1395 train_time:62674ms step_avg:124.35ms
step:515/1395 train_time:62800ms step_avg:124.36ms
step:516/1395 train_time:62926ms step_avg:124.36ms
step:517/1395 train_time:63053ms step_avg:124.36ms
step:518/1395 train_time:63179ms step_avg:124.37ms
step:519/1395 train_time:63308ms step_avg:124.38ms
step:520/1395 train_time:63436ms step_avg:124.39ms
step:521/1395 train_time:63564ms step_avg:124.39ms
step:522/1395 train_time:63692ms step_avg:124.40ms
step:523/1395 train_time:63820ms step_avg:124.41ms
step:524/1395 train_time:63948ms step_avg:124.41ms
step:525/1395 train_time:64076ms step_avg:124.42ms
step:526/1395 train_time:64205ms step_avg:124.43ms
step:527/1395 train_time:64334ms step_avg:124.44ms
step:528/1395 train_time:64462ms step_avg:124.44ms
step:529/1395 train_time:64590ms step_avg:124.45ms
step:530/1395 train_time:64720ms step_avg:124.46ms
step:531/1395 train_time:64848ms step_avg:124.47ms
step:532/1395 train_time:64977ms step_avg:124.48ms
step:533/1395 train_time:65107ms step_avg:124.49ms
step:534/1395 train_time:65236ms step_avg:124.50ms
step:535/1395 train_time:65365ms step_avg:124.50ms
step:536/1395 train_time:65493ms step_avg:124.51ms
step:537/1395 train_time:65622ms step_avg:124.52ms
step:538/1395 train_time:65750ms step_avg:124.53ms
step:539/1395 train_time:65878ms step_avg:124.53ms
step:540/1395 train_time:66007ms step_avg:124.54ms
step:541/1395 train_time:66136ms step_avg:124.55ms
step:542/1395 train_time:66265ms step_avg:124.56ms
step:543/1395 train_time:66393ms step_avg:124.56ms
step:544/1395 train_time:66521ms step_avg:124.57ms
step:545/1395 train_time:66649ms step_avg:124.58ms
step:546/1395 train_time:66778ms step_avg:124.59ms
step:547/1395 train_time:66907ms step_avg:124.59ms
step:548/1395 train_time:67035ms step_avg:124.60ms
step:549/1395 train_time:67165ms step_avg:124.61ms
step:550/1395 train_time:67293ms step_avg:124.62ms
step:551/1395 train_time:67422ms step_avg:124.62ms
step:552/1395 train_time:67551ms step_avg:124.63ms
step:553/1395 train_time:67680ms step_avg:124.64ms
step:554/1395 train_time:67808ms step_avg:124.65ms
step:555/1395 train_time:67936ms step_avg:124.65ms
step:556/1395 train_time:68065ms step_avg:124.66ms
step:557/1395 train_time:68193ms step_avg:124.67ms
step:558/1395 train_time:68320ms step_avg:124.67ms
step:559/1395 train_time:68449ms step_avg:124.68ms
step:560/1395 train_time:68578ms step_avg:124.69ms
step:561/1395 train_time:68706ms step_avg:124.69ms
step:562/1395 train_time:68837ms step_avg:124.71ms
step:563/1395 train_time:68966ms step_avg:124.71ms
step:564/1395 train_time:69094ms step_avg:124.72ms
step:565/1395 train_time:69221ms step_avg:124.72ms
step:566/1395 train_time:69349ms step_avg:124.73ms
step:567/1395 train_time:69477ms step_avg:124.74ms
step:568/1395 train_time:69607ms step_avg:124.74ms
step:569/1395 train_time:69736ms step_avg:124.75ms
step:570/1395 train_time:69865ms step_avg:124.76ms
step:571/1395 train_time:69994ms step_avg:124.77ms
step:572/1395 train_time:70122ms step_avg:124.77ms
step:573/1395 train_time:70249ms step_avg:124.78ms
step:574/1395 train_time:70378ms step_avg:124.78ms
step:575/1395 train_time:70507ms step_avg:124.79ms
step:576/1395 train_time:70635ms step_avg:124.80ms
step:577/1395 train_time:70763ms step_avg:124.80ms
step:578/1395 train_time:70892ms step_avg:124.81ms
step:579/1395 train_time:71022ms step_avg:124.82ms
step:580/1395 train_time:71150ms step_avg:124.82ms
step:581/1395 train_time:71279ms step_avg:124.83ms
step:582/1395 train_time:71407ms step_avg:124.84ms
step:583/1395 train_time:71535ms step_avg:124.84ms
step:584/1395 train_time:71665ms step_avg:124.85ms
step:585/1395 train_time:71794ms step_avg:124.86ms
step:586/1395 train_time:71922ms step_avg:124.87ms
step:587/1395 train_time:72050ms step_avg:124.87ms
step:588/1395 train_time:72179ms step_avg:124.88ms
step:589/1395 train_time:72306ms step_avg:124.88ms
step:590/1395 train_time:72435ms step_avg:124.89ms
step:591/1395 train_time:72564ms step_avg:124.90ms
step:592/1395 train_time:72693ms step_avg:124.90ms
step:593/1395 train_time:72821ms step_avg:124.91ms
step:594/1395 train_time:72949ms step_avg:124.91ms
step:595/1395 train_time:73078ms step_avg:124.92ms
step:596/1395 train_time:73207ms step_avg:124.93ms
step:597/1395 train_time:73336ms step_avg:124.93ms
step:598/1395 train_time:73464ms step_avg:124.94ms
step:599/1395 train_time:73595ms step_avg:124.95ms
step:600/1395 train_time:73722ms step_avg:124.95ms
step:601/1395 train_time:73851ms step_avg:124.96ms
step:602/1395 train_time:73979ms step_avg:124.96ms
step:603/1395 train_time:74107ms step_avg:124.97ms
step:604/1395 train_time:74237ms step_avg:124.98ms
step:605/1395 train_time:74366ms step_avg:124.99ms
step:606/1395 train_time:74495ms step_avg:124.99ms
step:607/1395 train_time:74624ms step_avg:125.00ms
step:608/1395 train_time:74753ms step_avg:125.00ms
step:609/1395 train_time:74881ms step_avg:125.01ms
step:610/1395 train_time:75010ms step_avg:125.02ms
step:611/1395 train_time:75139ms step_avg:125.02ms
step:612/1395 train_time:75267ms step_avg:125.03ms
step:613/1395 train_time:75395ms step_avg:125.03ms
step:614/1395 train_time:75524ms step_avg:125.04ms
step:615/1395 train_time:75652ms step_avg:125.04ms
step:616/1395 train_time:75780ms step_avg:125.05ms
step:617/1395 train_time:75908ms step_avg:125.05ms
step:618/1395 train_time:76037ms step_avg:125.06ms
step:619/1395 train_time:76166ms step_avg:125.07ms
step:620/1395 train_time:76295ms step_avg:125.07ms
step:621/1395 train_time:76424ms step_avg:125.08ms
step:622/1395 train_time:76552ms step_avg:125.09ms
step:623/1395 train_time:76681ms step_avg:125.09ms
step:624/1395 train_time:76809ms step_avg:125.10ms
step:625/1395 train_time:76938ms step_avg:125.10ms
step:625/1395 val_loss:3.5817 train_time:77065ms step_avg:125.31ms
step:626/1395 train_time:77087ms step_avg:125.14ms
step:627/1395 train_time:77206ms step_avg:125.13ms
step:628/1395 train_time:77337ms step_avg:125.14ms
step:629/1395 train_time:77466ms step_avg:125.15ms
step:630/1395 train_time:77594ms step_avg:125.15ms
step:631/1395 train_time:77723ms step_avg:125.16ms
step:632/1395 train_time:77850ms step_avg:125.16ms
step:633/1395 train_time:77979ms step_avg:125.17ms
step:634/1395 train_time:78108ms step_avg:125.17ms
step:635/1395 train_time:78238ms step_avg:125.18ms
step:636/1395 train_time:78367ms step_avg:125.19ms
step:637/1395 train_time:78496ms step_avg:125.19ms
step:638/1395 train_time:78625ms step_avg:125.20ms
step:639/1395 train_time:78753ms step_avg:125.20ms
step:640/1395 train_time:78882ms step_avg:125.21ms
step:641/1395 train_time:79010ms step_avg:125.21ms
step:642/1395 train_time:79138ms step_avg:125.22ms
step:643/1395 train_time:79267ms step_avg:125.22ms
step:644/1395 train_time:79396ms step_avg:125.23ms
step:645/1395 train_time:79525ms step_avg:125.24ms
step:646/1395 train_time:79655ms step_avg:125.24ms
step:647/1395 train_time:79783ms step_avg:125.25ms
step:648/1395 train_time:79913ms step_avg:125.26ms
step:649/1395 train_time:80043ms step_avg:125.26ms
step:650/1395 train_time:80172ms step_avg:125.27ms
step:651/1395 train_time:80301ms step_avg:125.27ms
step:652/1395 train_time:80430ms step_avg:125.28ms
step:653/1395 train_time:80559ms step_avg:125.29ms
step:654/1395 train_time:80688ms step_avg:125.29ms
step:655/1395 train_time:80816ms step_avg:125.30ms
step:656/1395 train_time:80945ms step_avg:125.30ms
step:657/1395 train_time:81074ms step_avg:125.31ms
step:658/1395 train_time:81203ms step_avg:125.31ms
step:659/1395 train_time:81332ms step_avg:125.32ms
step:660/1395 train_time:81460ms step_avg:125.32ms
step:661/1395 train_time:81588ms step_avg:125.33ms
step:662/1395 train_time:81717ms step_avg:125.33ms
step:663/1395 train_time:81845ms step_avg:125.34ms
step:664/1395 train_time:81975ms step_avg:125.34ms
step:665/1395 train_time:82104ms step_avg:125.35ms
step:666/1395 train_time:82233ms step_avg:125.36ms
step:667/1395 train_time:82363ms step_avg:125.36ms
step:668/1395 train_time:82491ms step_avg:125.37ms
step:669/1395 train_time:82619ms step_avg:125.37ms
step:670/1395 train_time:82747ms step_avg:125.37ms
step:671/1395 train_time:82877ms step_avg:125.38ms
step:672/1395 train_time:83004ms step_avg:125.38ms
step:673/1395 train_time:83133ms step_avg:125.39ms
step:674/1395 train_time:83262ms step_avg:125.40ms
step:675/1395 train_time:83392ms step_avg:125.40ms
step:676/1395 train_time:83520ms step_avg:125.41ms
step:677/1395 train_time:83649ms step_avg:125.41ms
step:678/1395 train_time:83777ms step_avg:125.41ms
step:679/1395 train_time:83905ms step_avg:125.42ms
step:680/1395 train_time:84034ms step_avg:125.42ms
step:681/1395 train_time:84164ms step_avg:125.43ms
step:682/1395 train_time:84293ms step_avg:125.44ms
step:683/1395 train_time:84422ms step_avg:125.44ms
step:684/1395 train_time:84550ms step_avg:125.45ms
step:685/1395 train_time:84679ms step_avg:125.45ms
step:686/1395 train_time:84808ms step_avg:125.46ms
step:687/1395 train_time:84937ms step_avg:125.46ms
step:688/1395 train_time:85066ms step_avg:125.47ms
step:689/1395 train_time:85195ms step_avg:125.47ms
step:690/1395 train_time:85323ms step_avg:125.48ms
step:691/1395 train_time:85453ms step_avg:125.48ms
step:692/1395 train_time:85581ms step_avg:125.49ms
step:693/1395 train_time:85710ms step_avg:125.49ms
step:694/1395 train_time:85838ms step_avg:125.49ms
step:695/1395 train_time:85967ms step_avg:125.50ms
step:696/1395 train_time:86097ms step_avg:125.51ms
step:697/1395 train_time:86226ms step_avg:125.51ms
step:698/1395 train_time:86355ms step_avg:125.52ms
step:699/1395 train_time:86483ms step_avg:125.52ms
step:700/1395 train_time:86612ms step_avg:125.52ms
step:701/1395 train_time:86741ms step_avg:125.53ms
step:702/1395 train_time:86871ms step_avg:125.54ms
step:703/1395 train_time:86999ms step_avg:125.54ms
step:704/1395 train_time:87127ms step_avg:125.54ms
step:705/1395 train_time:87257ms step_avg:125.55ms
step:706/1395 train_time:87386ms step_avg:125.55ms
step:707/1395 train_time:87515ms step_avg:125.56ms
step:708/1395 train_time:87644ms step_avg:125.56ms
step:709/1395 train_time:87772ms step_avg:125.57ms
step:710/1395 train_time:87902ms step_avg:125.57ms
step:711/1395 train_time:88030ms step_avg:125.58ms
step:712/1395 train_time:88159ms step_avg:125.58ms
step:713/1395 train_time:88287ms step_avg:125.59ms
step:714/1395 train_time:88416ms step_avg:125.59ms
step:715/1395 train_time:88545ms step_avg:125.60ms
step:716/1395 train_time:88674ms step_avg:125.60ms
step:717/1395 train_time:88802ms step_avg:125.60ms
step:718/1395 train_time:88931ms step_avg:125.61ms
step:719/1395 train_time:89059ms step_avg:125.61ms
step:720/1395 train_time:89187ms step_avg:125.62ms
step:721/1395 train_time:89316ms step_avg:125.62ms
step:722/1395 train_time:89444ms step_avg:125.62ms
step:723/1395 train_time:89572ms step_avg:125.63ms
step:724/1395 train_time:89701ms step_avg:125.63ms
step:725/1395 train_time:89829ms step_avg:125.63ms
step:726/1395 train_time:89959ms step_avg:125.64ms
step:727/1395 train_time:90090ms step_avg:125.65ms
step:728/1395 train_time:90220ms step_avg:125.65ms
step:729/1395 train_time:90350ms step_avg:125.66ms
step:730/1395 train_time:90481ms step_avg:125.67ms
step:731/1395 train_time:90611ms step_avg:125.67ms
step:732/1395 train_time:90742ms step_avg:125.68ms
step:733/1395 train_time:90874ms step_avg:125.69ms
step:734/1395 train_time:91004ms step_avg:125.70ms
step:735/1395 train_time:91134ms step_avg:125.70ms
step:736/1395 train_time:91265ms step_avg:125.71ms
step:737/1395 train_time:91395ms step_avg:125.72ms
step:738/1395 train_time:91526ms step_avg:125.72ms
step:739/1395 train_time:91657ms step_avg:125.73ms
step:740/1395 train_time:91787ms step_avg:125.74ms
step:741/1395 train_time:91920ms step_avg:125.75ms
step:742/1395 train_time:92050ms step_avg:125.75ms
step:743/1395 train_time:92180ms step_avg:125.76ms
step:744/1395 train_time:92311ms step_avg:125.76ms
step:745/1395 train_time:92441ms step_avg:125.77ms
step:746/1395 train_time:92572ms step_avg:125.78ms
step:747/1395 train_time:92703ms step_avg:125.78ms
step:748/1395 train_time:92834ms step_avg:125.79ms
step:749/1395 train_time:92965ms step_avg:125.80ms
step:750/1395 train_time:93096ms step_avg:125.81ms
step:750/1395 val_loss:3.5258 train_time:93225ms step_avg:125.98ms
step:751/1395 train_time:93247ms step_avg:125.84ms
step:752/1395 train_time:93370ms step_avg:125.84ms
step:753/1395 train_time:93501ms step_avg:125.84ms
step:754/1395 train_time:93631ms step_avg:125.85ms
step:755/1395 train_time:93762ms step_avg:125.85ms
step:756/1395 train_time:93891ms step_avg:125.86ms
step:757/1395 train_time:94022ms step_avg:125.87ms
step:758/1395 train_time:94153ms step_avg:125.87ms
step:759/1395 train_time:94283ms step_avg:125.88ms
step:760/1395 train_time:94415ms step_avg:125.89ms
step:761/1395 train_time:94545ms step_avg:125.89ms
step:762/1395 train_time:94676ms step_avg:125.90ms
step:763/1395 train_time:94807ms step_avg:125.91ms
step:764/1395 train_time:94937ms step_avg:125.91ms
step:765/1395 train_time:95068ms step_avg:125.92ms
step:766/1395 train_time:95199ms step_avg:125.92ms
step:767/1395 train_time:95329ms step_avg:125.93ms
step:768/1395 train_time:95459ms step_avg:125.94ms
step:769/1395 train_time:95590ms step_avg:125.94ms
step:770/1395 train_time:95721ms step_avg:125.95ms
step:771/1395 train_time:95852ms step_avg:125.96ms
step:772/1395 train_time:95982ms step_avg:125.96ms
step:773/1395 train_time:96113ms step_avg:125.97ms
step:774/1395 train_time:96243ms step_avg:125.97ms
step:775/1395 train_time:96374ms step_avg:125.98ms
step:776/1395 train_time:96504ms step_avg:125.98ms
step:777/1395 train_time:96636ms step_avg:125.99ms
step:778/1395 train_time:96766ms step_avg:126.00ms
step:779/1395 train_time:96896ms step_avg:126.00ms
step:780/1395 train_time:97029ms step_avg:126.01ms
step:781/1395 train_time:97159ms step_avg:126.02ms
step:782/1395 train_time:97289ms step_avg:126.02ms
step:783/1395 train_time:97420ms step_avg:126.03ms
step:784/1395 train_time:97551ms step_avg:126.03ms
step:785/1395 train_time:97682ms step_avg:126.04ms
step:786/1395 train_time:97813ms step_avg:126.05ms
step:787/1395 train_time:97944ms step_avg:126.05ms
step:788/1395 train_time:98075ms step_avg:126.06ms
step:789/1395 train_time:98205ms step_avg:126.07ms
step:790/1395 train_time:98335ms step_avg:126.07ms
step:791/1395 train_time:98465ms step_avg:126.08ms
step:792/1395 train_time:98596ms step_avg:126.08ms
step:793/1395 train_time:98727ms step_avg:126.09ms
step:794/1395 train_time:98858ms step_avg:126.09ms
step:795/1395 train_time:98990ms step_avg:126.10ms
step:796/1395 train_time:99122ms step_avg:126.11ms
step:797/1395 train_time:99253ms step_avg:126.12ms
step:798/1395 train_time:99384ms step_avg:126.12ms
step:799/1395 train_time:99514ms step_avg:126.13ms
step:800/1395 train_time:99645ms step_avg:126.13ms
step:801/1395 train_time:99776ms step_avg:126.14ms
step:802/1395 train_time:99908ms step_avg:126.15ms
step:803/1395 train_time:100038ms step_avg:126.15ms
step:804/1395 train_time:100168ms step_avg:126.16ms
step:805/1395 train_time:100300ms step_avg:126.16ms
step:806/1395 train_time:100431ms step_avg:126.17ms
step:807/1395 train_time:100561ms step_avg:126.17ms
step:808/1395 train_time:100692ms step_avg:126.18ms
step:809/1395 train_time:100822ms step_avg:126.18ms
step:810/1395 train_time:100953ms step_avg:126.19ms
step:811/1395 train_time:101083ms step_avg:126.20ms
step:812/1395 train_time:101215ms step_avg:126.20ms
step:813/1395 train_time:101345ms step_avg:126.21ms
step:814/1395 train_time:101476ms step_avg:126.21ms
step:815/1395 train_time:101606ms step_avg:126.22ms
step:816/1395 train_time:101737ms step_avg:126.22ms
step:817/1395 train_time:101867ms step_avg:126.23ms
step:818/1395 train_time:101997ms step_avg:126.23ms
step:819/1395 train_time:102128ms step_avg:126.24ms
step:820/1395 train_time:102258ms step_avg:126.24ms
step:821/1395 train_time:102390ms step_avg:126.25ms
step:822/1395 train_time:102521ms step_avg:126.26ms
step:823/1395 train_time:102652ms step_avg:126.26ms
step:824/1395 train_time:102783ms step_avg:126.27ms
step:825/1395 train_time:102914ms step_avg:126.27ms
step:826/1395 train_time:103046ms step_avg:126.28ms
step:827/1395 train_time:103176ms step_avg:126.29ms
step:828/1395 train_time:103306ms step_avg:126.29ms
step:829/1395 train_time:103437ms step_avg:126.30ms
step:830/1395 train_time:103568ms step_avg:126.30ms
step:831/1395 train_time:103699ms step_avg:126.31ms
step:832/1395 train_time:103830ms step_avg:126.31ms
step:833/1395 train_time:103959ms step_avg:126.32ms
step:834/1395 train_time:104090ms step_avg:126.32ms
step:835/1395 train_time:104220ms step_avg:126.33ms
step:836/1395 train_time:104351ms step_avg:126.33ms
step:837/1395 train_time:104483ms step_avg:126.34ms
step:838/1395 train_time:104614ms step_avg:126.35ms
step:839/1395 train_time:104745ms step_avg:126.35ms
step:840/1395 train_time:104876ms step_avg:126.36ms
step:841/1395 train_time:105007ms step_avg:126.36ms
step:842/1395 train_time:105139ms step_avg:126.37ms
step:843/1395 train_time:105269ms step_avg:126.37ms
step:844/1395 train_time:105399ms step_avg:126.38ms
step:845/1395 train_time:105529ms step_avg:126.38ms
step:846/1395 train_time:105661ms step_avg:126.39ms
step:847/1395 train_time:105792ms step_avg:126.39ms
step:848/1395 train_time:105922ms step_avg:126.40ms
step:849/1395 train_time:106053ms step_avg:126.40ms
step:850/1395 train_time:106184ms step_avg:126.41ms
step:851/1395 train_time:106316ms step_avg:126.42ms
step:852/1395 train_time:106447ms step_avg:126.42ms
step:853/1395 train_time:106578ms step_avg:126.43ms
step:854/1395 train_time:106709ms step_avg:126.43ms
step:855/1395 train_time:106839ms step_avg:126.44ms
step:856/1395 train_time:106970ms step_avg:126.44ms
step:857/1395 train_time:107100ms step_avg:126.45ms
step:858/1395 train_time:107231ms step_avg:126.45ms
step:859/1395 train_time:107362ms step_avg:126.46ms
step:860/1395 train_time:107494ms step_avg:126.46ms
step:861/1395 train_time:107623ms step_avg:126.47ms
step:862/1395 train_time:107754ms step_avg:126.47ms
step:863/1395 train_time:107885ms step_avg:126.48ms
step:864/1395 train_time:108017ms step_avg:126.48ms
step:865/1395 train_time:108147ms step_avg:126.49ms
step:866/1395 train_time:108280ms step_avg:126.50ms
step:867/1395 train_time:108412ms step_avg:126.50ms
step:868/1395 train_time:108543ms step_avg:126.51ms
step:869/1395 train_time:108674ms step_avg:126.51ms
step:870/1395 train_time:108806ms step_avg:126.52ms
step:871/1395 train_time:108936ms step_avg:126.52ms
step:872/1395 train_time:109067ms step_avg:126.53ms
step:873/1395 train_time:109197ms step_avg:126.53ms
step:874/1395 train_time:109328ms step_avg:126.54ms
step:875/1395 train_time:109459ms step_avg:126.54ms
step:875/1395 val_loss:3.4759 train_time:109588ms step_avg:126.69ms
step:876/1395 train_time:109609ms step_avg:126.57ms
step:877/1395 train_time:109733ms step_avg:126.57ms
step:878/1395 train_time:109866ms step_avg:126.57ms
step:879/1395 train_time:109997ms step_avg:126.58ms
step:880/1395 train_time:110128ms step_avg:126.58ms
step:881/1395 train_time:110258ms step_avg:126.59ms
step:882/1395 train_time:110389ms step_avg:126.59ms
step:883/1395 train_time:110519ms step_avg:126.60ms
step:884/1395 train_time:110651ms step_avg:126.60ms
step:885/1395 train_time:110783ms step_avg:126.61ms
step:886/1395 train_time:110915ms step_avg:126.62ms
step:887/1395 train_time:111045ms step_avg:126.62ms
step:888/1395 train_time:111176ms step_avg:126.62ms
step:889/1395 train_time:111309ms step_avg:126.63ms
step:890/1395 train_time:111439ms step_avg:126.64ms
step:891/1395 train_time:111570ms step_avg:126.64ms
step:892/1395 train_time:111700ms step_avg:126.64ms
step:893/1395 train_time:111832ms step_avg:126.65ms
step:894/1395 train_time:111962ms step_avg:126.65ms
step:895/1395 train_time:112094ms step_avg:126.66ms
step:896/1395 train_time:112224ms step_avg:126.66ms
step:897/1395 train_time:112354ms step_avg:126.67ms
step:898/1395 train_time:112484ms step_avg:126.67ms
step:899/1395 train_time:112617ms step_avg:126.68ms
step:900/1395 train_time:112747ms step_avg:126.68ms
step:901/1395 train_time:112878ms step_avg:126.69ms
step:902/1395 train_time:113008ms step_avg:126.69ms
step:903/1395 train_time:113139ms step_avg:126.70ms
step:904/1395 train_time:113270ms step_avg:126.70ms
step:905/1395 train_time:113400ms step_avg:126.70ms
step:906/1395 train_time:113531ms step_avg:126.71ms
step:907/1395 train_time:113663ms step_avg:126.71ms
step:908/1395 train_time:113794ms step_avg:126.72ms
step:909/1395 train_time:113925ms step_avg:126.72ms
step:910/1395 train_time:114057ms step_avg:126.73ms
step:911/1395 train_time:114188ms step_avg:126.73ms
step:912/1395 train_time:114318ms step_avg:126.74ms
step:913/1395 train_time:114449ms step_avg:126.74ms
step:914/1395 train_time:114579ms step_avg:126.75ms
step:915/1395 train_time:114711ms step_avg:126.75ms
step:916/1395 train_time:114841ms step_avg:126.76ms
step:917/1395 train_time:114972ms step_avg:126.76ms
step:918/1395 train_time:115102ms step_avg:126.76ms
step:919/1395 train_time:115237ms step_avg:126.77ms
step:920/1395 train_time:115367ms step_avg:126.78ms
step:921/1395 train_time:115498ms step_avg:126.78ms
step:922/1395 train_time:115629ms step_avg:126.79ms
step:923/1395 train_time:115758ms step_avg:126.79ms
step:924/1395 train_time:115889ms step_avg:126.79ms
step:925/1395 train_time:116021ms step_avg:126.80ms
step:926/1395 train_time:116152ms step_avg:126.80ms
step:927/1395 train_time:116283ms step_avg:126.81ms
step:928/1395 train_time:116414ms step_avg:126.81ms
step:929/1395 train_time:116545ms step_avg:126.82ms
step:930/1395 train_time:116675ms step_avg:126.82ms
step:931/1395 train_time:116806ms step_avg:126.83ms
step:932/1395 train_time:116937ms step_avg:126.83ms
step:933/1395 train_time:117070ms step_avg:126.84ms
step:934/1395 train_time:117202ms step_avg:126.84ms
step:935/1395 train_time:117335ms step_avg:126.85ms
step:936/1395 train_time:117468ms step_avg:126.85ms
step:937/1395 train_time:117601ms step_avg:126.86ms
step:938/1395 train_time:117736ms step_avg:126.87ms
step:939/1395 train_time:117867ms step_avg:126.87ms
step:940/1395 train_time:118001ms step_avg:126.88ms
step:941/1395 train_time:118132ms step_avg:126.89ms
step:942/1395 train_time:118265ms step_avg:126.89ms
step:943/1395 train_time:118399ms step_avg:126.90ms
step:944/1395 train_time:118532ms step_avg:126.91ms
step:945/1395 train_time:118666ms step_avg:126.91ms
step:946/1395 train_time:118798ms step_avg:126.92ms
step:947/1395 train_time:118932ms step_avg:126.93ms
step:948/1395 train_time:119064ms step_avg:126.93ms
step:949/1395 train_time:119196ms step_avg:126.94ms
step:950/1395 train_time:119330ms step_avg:126.95ms
step:951/1395 train_time:119464ms step_avg:126.95ms
step:952/1395 train_time:119596ms step_avg:126.96ms
step:953/1395 train_time:119730ms step_avg:126.97ms
step:954/1395 train_time:119862ms step_avg:126.97ms
step:955/1395 train_time:119994ms step_avg:126.98ms
step:956/1395 train_time:120128ms step_avg:126.98ms
step:957/1395 train_time:120260ms step_avg:126.99ms
step:958/1395 train_time:120393ms step_avg:127.00ms
step:959/1395 train_time:120525ms step_avg:127.00ms
step:960/1395 train_time:120658ms step_avg:127.01ms
step:961/1395 train_time:120793ms step_avg:127.02ms
step:962/1395 train_time:120924ms step_avg:127.02ms
step:963/1395 train_time:121058ms step_avg:127.03ms
step:964/1395 train_time:121191ms step_avg:127.03ms
step:965/1395 train_time:121325ms step_avg:127.04ms
step:966/1395 train_time:121457ms step_avg:127.05ms
step:967/1395 train_time:121589ms step_avg:127.05ms
step:968/1395 train_time:121721ms step_avg:127.06ms
step:969/1395 train_time:121853ms step_avg:127.06ms
step:970/1395 train_time:121985ms step_avg:127.07ms
step:971/1395 train_time:122117ms step_avg:127.07ms
step:972/1395 train_time:122249ms step_avg:127.08ms
step:973/1395 train_time:122382ms step_avg:127.08ms
step:974/1395 train_time:122513ms step_avg:127.09ms
step:975/1395 train_time:122646ms step_avg:127.09ms
step:976/1395 train_time:122779ms step_avg:127.10ms
step:977/1395 train_time:122912ms step_avg:127.11ms
step:978/1395 train_time:123044ms step_avg:127.11ms
step:979/1395 train_time:123177ms step_avg:127.12ms
step:980/1395 train_time:123312ms step_avg:127.13ms
step:981/1395 train_time:123444ms step_avg:127.13ms
step:982/1395 train_time:123575ms step_avg:127.14ms
step:983/1395 train_time:123708ms step_avg:127.14ms
step:984/1395 train_time:123839ms step_avg:127.14ms
step:985/1395 train_time:123972ms step_avg:127.15ms
step:986/1395 train_time:124105ms step_avg:127.16ms
step:987/1395 train_time:124237ms step_avg:127.16ms
step:988/1395 train_time:124369ms step_avg:127.17ms
step:989/1395 train_time:124501ms step_avg:127.17ms
step:990/1395 train_time:124635ms step_avg:127.18ms
step:991/1395 train_time:124769ms step_avg:127.19ms
step:992/1395 train_time:124902ms step_avg:127.19ms
step:993/1395 train_time:125039ms step_avg:127.20ms
step:994/1395 train_time:125171ms step_avg:127.21ms
step:995/1395 train_time:125303ms step_avg:127.21ms
step:996/1395 train_time:125435ms step_avg:127.22ms
step:997/1395 train_time:125567ms step_avg:127.22ms
step:998/1395 train_time:125699ms step_avg:127.23ms
step:999/1395 train_time:125833ms step_avg:127.23ms
step:1000/1395 train_time:125965ms step_avg:127.24ms
step:1000/1395 val_loss:3.4128 train_time:126096ms step_avg:127.37ms
step:1001/1395 train_time:126117ms step_avg:127.26ms
step:1002/1395 train_time:126240ms step_avg:127.26ms
step:1003/1395 train_time:126374ms step_avg:127.26ms
step:1004/1395 train_time:126506ms step_avg:127.27ms
step:1005/1395 train_time:126640ms step_avg:127.28ms
step:1006/1395 train_time:126772ms step_avg:127.28ms
step:1007/1395 train_time:126903ms step_avg:127.29ms
step:1008/1395 train_time:127037ms step_avg:127.29ms
step:1009/1395 train_time:127172ms step_avg:127.30ms
step:1010/1395 train_time:127304ms step_avg:127.30ms
step:1011/1395 train_time:127438ms step_avg:127.31ms
step:1012/1395 train_time:127571ms step_avg:127.32ms
step:1013/1395 train_time:127704ms step_avg:127.32ms
step:1014/1395 train_time:127835ms step_avg:127.33ms
step:1015/1395 train_time:127967ms step_avg:127.33ms
step:1016/1395 train_time:128100ms step_avg:127.34ms
step:1017/1395 train_time:128233ms step_avg:127.34ms
step:1018/1395 train_time:128365ms step_avg:127.35ms
step:1019/1395 train_time:128499ms step_avg:127.35ms
step:1020/1395 train_time:128633ms step_avg:127.36ms
step:1021/1395 train_time:128765ms step_avg:127.36ms
step:1022/1395 train_time:128897ms step_avg:127.37ms
step:1023/1395 train_time:129029ms step_avg:127.37ms
step:1024/1395 train_time:129161ms step_avg:127.38ms
step:1025/1395 train_time:129294ms step_avg:127.38ms
step:1026/1395 train_time:129427ms step_avg:127.39ms
step:1027/1395 train_time:129560ms step_avg:127.39ms
step:1028/1395 train_time:129693ms step_avg:127.40ms
step:1029/1395 train_time:129826ms step_avg:127.41ms
step:1030/1395 train_time:129960ms step_avg:127.41ms
step:1031/1395 train_time:130093ms step_avg:127.42ms
step:1032/1395 train_time:130224ms step_avg:127.42ms
step:1033/1395 train_time:130357ms step_avg:127.43ms
step:1034/1395 train_time:130490ms step_avg:127.43ms
step:1035/1395 train_time:130624ms step_avg:127.44ms
step:1036/1395 train_time:130756ms step_avg:127.44ms
step:1037/1395 train_time:130889ms step_avg:127.45ms
step:1038/1395 train_time:131021ms step_avg:127.45ms
step:1039/1395 train_time:131153ms step_avg:127.46ms
step:1040/1395 train_time:131285ms step_avg:127.46ms
step:1041/1395 train_time:131419ms step_avg:127.47ms
step:1042/1395 train_time:131551ms step_avg:127.47ms
step:1043/1395 train_time:131684ms step_avg:127.48ms
step:1044/1395 train_time:131818ms step_avg:127.48ms
step:1045/1395 train_time:131951ms step_avg:127.49ms
step:1046/1395 train_time:132084ms step_avg:127.49ms
step:1047/1395 train_time:132215ms step_avg:127.50ms
step:1048/1395 train_time:132349ms step_avg:127.50ms
step:1049/1395 train_time:132481ms step_avg:127.51ms
step:1050/1395 train_time:132615ms step_avg:127.51ms
step:1051/1395 train_time:132748ms step_avg:127.52ms
step:1052/1395 train_time:132881ms step_avg:127.53ms
step:1053/1395 train_time:133014ms step_avg:127.53ms
step:1054/1395 train_time:133147ms step_avg:127.54ms
step:1055/1395 train_time:133279ms step_avg:127.54ms
step:1056/1395 train_time:133413ms step_avg:127.55ms
step:1057/1395 train_time:133545ms step_avg:127.55ms
step:1058/1395 train_time:133679ms step_avg:127.56ms
step:1059/1395 train_time:133813ms step_avg:127.56ms
step:1060/1395 train_time:133947ms step_avg:127.57ms
step:1061/1395 train_time:134080ms step_avg:127.57ms
step:1062/1395 train_time:134214ms step_avg:127.58ms
step:1063/1395 train_time:134347ms step_avg:127.58ms
step:1064/1395 train_time:134480ms step_avg:127.59ms
step:1065/1395 train_time:134612ms step_avg:127.59ms
step:1066/1395 train_time:134745ms step_avg:127.60ms
step:1067/1395 train_time:134878ms step_avg:127.60ms
step:1068/1395 train_time:135011ms step_avg:127.61ms
step:1069/1395 train_time:135145ms step_avg:127.62ms
step:1070/1395 train_time:135276ms step_avg:127.62ms
step:1071/1395 train_time:135410ms step_avg:127.62ms
step:1072/1395 train_time:135542ms step_avg:127.63ms
step:1073/1395 train_time:135674ms step_avg:127.63ms
step:1074/1395 train_time:135806ms step_avg:127.64ms
step:1075/1395 train_time:135939ms step_avg:127.64ms
step:1076/1395 train_time:136072ms step_avg:127.65ms
step:1077/1395 train_time:136204ms step_avg:127.65ms
step:1078/1395 train_time:136337ms step_avg:127.66ms
step:1079/1395 train_time:136474ms step_avg:127.66ms
step:1080/1395 train_time:136606ms step_avg:127.67ms
step:1081/1395 train_time:136738ms step_avg:127.67ms
step:1082/1395 train_time:136870ms step_avg:127.68ms
step:1083/1395 train_time:137003ms step_avg:127.68ms
step:1084/1395 train_time:137137ms step_avg:127.69ms
step:1085/1395 train_time:137271ms step_avg:127.69ms
step:1086/1395 train_time:137403ms step_avg:127.70ms
step:1087/1395 train_time:137536ms step_avg:127.70ms
step:1088/1395 train_time:137669ms step_avg:127.71ms
step:1089/1395 train_time:137803ms step_avg:127.71ms
step:1090/1395 train_time:137938ms step_avg:127.72ms
step:1091/1395 train_time:138071ms step_avg:127.73ms
step:1092/1395 train_time:138203ms step_avg:127.73ms
step:1093/1395 train_time:138335ms step_avg:127.73ms
step:1094/1395 train_time:138469ms step_avg:127.74ms
step:1095/1395 train_time:138602ms step_avg:127.74ms
step:1096/1395 train_time:138737ms step_avg:127.75ms
step:1097/1395 train_time:138869ms step_avg:127.75ms
step:1098/1395 train_time:139003ms step_avg:127.76ms
step:1099/1395 train_time:139135ms step_avg:127.76ms
step:1100/1395 train_time:139267ms step_avg:127.77ms
step:1101/1395 train_time:139401ms step_avg:127.77ms
step:1102/1395 train_time:139534ms step_avg:127.78ms
step:1103/1395 train_time:139667ms step_avg:127.78ms
step:1104/1395 train_time:139801ms step_avg:127.79ms
step:1105/1395 train_time:139935ms step_avg:127.79ms
step:1106/1395 train_time:140069ms step_avg:127.80ms
step:1107/1395 train_time:140201ms step_avg:127.80ms
step:1108/1395 train_time:140336ms step_avg:127.81ms
step:1109/1395 train_time:140467ms step_avg:127.81ms
step:1110/1395 train_time:140601ms step_avg:127.82ms
step:1111/1395 train_time:140735ms step_avg:127.82ms
step:1112/1395 train_time:140867ms step_avg:127.83ms
step:1113/1395 train_time:141003ms step_avg:127.84ms
step:1114/1395 train_time:141137ms step_avg:127.84ms
step:1115/1395 train_time:141270ms step_avg:127.85ms
step:1116/1395 train_time:141402ms step_avg:127.85ms
step:1117/1395 train_time:141536ms step_avg:127.86ms
step:1118/1395 train_time:141670ms step_avg:127.86ms
step:1119/1395 train_time:141802ms step_avg:127.86ms
step:1120/1395 train_time:141934ms step_avg:127.87ms
step:1121/1395 train_time:142070ms step_avg:127.88ms
step:1122/1395 train_time:142202ms step_avg:127.88ms
step:1123/1395 train_time:142334ms step_avg:127.88ms
step:1124/1395 train_time:142468ms step_avg:127.89ms
step:1125/1395 train_time:142600ms step_avg:127.89ms
step:1125/1395 val_loss:3.3614 train_time:142732ms step_avg:128.01ms
step:1126/1395 train_time:142754ms step_avg:127.92ms
step:1127/1395 train_time:142875ms step_avg:127.91ms
step:1128/1395 train_time:143008ms step_avg:127.91ms
step:1129/1395 train_time:143141ms step_avg:127.92ms
step:1130/1395 train_time:143273ms step_avg:127.92ms
step:1131/1395 train_time:143405ms step_avg:127.93ms
step:1132/1395 train_time:143537ms step_avg:127.93ms
step:1133/1395 train_time:143669ms step_avg:127.93ms
step:1134/1395 train_time:143805ms step_avg:127.94ms
step:1135/1395 train_time:143939ms step_avg:127.95ms
step:1136/1395 train_time:144075ms step_avg:127.95ms
step:1137/1395 train_time:144207ms step_avg:127.96ms
step:1138/1395 train_time:144340ms step_avg:127.96ms
step:1139/1395 train_time:144474ms step_avg:127.97ms
step:1140/1395 train_time:144609ms step_avg:127.97ms
step:1141/1395 train_time:144742ms step_avg:127.98ms
step:1142/1395 train_time:144877ms step_avg:127.98ms
step:1143/1395 train_time:145015ms step_avg:127.99ms
step:1144/1395 train_time:145150ms step_avg:128.00ms
step:1145/1395 train_time:145283ms step_avg:128.00ms
step:1146/1395 train_time:145417ms step_avg:128.01ms
step:1147/1395 train_time:145551ms step_avg:128.01ms
step:1148/1395 train_time:145684ms step_avg:128.02ms
step:1149/1395 train_time:145817ms step_avg:128.02ms
step:1150/1395 train_time:145952ms step_avg:128.03ms
step:1151/1395 train_time:146087ms step_avg:128.03ms
step:1152/1395 train_time:146220ms step_avg:128.04ms
step:1153/1395 train_time:146356ms step_avg:128.05ms
step:1154/1395 train_time:146492ms step_avg:128.05ms
step:1155/1395 train_time:146625ms step_avg:128.06ms
step:1156/1395 train_time:146762ms step_avg:128.06ms
step:1157/1395 train_time:146896ms step_avg:128.07ms
step:1158/1395 train_time:147030ms step_avg:128.08ms
step:1159/1395 train_time:147164ms step_avg:128.08ms
step:1160/1395 train_time:147297ms step_avg:128.08ms
step:1161/1395 train_time:147432ms step_avg:128.09ms
step:1162/1395 train_time:147565ms step_avg:128.09ms
step:1163/1395 train_time:147699ms step_avg:128.10ms
step:1164/1395 train_time:147833ms step_avg:128.10ms
step:1165/1395 train_time:147967ms step_avg:128.11ms
step:1166/1395 train_time:148101ms step_avg:128.12ms
step:1167/1395 train_time:148235ms step_avg:128.12ms
step:1168/1395 train_time:148369ms step_avg:128.13ms
step:1169/1395 train_time:148503ms step_avg:128.13ms
step:1170/1395 train_time:148637ms step_avg:128.14ms
step:1171/1395 train_time:148773ms step_avg:128.14ms
step:1172/1395 train_time:148907ms step_avg:128.15ms
step:1173/1395 train_time:149040ms step_avg:128.15ms
step:1174/1395 train_time:149179ms step_avg:128.16ms
step:1175/1395 train_time:149313ms step_avg:128.17ms
step:1176/1395 train_time:149449ms step_avg:128.17ms
step:1177/1395 train_time:149585ms step_avg:128.18ms
step:1178/1395 train_time:149719ms step_avg:128.18ms
step:1179/1395 train_time:149853ms step_avg:128.19ms
step:1180/1395 train_time:149988ms step_avg:128.19ms
step:1181/1395 train_time:150126ms step_avg:128.20ms
step:1182/1395 train_time:150259ms step_avg:128.21ms
step:1183/1395 train_time:150395ms step_avg:128.21ms
step:1184/1395 train_time:150530ms step_avg:128.22ms
step:1185/1395 train_time:150665ms step_avg:128.23ms
step:1186/1395 train_time:150798ms step_avg:128.23ms
step:1187/1395 train_time:150936ms step_avg:128.24ms
step:1188/1395 train_time:151071ms step_avg:128.24ms
step:1189/1395 train_time:151205ms step_avg:128.25ms
step:1190/1395 train_time:151338ms step_avg:128.25ms
step:1191/1395 train_time:151473ms step_avg:128.26ms
step:1192/1395 train_time:151606ms step_avg:128.26ms
step:1193/1395 train_time:151741ms step_avg:128.27ms
step:1194/1395 train_time:151875ms step_avg:128.27ms
step:1195/1395 train_time:152010ms step_avg:128.28ms
step:1196/1395 train_time:152144ms step_avg:128.28ms
step:1197/1395 train_time:152279ms step_avg:128.29ms
step:1198/1395 train_time:152415ms step_avg:128.30ms
step:1199/1395 train_time:152549ms step_avg:128.30ms
step:1200/1395 train_time:152683ms step_avg:128.30ms
step:1201/1395 train_time:152816ms step_avg:128.31ms
step:1202/1395 train_time:152954ms step_avg:128.32ms
step:1203/1395 train_time:153093ms step_avg:128.33ms
step:1204/1395 train_time:153228ms step_avg:128.33ms
step:1205/1395 train_time:153364ms step_avg:128.34ms
step:1206/1395 train_time:153499ms step_avg:128.34ms
step:1207/1395 train_time:153632ms step_avg:128.35ms
step:1208/1395 train_time:153766ms step_avg:128.35ms
step:1209/1395 train_time:153900ms step_avg:128.36ms
step:1210/1395 train_time:154036ms step_avg:128.36ms
step:1211/1395 train_time:154170ms step_avg:128.37ms
step:1212/1395 train_time:154304ms step_avg:128.37ms
step:1213/1395 train_time:154438ms step_avg:128.38ms
step:1214/1395 train_time:154573ms step_avg:128.38ms
step:1215/1395 train_time:154709ms step_avg:128.39ms
step:1216/1395 train_time:154842ms step_avg:128.39ms
step:1217/1395 train_time:154976ms step_avg:128.40ms
step:1218/1395 train_time:155109ms step_avg:128.40ms
step:1219/1395 train_time:155243ms step_avg:128.41ms
step:1220/1395 train_time:155377ms step_avg:128.41ms
step:1221/1395 train_time:155511ms step_avg:128.42ms
step:1222/1395 train_time:155647ms step_avg:128.42ms
step:1223/1395 train_time:155780ms step_avg:128.43ms
step:1224/1395 train_time:155915ms step_avg:128.43ms
step:1225/1395 train_time:156051ms step_avg:128.44ms
step:1226/1395 train_time:156185ms step_avg:128.44ms
step:1227/1395 train_time:156318ms step_avg:128.45ms
step:1228/1395 train_time:156453ms step_avg:128.45ms
step:1229/1395 train_time:156587ms step_avg:128.45ms
step:1230/1395 train_time:156721ms step_avg:128.46ms
step:1231/1395 train_time:156857ms step_avg:128.47ms
step:1232/1395 train_time:156993ms step_avg:128.47ms
step:1233/1395 train_time:157126ms step_avg:128.48ms
step:1234/1395 train_time:157260ms step_avg:128.48ms
step:1235/1395 train_time:157394ms step_avg:128.48ms
step:1236/1395 train_time:157530ms step_avg:128.49ms
step:1237/1395 train_time:157663ms step_avg:128.50ms
step:1238/1395 train_time:157801ms step_avg:128.50ms
step:1239/1395 train_time:157935ms step_avg:128.51ms
step:1240/1395 train_time:158070ms step_avg:128.51ms
step:1241/1395 train_time:158206ms step_avg:128.52ms
step:1242/1395 train_time:158339ms step_avg:128.52ms
step:1243/1395 train_time:158474ms step_avg:128.53ms
step:1244/1395 train_time:158608ms step_avg:128.53ms
step:1245/1395 train_time:158742ms step_avg:128.54ms
step:1246/1395 train_time:158876ms step_avg:128.54ms
step:1247/1395 train_time:159011ms step_avg:128.55ms
step:1248/1395 train_time:159145ms step_avg:128.55ms
step:1249/1395 train_time:159278ms step_avg:128.55ms
step:1250/1395 train_time:159412ms step_avg:128.56ms
step:1250/1395 val_loss:3.3151 train_time:159545ms step_avg:128.67ms
step:1251/1395 train_time:159566ms step_avg:128.58ms
step:1252/1395 train_time:159694ms step_avg:128.58ms
step:1253/1395 train_time:159827ms step_avg:128.58ms
step:1254/1395 train_time:159960ms step_avg:128.59ms
step:1255/1395 train_time:160099ms step_avg:128.59ms
step:1256/1395 train_time:160233ms step_avg:128.60ms
step:1257/1395 train_time:160367ms step_avg:128.60ms
step:1258/1395 train_time:160502ms step_avg:128.61ms
step:1259/1395 train_time:160639ms step_avg:128.61ms
step:1260/1395 train_time:160774ms step_avg:128.62ms
step:1261/1395 train_time:160907ms step_avg:128.62ms
step:1262/1395 train_time:161043ms step_avg:128.63ms
step:1263/1395 train_time:161177ms step_avg:128.63ms
step:1264/1395 train_time:161310ms step_avg:128.64ms
step:1265/1395 train_time:161444ms step_avg:128.64ms
step:1266/1395 train_time:161578ms step_avg:128.65ms
step:1267/1395 train_time:161712ms step_avg:128.65ms
step:1268/1395 train_time:161846ms step_avg:128.65ms
step:1269/1395 train_time:161983ms step_avg:128.66ms
step:1270/1395 train_time:162117ms step_avg:128.66ms
step:1271/1395 train_time:162251ms step_avg:128.67ms
step:1272/1395 train_time:162385ms step_avg:128.67ms
step:1273/1395 train_time:162518ms step_avg:128.68ms
step:1274/1395 train_time:162652ms step_avg:128.68ms
step:1275/1395 train_time:162787ms step_avg:128.69ms
step:1276/1395 train_time:162921ms step_avg:128.69ms
step:1277/1395 train_time:163055ms step_avg:128.69ms
step:1278/1395 train_time:163189ms step_avg:128.70ms
step:1279/1395 train_time:163323ms step_avg:128.70ms
step:1280/1395 train_time:163458ms step_avg:128.71ms
step:1281/1395 train_time:163592ms step_avg:128.71ms
step:1282/1395 train_time:163726ms step_avg:128.72ms
step:1283/1395 train_time:163860ms step_avg:128.72ms
step:1284/1395 train_time:163996ms step_avg:128.73ms
step:1285/1395 train_time:164130ms step_avg:128.73ms
step:1286/1395 train_time:164264ms step_avg:128.73ms
step:1287/1395 train_time:164399ms step_avg:128.74ms
step:1288/1395 train_time:164534ms step_avg:128.74ms
step:1289/1395 train_time:164670ms step_avg:128.75ms
step:1290/1395 train_time:164806ms step_avg:128.75ms
step:1291/1395 train_time:164942ms step_avg:128.76ms
step:1292/1395 train_time:165076ms step_avg:128.76ms
step:1293/1395 train_time:165213ms step_avg:128.77ms
step:1294/1395 train_time:165346ms step_avg:128.77ms
step:1295/1395 train_time:165482ms step_avg:128.78ms
step:1296/1395 train_time:165618ms step_avg:128.79ms
step:1297/1395 train_time:165754ms step_avg:128.79ms
step:1298/1395 train_time:165887ms step_avg:128.79ms
step:1299/1395 train_time:166021ms step_avg:128.80ms
step:1300/1395 train_time:166155ms step_avg:128.80ms
step:1301/1395 train_time:166289ms step_avg:128.81ms
step:1302/1395 train_time:166423ms step_avg:128.81ms
step:1303/1395 train_time:166558ms step_avg:128.81ms
step:1304/1395 train_time:166695ms step_avg:128.82ms
step:1305/1395 train_time:166830ms step_avg:128.83ms
step:1306/1395 train_time:166964ms step_avg:128.83ms
step:1307/1395 train_time:167097ms step_avg:128.83ms
step:1308/1395 train_time:167233ms step_avg:128.84ms
step:1309/1395 train_time:167367ms step_avg:128.84ms
step:1310/1395 train_time:167501ms step_avg:128.85ms
step:1311/1395 train_time:167635ms step_avg:128.85ms
step:1312/1395 train_time:167768ms step_avg:128.85ms
step:1313/1395 train_time:167902ms step_avg:128.86ms
step:1314/1395 train_time:168036ms step_avg:128.86ms
step:1315/1395 train_time:168171ms step_avg:128.87ms
step:1316/1395 train_time:168304ms step_avg:128.87ms
step:1317/1395 train_time:168438ms step_avg:128.87ms
step:1318/1395 train_time:168573ms step_avg:128.88ms
step:1319/1395 train_time:168709ms step_avg:128.88ms
step:1320/1395 train_time:168843ms step_avg:128.89ms
step:1321/1395 train_time:168977ms step_avg:128.89ms
step:1322/1395 train_time:169114ms step_avg:128.90ms
step:1323/1395 train_time:169247ms step_avg:128.90ms
step:1324/1395 train_time:169380ms step_avg:128.90ms
step:1325/1395 train_time:169516ms step_avg:128.91ms
step:1326/1395 train_time:169651ms step_avg:128.91ms
step:1327/1395 train_time:169785ms step_avg:128.92ms
step:1328/1395 train_time:169918ms step_avg:128.92ms
step:1329/1395 train_time:170057ms step_avg:128.93ms
step:1330/1395 train_time:170192ms step_avg:128.93ms
step:1331/1395 train_time:170329ms step_avg:128.94ms
step:1332/1395 train_time:170465ms step_avg:128.95ms
step:1333/1395 train_time:170600ms step_avg:128.95ms
step:1334/1395 train_time:170735ms step_avg:128.95ms
step:1335/1395 train_time:170867ms step_avg:128.96ms
step:1336/1395 train_time:171004ms step_avg:128.96ms
step:1337/1395 train_time:171138ms step_avg:128.97ms
step:1338/1395 train_time:171272ms step_avg:128.97ms
step:1339/1395 train_time:171407ms step_avg:128.97ms
step:1340/1395 train_time:171544ms step_avg:128.98ms
step:1341/1395 train_time:171678ms step_avg:128.98ms
step:1342/1395 train_time:171812ms step_avg:128.99ms
step:1343/1395 train_time:171946ms step_avg:128.99ms
step:1344/1395 train_time:172079ms step_avg:129.00ms
step:1345/1395 train_time:172215ms step_avg:129.00ms
step:1346/1395 train_time:172350ms step_avg:129.00ms
step:1347/1395 train_time:172485ms step_avg:129.01ms
step:1348/1395 train_time:172620ms step_avg:129.01ms
step:1349/1395 train_time:172756ms step_avg:129.02ms
step:1350/1395 train_time:172890ms step_avg:129.02ms
step:1351/1395 train_time:173026ms step_avg:129.03ms
step:1352/1395 train_time:173164ms step_avg:129.03ms
step:1353/1395 train_time:173302ms step_avg:129.04ms
step:1354/1395 train_time:173437ms step_avg:129.05ms
step:1355/1395 train_time:173572ms step_avg:129.05ms
step:1356/1395 train_time:173706ms step_avg:129.05ms
step:1357/1395 train_time:173841ms step_avg:129.06ms
step:1358/1395 train_time:173979ms step_avg:129.06ms
step:1359/1395 train_time:174114ms step_avg:129.07ms
step:1360/1395 train_time:174250ms step_avg:129.07ms
step:1361/1395 train_time:174385ms step_avg:129.08ms
step:1362/1395 train_time:174523ms step_avg:129.08ms
step:1363/1395 train_time:174660ms step_avg:129.09ms
step:1364/1395 train_time:174797ms step_avg:129.10ms
step:1365/1395 train_time:174932ms step_avg:129.10ms
step:1366/1395 train_time:175066ms step_avg:129.10ms
step:1367/1395 train_time:175202ms step_avg:129.11ms
step:1368/1395 train_time:175338ms step_avg:129.11ms
step:1369/1395 train_time:175478ms step_avg:129.12ms
step:1370/1395 train_time:175615ms step_avg:129.13ms
step:1371/1395 train_time:175752ms step_avg:129.13ms
step:1372/1395 train_time:175888ms step_avg:129.14ms
step:1373/1395 train_time:176024ms step_avg:129.14ms
step:1374/1395 train_time:176161ms step_avg:129.15ms
step:1375/1395 train_time:176296ms step_avg:129.15ms
step:1375/1395 val_loss:3.2805 train_time:176430ms step_avg:129.25ms
step:1376/1395 train_time:176451ms step_avg:129.17ms
step:1377/1395 train_time:176573ms step_avg:129.17ms
step:1378/1395 train_time:176709ms step_avg:129.17ms
step:1379/1395 train_time:176843ms step_avg:129.18ms
step:1380/1395 train_time:176980ms step_avg:129.18ms
step:1381/1395 train_time:177116ms step_avg:129.19ms
step:1382/1395 train_time:177251ms step_avg:129.19ms
step:1383/1395 train_time:177388ms step_avg:129.20ms
step:1384/1395 train_time:177526ms step_avg:129.20ms
step:1385/1395 train_time:177661ms step_avg:129.21ms
step:1386/1395 train_time:177795ms step_avg:129.21ms
step:1387/1395 train_time:177931ms step_avg:129.22ms
step:1388/1395 train_time:178067ms step_avg:129.22ms
step:1389/1395 train_time:178203ms step_avg:129.23ms
step:1390/1395 train_time:178338ms step_avg:129.23ms
step:1391/1395 train_time:178473ms step_avg:129.23ms
step:1392/1395 train_time:178609ms step_avg:129.24ms
step:1393/1395 train_time:178743ms step_avg:129.24ms
step:1394/1395 train_time:178879ms step_avg:129.25ms
step:1395/1395 train_time:179013ms step_avg:129.25ms
step:1395/1395 val_loss:3.2764 train_time:179147ms step_avg:129.35ms
peak memory allocated: 37653 MiB reserved: 39236 MiB
