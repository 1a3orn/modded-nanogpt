import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True

# -----------------------------------------------------------------------------
# Custom operators

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

def lm_head_fp8(x: Tensor, w: Tensor) -> Tensor:
    _x = x.flatten(0, -2)
    out: Tensor = torch.ops.nanogpt.mm(_x, w, x_s=2.0, w_s=32.0, grad_s=2.0**29)[0]
    return out.reshape(*x.shape[:-1], -1)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev():
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int):
        super().__init__(in_features, out_features, bias=False)

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x):
        return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len=65536):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        # self.c_q = CastedLinear(dim, dim)
        # self.c_k = CastedLinear(dim, dim)
        # self.c_v = CastedLinear(dim, dim)
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std
        self.qkv_w = nn.Parameter(torch.empty(3, dim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads) # dim // num_heads = head_dim
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # Set attention scale such that the minimum attainable attention entropy
        # (but not necessary the attention entropy itself) is close to 0. By @leloykun
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        # q = self.c_q(x).view(B, T, self.num_heads, -1)
        # k = self.c_k(x).view(B, T, self.num_heads, -1)
        # v = self.c_v(x).view(B, T, self.num_heads, -1)
        # qkv_weight = torch.cat([self.c_q.weight, self.c_k.weight, self.c_v.weight], dim=0).type_as(x)
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3*self.num_heads, -1).chunk(3, dim=-2)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, model_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(model_dim, num_heads, layer_idx) if layer_idx != 7 else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, ve, x0, block_mask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, num_embeddings: int, embedding_dim: int):
        super().__init__()
        self.embed = nn.ModuleList([nn.Embedding(num_embeddings, embedding_dim) for _ in range(3)])

    def forward(self, input_seq) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2], None, None, None, None, None, None, ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual learning
        self.value_embeds = ValueEmbedding(vocab_size, model_dim)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128))
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        assert input_seq.ndim == 1
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        docs = (input_seq == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        def create_doc_swc_block_masks(sliding_window_num_blocks: Tensor):
            kv_idx = block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & document_bm
            full_bm  = causal_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            def build_bm(sw_num_blocks: Tensor) -> BlockMask:
                return BlockMask.from_kv_blocks(
                    torch.clamp_max(kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                    kv_indices,
                    torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                    full_kv_indices,
                    BLOCK_SIZE=BLOCK_SIZE,
                    mask_mod=document_causal,
                )
            return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

        # Long-short SWA block masks by @leloykun & @YouJiacheng
        long_bm, short_bm = create_doc_swc_block_masks(sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = lm_head_fp8(x, self.lm_head.weight) if self.training else self.lm_head(x)
        # @Grad62304977 added tanh softcapping, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use cycle(files) if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    batch_size = 8*64*1024 # batch size in tokens
    num_iterations = 1395 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 64*1024 # FlexAttention sequence length
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
             if console:
                 print(s)
             print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_loader = distributed_data_generator(args.train_files, args.batch_size, rank, world_size)

model = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim >= 2]
embed_params = [model.embed.weight, *model.value_embeds.parameters()]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), fused=True, eps=1e-10)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(it: int):
    t = 1 - it / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_bs = world_size * args.seq_len
        assert args.val_tokens % val_bs == 0
        val_steps = args.val_tokens // val_bs
        val_loader = distributed_data_generator(args.val_files, val_bs, rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION BEGIN -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB"
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250110+cu126 compiled for CUDA 12.6
Thu Jan 16 21:17:01 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   38C    P0             121W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   28C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0             118W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   38C    P0             119W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   37C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1395 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1395 train_time:25857ms step_avg:nanms
step:2/1395 train_time:26367ms step_avg:nanms
step:3/1395 train_time:26488ms step_avg:nanms
step:4/1395 train_time:26608ms step_avg:nanms
step:5/1395 train_time:26729ms step_avg:nanms
step:6/1395 train_time:26850ms step_avg:nanms
step:7/1395 train_time:26972ms step_avg:nanms
step:8/1395 train_time:27093ms step_avg:nanms
step:9/1395 train_time:27214ms step_avg:nanms
step:10/1395 train_time:27338ms step_avg:nanms
step:11/1395 train_time:125ms step_avg:nanms
step:12/1395 train_time:250ms step_avg:nanms
step:13/1395 train_time:371ms step_avg:123.74ms
step:14/1395 train_time:494ms step_avg:123.55ms
step:15/1395 train_time:617ms step_avg:123.38ms
step:16/1395 train_time:739ms step_avg:123.09ms
step:17/1395 train_time:860ms step_avg:122.87ms
step:18/1395 train_time:981ms step_avg:122.67ms
step:19/1395 train_time:1104ms step_avg:122.67ms
step:20/1395 train_time:1227ms step_avg:122.71ms
step:21/1395 train_time:1350ms step_avg:122.70ms
step:22/1395 train_time:1472ms step_avg:122.66ms
step:23/1395 train_time:1596ms step_avg:122.74ms
step:24/1395 train_time:1717ms step_avg:122.66ms
step:25/1395 train_time:1839ms step_avg:122.62ms
step:26/1395 train_time:1961ms step_avg:122.55ms
step:27/1395 train_time:2083ms step_avg:122.51ms
step:28/1395 train_time:2206ms step_avg:122.57ms
step:29/1395 train_time:2328ms step_avg:122.53ms
step:30/1395 train_time:2450ms step_avg:122.52ms
step:31/1395 train_time:2573ms step_avg:122.53ms
step:32/1395 train_time:2695ms step_avg:122.50ms
step:33/1395 train_time:2819ms step_avg:122.55ms
step:34/1395 train_time:2940ms step_avg:122.49ms
step:35/1395 train_time:3062ms step_avg:122.49ms
step:36/1395 train_time:3185ms step_avg:122.49ms
step:37/1395 train_time:3306ms step_avg:122.45ms
step:38/1395 train_time:3428ms step_avg:122.43ms
step:39/1395 train_time:3549ms step_avg:122.38ms
step:40/1395 train_time:3671ms step_avg:122.36ms
step:41/1395 train_time:3794ms step_avg:122.38ms
step:42/1395 train_time:3931ms step_avg:122.85ms
step:43/1395 train_time:4038ms step_avg:122.35ms
step:44/1395 train_time:4159ms step_avg:122.32ms
step:45/1395 train_time:4282ms step_avg:122.33ms
step:46/1395 train_time:4404ms step_avg:122.33ms
step:47/1395 train_time:4526ms step_avg:122.32ms
step:48/1395 train_time:4647ms step_avg:122.29ms
step:49/1395 train_time:4770ms step_avg:122.30ms
step:50/1395 train_time:4894ms step_avg:122.34ms
step:51/1395 train_time:5015ms step_avg:122.33ms
step:52/1395 train_time:5137ms step_avg:122.32ms
step:53/1395 train_time:5259ms step_avg:122.31ms
step:54/1395 train_time:5381ms step_avg:122.31ms
step:55/1395 train_time:5503ms step_avg:122.30ms
step:56/1395 train_time:5626ms step_avg:122.30ms
step:57/1395 train_time:5748ms step_avg:122.30ms
step:58/1395 train_time:5870ms step_avg:122.30ms
step:59/1395 train_time:5993ms step_avg:122.31ms
step:60/1395 train_time:6116ms step_avg:122.31ms
step:61/1395 train_time:6238ms step_avg:122.32ms
step:62/1395 train_time:6359ms step_avg:122.29ms
step:63/1395 train_time:6482ms step_avg:122.30ms
step:64/1395 train_time:6604ms step_avg:122.30ms
step:65/1395 train_time:6726ms step_avg:122.29ms
step:66/1395 train_time:6848ms step_avg:122.29ms
step:67/1395 train_time:6971ms step_avg:122.30ms
step:68/1395 train_time:7094ms step_avg:122.30ms
step:69/1395 train_time:7216ms step_avg:122.31ms
step:70/1395 train_time:7339ms step_avg:122.31ms
step:71/1395 train_time:7461ms step_avg:122.31ms
step:72/1395 train_time:7583ms step_avg:122.30ms
step:73/1395 train_time:7706ms step_avg:122.31ms
step:74/1395 train_time:7827ms step_avg:122.30ms
step:75/1395 train_time:7949ms step_avg:122.29ms
step:76/1395 train_time:8071ms step_avg:122.29ms
step:77/1395 train_time:8193ms step_avg:122.28ms
step:78/1395 train_time:8315ms step_avg:122.29ms
step:79/1395 train_time:8437ms step_avg:122.28ms
step:80/1395 train_time:8561ms step_avg:122.30ms
step:81/1395 train_time:8683ms step_avg:122.30ms
step:82/1395 train_time:8807ms step_avg:122.32ms
step:83/1395 train_time:8932ms step_avg:122.35ms
step:84/1395 train_time:9054ms step_avg:122.36ms
step:85/1395 train_time:9176ms step_avg:122.35ms
step:86/1395 train_time:9298ms step_avg:122.34ms
step:87/1395 train_time:9420ms step_avg:122.34ms
step:88/1395 train_time:9542ms step_avg:122.34ms
step:89/1395 train_time:9665ms step_avg:122.34ms
step:90/1395 train_time:9790ms step_avg:122.38ms
step:91/1395 train_time:9912ms step_avg:122.38ms
step:92/1395 train_time:10037ms step_avg:122.40ms
step:93/1395 train_time:10160ms step_avg:122.41ms
step:94/1395 train_time:10283ms step_avg:122.42ms
step:95/1395 train_time:10404ms step_avg:122.40ms
step:96/1395 train_time:10526ms step_avg:122.40ms
step:97/1395 train_time:10648ms step_avg:122.39ms
step:98/1395 train_time:10771ms step_avg:122.39ms
step:99/1395 train_time:10894ms step_avg:122.40ms
step:100/1395 train_time:11016ms step_avg:122.40ms
step:101/1395 train_time:11139ms step_avg:122.41ms
step:102/1395 train_time:11261ms step_avg:122.40ms
step:103/1395 train_time:11383ms step_avg:122.40ms
step:104/1395 train_time:11503ms step_avg:122.38ms
step:105/1395 train_time:11625ms step_avg:122.37ms
step:106/1395 train_time:11748ms step_avg:122.37ms
step:107/1395 train_time:11870ms step_avg:122.37ms
step:108/1395 train_time:11994ms step_avg:122.39ms
step:109/1395 train_time:12118ms step_avg:122.40ms
step:110/1395 train_time:12241ms step_avg:122.41ms
step:111/1395 train_time:12363ms step_avg:122.41ms
step:112/1395 train_time:12486ms step_avg:122.41ms
step:113/1395 train_time:12608ms step_avg:122.41ms
step:114/1395 train_time:12731ms step_avg:122.41ms
step:115/1395 train_time:12854ms step_avg:122.42ms
step:116/1395 train_time:12978ms step_avg:122.43ms
step:117/1395 train_time:13102ms step_avg:122.45ms
step:118/1395 train_time:13223ms step_avg:122.44ms
step:119/1395 train_time:13346ms step_avg:122.44ms
step:120/1395 train_time:13469ms step_avg:122.44ms
step:121/1395 train_time:13593ms step_avg:122.46ms
step:122/1395 train_time:13715ms step_avg:122.46ms
step:123/1395 train_time:13837ms step_avg:122.45ms
step:124/1395 train_time:13961ms step_avg:122.46ms
step:125/1395 train_time:14085ms step_avg:122.48ms
step:125/1395 val_loss:4.4046 train_time:14207ms step_avg:123.54ms
step:126/1395 train_time:14228ms step_avg:122.66ms
step:127/1395 train_time:14348ms step_avg:122.63ms
step:128/1395 train_time:14472ms step_avg:122.64ms
step:129/1395 train_time:14594ms step_avg:122.64ms
step:130/1395 train_time:14718ms step_avg:122.65ms
step:131/1395 train_time:14839ms step_avg:122.64ms
step:132/1395 train_time:14962ms step_avg:122.64ms
step:133/1395 train_time:15085ms step_avg:122.64ms
step:134/1395 train_time:15207ms step_avg:122.63ms
step:135/1395 train_time:15330ms step_avg:122.64ms
step:136/1395 train_time:15456ms step_avg:122.67ms
step:137/1395 train_time:15579ms step_avg:122.67ms
step:138/1395 train_time:15702ms step_avg:122.68ms
step:139/1395 train_time:15825ms step_avg:122.67ms
step:140/1395 train_time:15947ms step_avg:122.67ms
step:141/1395 train_time:16070ms step_avg:122.67ms
step:142/1395 train_time:16192ms step_avg:122.67ms
step:143/1395 train_time:16315ms step_avg:122.67ms
step:144/1395 train_time:16438ms step_avg:122.67ms
step:145/1395 train_time:16561ms step_avg:122.68ms
step:146/1395 train_time:16683ms step_avg:122.67ms
step:147/1395 train_time:16807ms step_avg:122.68ms
step:148/1395 train_time:16930ms step_avg:122.68ms
step:149/1395 train_time:17055ms step_avg:122.70ms
step:150/1395 train_time:17177ms step_avg:122.69ms
step:151/1395 train_time:17299ms step_avg:122.69ms
step:152/1395 train_time:17422ms step_avg:122.69ms
step:153/1395 train_time:17546ms step_avg:122.70ms
step:154/1395 train_time:17669ms step_avg:122.70ms
step:155/1395 train_time:17792ms step_avg:122.70ms
step:156/1395 train_time:17914ms step_avg:122.70ms
step:157/1395 train_time:18036ms step_avg:122.70ms
step:158/1395 train_time:18160ms step_avg:122.70ms
step:159/1395 train_time:18282ms step_avg:122.70ms
step:160/1395 train_time:18406ms step_avg:122.71ms
step:161/1395 train_time:18530ms step_avg:122.72ms
step:162/1395 train_time:18656ms step_avg:122.74ms
step:163/1395 train_time:18778ms step_avg:122.73ms
step:164/1395 train_time:18901ms step_avg:122.73ms
step:165/1395 train_time:19023ms step_avg:122.73ms
step:166/1395 train_time:19147ms step_avg:122.74ms
step:167/1395 train_time:19271ms step_avg:122.74ms
step:168/1395 train_time:19393ms step_avg:122.74ms
step:169/1395 train_time:19516ms step_avg:122.74ms
step:170/1395 train_time:19640ms step_avg:122.75ms
step:171/1395 train_time:19762ms step_avg:122.75ms
step:172/1395 train_time:19884ms step_avg:122.74ms
step:173/1395 train_time:20007ms step_avg:122.74ms
step:174/1395 train_time:20129ms step_avg:122.74ms
step:175/1395 train_time:20252ms step_avg:122.74ms
step:176/1395 train_time:20375ms step_avg:122.74ms
step:177/1395 train_time:20498ms step_avg:122.74ms
step:178/1395 train_time:20622ms step_avg:122.75ms
step:179/1395 train_time:20744ms step_avg:122.75ms
step:180/1395 train_time:20868ms step_avg:122.76ms
step:181/1395 train_time:20993ms step_avg:122.77ms
step:182/1395 train_time:21115ms step_avg:122.76ms
step:183/1395 train_time:21238ms step_avg:122.76ms
step:184/1395 train_time:21360ms step_avg:122.76ms
step:185/1395 train_time:21483ms step_avg:122.76ms
step:186/1395 train_time:21606ms step_avg:122.76ms
step:187/1395 train_time:21728ms step_avg:122.76ms
step:188/1395 train_time:21850ms step_avg:122.75ms
step:189/1395 train_time:21973ms step_avg:122.76ms
step:190/1395 train_time:22097ms step_avg:122.76ms
step:191/1395 train_time:22220ms step_avg:122.76ms
step:192/1395 train_time:22342ms step_avg:122.76ms
step:193/1395 train_time:22464ms step_avg:122.75ms
step:194/1395 train_time:22587ms step_avg:122.75ms
step:195/1395 train_time:22709ms step_avg:122.75ms
step:196/1395 train_time:22833ms step_avg:122.76ms
step:197/1395 train_time:22956ms step_avg:122.76ms
step:198/1395 train_time:23078ms step_avg:122.76ms
step:199/1395 train_time:23202ms step_avg:122.76ms
step:200/1395 train_time:23325ms step_avg:122.76ms
step:201/1395 train_time:23448ms step_avg:122.76ms
step:202/1395 train_time:23572ms step_avg:122.77ms
step:203/1395 train_time:23696ms step_avg:122.78ms
step:204/1395 train_time:23820ms step_avg:122.78ms
step:205/1395 train_time:23943ms step_avg:122.78ms
step:206/1395 train_time:24065ms step_avg:122.78ms
step:207/1395 train_time:24187ms step_avg:122.78ms
step:208/1395 train_time:24309ms step_avg:122.77ms
step:209/1395 train_time:24432ms step_avg:122.77ms
step:210/1395 train_time:24555ms step_avg:122.77ms
step:211/1395 train_time:24678ms step_avg:122.78ms
step:212/1395 train_time:24801ms step_avg:122.78ms
step:213/1395 train_time:24924ms step_avg:122.78ms
step:214/1395 train_time:25049ms step_avg:122.79ms
step:215/1395 train_time:25173ms step_avg:122.79ms
step:216/1395 train_time:25296ms step_avg:122.80ms
step:217/1395 train_time:25420ms step_avg:122.80ms
step:218/1395 train_time:25543ms step_avg:122.81ms
step:219/1395 train_time:25667ms step_avg:122.81ms
step:220/1395 train_time:25791ms step_avg:122.82ms
step:221/1395 train_time:25915ms step_avg:122.82ms
step:222/1395 train_time:26039ms step_avg:122.82ms
step:223/1395 train_time:26162ms step_avg:122.83ms
step:224/1395 train_time:26286ms step_avg:122.83ms
step:225/1395 train_time:26408ms step_avg:122.83ms
step:226/1395 train_time:26531ms step_avg:122.83ms
step:227/1395 train_time:26654ms step_avg:122.83ms
step:228/1395 train_time:26779ms step_avg:122.84ms
step:229/1395 train_time:26901ms step_avg:122.84ms
step:230/1395 train_time:27024ms step_avg:122.84ms
step:231/1395 train_time:27147ms step_avg:122.84ms
step:232/1395 train_time:27271ms step_avg:122.84ms
step:233/1395 train_time:27393ms step_avg:122.84ms
step:234/1395 train_time:27517ms step_avg:122.84ms
step:235/1395 train_time:27641ms step_avg:122.85ms
step:236/1395 train_time:27764ms step_avg:122.85ms
step:237/1395 train_time:27888ms step_avg:122.86ms
step:238/1395 train_time:28010ms step_avg:122.85ms
step:239/1395 train_time:28134ms step_avg:122.86ms
step:240/1395 train_time:28258ms step_avg:122.86ms
step:241/1395 train_time:28381ms step_avg:122.86ms
step:242/1395 train_time:28504ms step_avg:122.86ms
step:243/1395 train_time:28627ms step_avg:122.86ms
step:244/1395 train_time:28752ms step_avg:122.87ms
step:245/1395 train_time:28876ms step_avg:122.88ms
step:246/1395 train_time:29000ms step_avg:122.88ms
step:247/1395 train_time:29123ms step_avg:122.88ms
step:248/1395 train_time:29247ms step_avg:122.89ms
step:249/1395 train_time:29371ms step_avg:122.89ms
step:250/1395 train_time:29495ms step_avg:122.89ms
step:250/1395 val_loss:3.9789 train_time:29617ms step_avg:123.40ms
step:251/1395 train_time:29638ms step_avg:122.98ms
step:252/1395 train_time:29756ms step_avg:122.96ms
step:253/1395 train_time:29882ms step_avg:122.97ms
step:254/1395 train_time:30006ms step_avg:122.97ms
step:255/1395 train_time:30128ms step_avg:122.97ms
step:256/1395 train_time:30251ms step_avg:122.97ms
step:257/1395 train_time:30374ms step_avg:122.97ms
step:258/1395 train_time:30497ms step_avg:122.97ms
step:259/1395 train_time:30620ms step_avg:122.97ms
step:260/1395 train_time:30744ms step_avg:122.98ms
step:261/1395 train_time:30868ms step_avg:122.98ms
step:262/1395 train_time:30993ms step_avg:122.99ms
step:263/1395 train_time:31116ms step_avg:122.99ms
step:264/1395 train_time:31240ms step_avg:122.99ms
step:265/1395 train_time:31363ms step_avg:122.99ms
step:266/1395 train_time:31486ms step_avg:122.99ms
step:267/1395 train_time:31609ms step_avg:122.99ms
step:268/1395 train_time:31734ms step_avg:123.00ms
step:269/1395 train_time:31857ms step_avg:123.00ms
step:270/1395 train_time:31981ms step_avg:123.00ms
step:271/1395 train_time:32105ms step_avg:123.01ms
step:272/1395 train_time:32228ms step_avg:123.01ms
step:273/1395 train_time:32351ms step_avg:123.01ms
step:274/1395 train_time:32473ms step_avg:123.00ms
step:275/1395 train_time:32595ms step_avg:123.00ms
step:276/1395 train_time:32719ms step_avg:123.00ms
step:277/1395 train_time:32843ms step_avg:123.01ms
step:278/1395 train_time:32968ms step_avg:123.02ms
step:279/1395 train_time:33089ms step_avg:123.01ms
step:280/1395 train_time:33212ms step_avg:123.01ms
step:281/1395 train_time:33337ms step_avg:123.01ms
step:282/1395 train_time:33460ms step_avg:123.02ms
step:283/1395 train_time:33584ms step_avg:123.02ms
step:284/1395 train_time:33707ms step_avg:123.02ms
step:285/1395 train_time:33831ms step_avg:123.02ms
step:286/1395 train_time:33954ms step_avg:123.02ms
step:287/1395 train_time:34076ms step_avg:123.02ms
step:288/1395 train_time:34200ms step_avg:123.02ms
step:289/1395 train_time:34324ms step_avg:123.02ms
step:290/1395 train_time:34447ms step_avg:123.02ms
step:291/1395 train_time:34570ms step_avg:123.02ms
step:292/1395 train_time:34693ms step_avg:123.02ms
step:293/1395 train_time:34816ms step_avg:123.03ms
step:294/1395 train_time:34940ms step_avg:123.03ms
step:295/1395 train_time:35064ms step_avg:123.03ms
step:296/1395 train_time:35187ms step_avg:123.03ms
step:297/1395 train_time:35310ms step_avg:123.03ms
step:298/1395 train_time:35434ms step_avg:123.03ms
step:299/1395 train_time:35557ms step_avg:123.04ms
step:300/1395 train_time:35680ms step_avg:123.04ms
step:301/1395 train_time:35803ms step_avg:123.03ms
step:302/1395 train_time:35927ms step_avg:123.04ms
step:303/1395 train_time:36050ms step_avg:123.04ms
step:304/1395 train_time:36174ms step_avg:123.04ms
step:305/1395 train_time:36298ms step_avg:123.04ms
step:306/1395 train_time:36421ms step_avg:123.04ms
step:307/1395 train_time:36546ms step_avg:123.05ms
step:308/1395 train_time:36669ms step_avg:123.05ms
step:309/1395 train_time:36792ms step_avg:123.05ms
step:310/1395 train_time:36915ms step_avg:123.05ms
step:311/1395 train_time:37039ms step_avg:123.05ms
step:312/1395 train_time:37162ms step_avg:123.05ms
step:313/1395 train_time:37288ms step_avg:123.06ms
step:314/1395 train_time:37412ms step_avg:123.07ms
step:315/1395 train_time:37539ms step_avg:123.08ms
step:316/1395 train_time:37664ms step_avg:123.09ms
step:317/1395 train_time:37790ms step_avg:123.10ms
step:318/1395 train_time:37916ms step_avg:123.11ms
step:319/1395 train_time:38043ms step_avg:123.12ms
step:320/1395 train_time:38169ms step_avg:123.13ms
step:321/1395 train_time:38295ms step_avg:123.13ms
step:322/1395 train_time:38420ms step_avg:123.14ms
step:323/1395 train_time:38546ms step_avg:123.15ms
step:324/1395 train_time:38671ms step_avg:123.15ms
step:325/1395 train_time:38796ms step_avg:123.16ms
step:326/1395 train_time:38922ms step_avg:123.17ms
step:327/1395 train_time:39048ms step_avg:123.18ms
step:328/1395 train_time:39174ms step_avg:123.19ms
step:329/1395 train_time:39301ms step_avg:123.20ms
step:330/1395 train_time:39426ms step_avg:123.21ms
step:331/1395 train_time:39553ms step_avg:123.22ms
step:332/1395 train_time:39679ms step_avg:123.23ms
step:333/1395 train_time:39806ms step_avg:123.24ms
step:334/1395 train_time:39931ms step_avg:123.24ms
step:335/1395 train_time:40057ms step_avg:123.25ms
step:336/1395 train_time:40182ms step_avg:123.26ms
step:337/1395 train_time:40308ms step_avg:123.26ms
step:338/1395 train_time:40433ms step_avg:123.27ms
step:339/1395 train_time:40559ms step_avg:123.28ms
step:340/1395 train_time:40684ms step_avg:123.29ms
step:341/1395 train_time:40810ms step_avg:123.29ms
step:342/1395 train_time:40936ms step_avg:123.30ms
step:343/1395 train_time:41062ms step_avg:123.31ms
step:344/1395 train_time:41187ms step_avg:123.32ms
step:345/1395 train_time:41313ms step_avg:123.32ms
step:346/1395 train_time:41439ms step_avg:123.33ms
step:347/1395 train_time:41567ms step_avg:123.34ms
step:348/1395 train_time:41693ms step_avg:123.35ms
step:349/1395 train_time:41819ms step_avg:123.36ms
step:350/1395 train_time:41945ms step_avg:123.37ms
step:351/1395 train_time:42070ms step_avg:123.37ms
step:352/1395 train_time:42196ms step_avg:123.38ms
step:353/1395 train_time:42321ms step_avg:123.39ms
step:354/1395 train_time:42448ms step_avg:123.40ms
step:355/1395 train_time:42574ms step_avg:123.40ms
step:356/1395 train_time:42700ms step_avg:123.41ms
step:357/1395 train_time:42826ms step_avg:123.42ms
step:358/1395 train_time:42952ms step_avg:123.43ms
step:359/1395 train_time:43078ms step_avg:123.43ms
step:360/1395 train_time:43204ms step_avg:123.44ms
step:361/1395 train_time:43331ms step_avg:123.45ms
step:362/1395 train_time:43458ms step_avg:123.46ms
step:363/1395 train_time:43584ms step_avg:123.47ms
step:364/1395 train_time:43709ms step_avg:123.47ms
step:365/1395 train_time:43835ms step_avg:123.48ms
step:366/1395 train_time:43962ms step_avg:123.49ms
step:367/1395 train_time:44088ms step_avg:123.49ms
step:368/1395 train_time:44213ms step_avg:123.50ms
step:369/1395 train_time:44338ms step_avg:123.51ms
step:370/1395 train_time:44464ms step_avg:123.51ms
step:371/1395 train_time:44590ms step_avg:123.52ms
step:372/1395 train_time:44715ms step_avg:123.52ms
step:373/1395 train_time:44841ms step_avg:123.53ms
step:374/1395 train_time:44967ms step_avg:123.53ms
step:375/1395 train_time:45093ms step_avg:123.54ms
step:375/1395 val_loss:3.7835 train_time:45217ms step_avg:123.88ms
step:376/1395 train_time:45238ms step_avg:123.60ms
step:377/1395 train_time:45359ms step_avg:123.59ms
step:378/1395 train_time:45486ms step_avg:123.60ms
step:379/1395 train_time:45612ms step_avg:123.61ms
step:380/1395 train_time:45738ms step_avg:123.62ms
step:381/1395 train_time:45863ms step_avg:123.62ms
step:382/1395 train_time:45989ms step_avg:123.63ms
step:383/1395 train_time:46114ms step_avg:123.63ms
step:384/1395 train_time:46239ms step_avg:123.63ms
step:385/1395 train_time:46367ms step_avg:123.64ms
step:386/1395 train_time:46494ms step_avg:123.65ms
step:387/1395 train_time:46620ms step_avg:123.66ms
step:388/1395 train_time:46746ms step_avg:123.67ms
step:389/1395 train_time:46872ms step_avg:123.67ms
step:390/1395 train_time:46998ms step_avg:123.68ms
step:391/1395 train_time:47123ms step_avg:123.68ms
step:392/1395 train_time:47248ms step_avg:123.69ms
step:393/1395 train_time:47376ms step_avg:123.70ms
step:394/1395 train_time:47501ms step_avg:123.70ms
step:395/1395 train_time:47627ms step_avg:123.71ms
step:396/1395 train_time:47753ms step_avg:123.71ms
step:397/1395 train_time:47881ms step_avg:123.72ms
step:398/1395 train_time:48006ms step_avg:123.73ms
step:399/1395 train_time:48133ms step_avg:123.73ms
step:400/1395 train_time:48259ms step_avg:123.74ms
step:401/1395 train_time:48384ms step_avg:123.75ms
step:402/1395 train_time:48510ms step_avg:123.75ms
step:403/1395 train_time:48637ms step_avg:123.76ms
step:404/1395 train_time:48764ms step_avg:123.77ms
step:405/1395 train_time:48890ms step_avg:123.77ms
step:406/1395 train_time:49016ms step_avg:123.78ms
step:407/1395 train_time:49141ms step_avg:123.78ms
step:408/1395 train_time:49268ms step_avg:123.79ms
step:409/1395 train_time:49394ms step_avg:123.79ms
step:410/1395 train_time:49520ms step_avg:123.80ms
step:411/1395 train_time:49646ms step_avg:123.81ms
step:412/1395 train_time:49772ms step_avg:123.81ms
step:413/1395 train_time:49897ms step_avg:123.81ms
step:414/1395 train_time:50023ms step_avg:123.82ms
step:415/1395 train_time:50150ms step_avg:123.83ms
step:416/1395 train_time:50276ms step_avg:123.83ms
step:417/1395 train_time:50402ms step_avg:123.84ms
step:418/1395 train_time:50528ms step_avg:123.84ms
step:419/1395 train_time:50655ms step_avg:123.85ms
step:420/1395 train_time:50782ms step_avg:123.86ms
step:421/1395 train_time:50908ms step_avg:123.86ms
step:422/1395 train_time:51034ms step_avg:123.87ms
step:423/1395 train_time:51160ms step_avg:123.87ms
step:424/1395 train_time:51287ms step_avg:123.88ms
step:425/1395 train_time:51413ms step_avg:123.89ms
step:426/1395 train_time:51540ms step_avg:123.89ms
step:427/1395 train_time:51667ms step_avg:123.90ms
step:428/1395 train_time:51793ms step_avg:123.91ms
step:429/1395 train_time:51919ms step_avg:123.91ms
step:430/1395 train_time:52045ms step_avg:123.92ms
step:431/1395 train_time:52172ms step_avg:123.92ms
step:432/1395 train_time:52299ms step_avg:123.93ms
step:433/1395 train_time:52424ms step_avg:123.93ms
step:434/1395 train_time:52550ms step_avg:123.94ms
step:435/1395 train_time:52677ms step_avg:123.95ms
step:436/1395 train_time:52803ms step_avg:123.95ms
step:437/1395 train_time:52930ms step_avg:123.96ms
step:438/1395 train_time:53056ms step_avg:123.96ms
step:439/1395 train_time:53183ms step_avg:123.97ms
step:440/1395 train_time:53308ms step_avg:123.97ms
step:441/1395 train_time:53435ms step_avg:123.98ms
step:442/1395 train_time:53561ms step_avg:123.98ms
step:443/1395 train_time:53688ms step_avg:123.99ms
step:444/1395 train_time:53814ms step_avg:124.00ms
step:445/1395 train_time:53940ms step_avg:124.00ms
step:446/1395 train_time:54066ms step_avg:124.01ms
step:447/1395 train_time:54193ms step_avg:124.01ms
step:448/1395 train_time:54319ms step_avg:124.02ms
step:449/1395 train_time:54445ms step_avg:124.02ms
step:450/1395 train_time:54571ms step_avg:124.02ms
step:451/1395 train_time:54698ms step_avg:124.03ms
step:452/1395 train_time:54824ms step_avg:124.04ms
step:453/1395 train_time:54951ms step_avg:124.04ms
step:454/1395 train_time:55078ms step_avg:124.05ms
step:455/1395 train_time:55204ms step_avg:124.05ms
step:456/1395 train_time:55330ms step_avg:124.06ms
step:457/1395 train_time:55458ms step_avg:124.07ms
step:458/1395 train_time:55584ms step_avg:124.07ms
step:459/1395 train_time:55710ms step_avg:124.08ms
step:460/1395 train_time:55837ms step_avg:124.08ms
step:461/1395 train_time:55963ms step_avg:124.09ms
step:462/1395 train_time:56090ms step_avg:124.09ms
step:463/1395 train_time:56217ms step_avg:124.10ms
step:464/1395 train_time:56343ms step_avg:124.10ms
step:465/1395 train_time:56471ms step_avg:124.11ms
step:466/1395 train_time:56598ms step_avg:124.12ms
step:467/1395 train_time:56724ms step_avg:124.12ms
step:468/1395 train_time:56851ms step_avg:124.13ms
step:469/1395 train_time:56979ms step_avg:124.14ms
step:470/1395 train_time:57105ms step_avg:124.14ms
step:471/1395 train_time:57232ms step_avg:124.15ms
step:472/1395 train_time:57358ms step_avg:124.15ms
step:473/1395 train_time:57484ms step_avg:124.16ms
step:474/1395 train_time:57611ms step_avg:124.16ms
step:475/1395 train_time:57737ms step_avg:124.17ms
step:476/1395 train_time:57863ms step_avg:124.17ms
step:477/1395 train_time:57990ms step_avg:124.18ms
step:478/1395 train_time:58116ms step_avg:124.18ms
step:479/1395 train_time:58241ms step_avg:124.18ms
step:480/1395 train_time:58367ms step_avg:124.19ms
step:481/1395 train_time:58496ms step_avg:124.20ms
step:482/1395 train_time:58622ms step_avg:124.20ms
step:483/1395 train_time:58747ms step_avg:124.20ms
step:484/1395 train_time:58873ms step_avg:124.21ms
step:485/1395 train_time:59000ms step_avg:124.21ms
step:486/1395 train_time:59126ms step_avg:124.21ms
step:487/1395 train_time:59252ms step_avg:124.22ms
step:488/1395 train_time:59378ms step_avg:124.22ms
step:489/1395 train_time:59506ms step_avg:124.23ms
step:490/1395 train_time:59633ms step_avg:124.23ms
step:491/1395 train_time:59759ms step_avg:124.24ms
step:492/1395 train_time:59885ms step_avg:124.24ms
step:493/1395 train_time:60010ms step_avg:124.24ms
step:494/1395 train_time:60137ms step_avg:124.25ms
step:495/1395 train_time:60263ms step_avg:124.25ms
step:496/1395 train_time:60390ms step_avg:124.26ms
step:497/1395 train_time:60517ms step_avg:124.26ms
step:498/1395 train_time:60644ms step_avg:124.27ms
step:499/1395 train_time:60771ms step_avg:124.28ms
step:500/1395 train_time:60898ms step_avg:124.28ms
step:500/1395 val_loss:3.6653 train_time:61022ms step_avg:124.53ms
step:501/1395 train_time:61045ms step_avg:124.33ms
step:502/1395 train_time:61163ms step_avg:124.32ms
step:503/1395 train_time:61291ms step_avg:124.32ms
step:504/1395 train_time:61417ms step_avg:124.33ms
step:505/1395 train_time:61542ms step_avg:124.33ms
step:506/1395 train_time:61668ms step_avg:124.33ms
step:507/1395 train_time:61793ms step_avg:124.33ms
step:508/1395 train_time:61919ms step_avg:124.34ms
step:509/1395 train_time:62045ms step_avg:124.34ms
step:510/1395 train_time:62173ms step_avg:124.35ms
step:511/1395 train_time:62300ms step_avg:124.35ms
step:512/1395 train_time:62426ms step_avg:124.35ms
step:513/1395 train_time:62552ms step_avg:124.36ms
step:514/1395 train_time:62679ms step_avg:124.36ms
step:515/1395 train_time:62805ms step_avg:124.37ms
step:516/1395 train_time:62931ms step_avg:124.37ms
step:517/1395 train_time:63057ms step_avg:124.37ms
step:518/1395 train_time:63184ms step_avg:124.38ms
step:519/1395 train_time:63312ms step_avg:124.39ms
step:520/1395 train_time:63441ms step_avg:124.39ms
step:521/1395 train_time:63569ms step_avg:124.40ms
step:522/1395 train_time:63697ms step_avg:124.41ms
step:523/1395 train_time:63826ms step_avg:124.42ms
step:524/1395 train_time:63953ms step_avg:124.42ms
step:525/1395 train_time:64082ms step_avg:124.43ms
step:526/1395 train_time:64210ms step_avg:124.44ms
step:527/1395 train_time:64339ms step_avg:124.45ms
step:528/1395 train_time:64467ms step_avg:124.45ms
step:529/1395 train_time:64595ms step_avg:124.46ms
step:530/1395 train_time:64723ms step_avg:124.47ms
step:531/1395 train_time:64852ms step_avg:124.48ms
step:532/1395 train_time:64980ms step_avg:124.48ms
step:533/1395 train_time:65109ms step_avg:124.49ms
step:534/1395 train_time:65238ms step_avg:124.50ms
step:535/1395 train_time:65368ms step_avg:124.51ms
step:536/1395 train_time:65497ms step_avg:124.52ms
step:537/1395 train_time:65627ms step_avg:124.53ms
step:538/1395 train_time:65756ms step_avg:124.54ms
step:539/1395 train_time:65884ms step_avg:124.54ms
step:540/1395 train_time:66014ms step_avg:124.55ms
step:541/1395 train_time:66142ms step_avg:124.56ms
step:542/1395 train_time:66269ms step_avg:124.57ms
step:543/1395 train_time:66398ms step_avg:124.57ms
step:544/1395 train_time:66526ms step_avg:124.58ms
step:545/1395 train_time:66654ms step_avg:124.59ms
step:546/1395 train_time:66783ms step_avg:124.60ms
step:547/1395 train_time:66911ms step_avg:124.60ms
step:548/1395 train_time:67039ms step_avg:124.61ms
step:549/1395 train_time:67167ms step_avg:124.61ms
step:550/1395 train_time:67295ms step_avg:124.62ms
step:551/1395 train_time:67425ms step_avg:124.63ms
step:552/1395 train_time:67553ms step_avg:124.64ms
step:553/1395 train_time:67682ms step_avg:124.64ms
step:554/1395 train_time:67811ms step_avg:124.65ms
step:555/1395 train_time:67939ms step_avg:124.66ms
step:556/1395 train_time:68067ms step_avg:124.67ms
step:557/1395 train_time:68196ms step_avg:124.67ms
step:558/1395 train_time:68324ms step_avg:124.68ms
step:559/1395 train_time:68452ms step_avg:124.68ms
step:560/1395 train_time:68580ms step_avg:124.69ms
step:561/1395 train_time:68708ms step_avg:124.70ms
step:562/1395 train_time:68837ms step_avg:124.70ms
step:563/1395 train_time:68966ms step_avg:124.71ms
step:564/1395 train_time:69094ms step_avg:124.72ms
step:565/1395 train_time:69224ms step_avg:124.73ms
step:566/1395 train_time:69352ms step_avg:124.73ms
step:567/1395 train_time:69481ms step_avg:124.74ms
step:568/1395 train_time:69609ms step_avg:124.75ms
step:569/1395 train_time:69738ms step_avg:124.75ms
step:570/1395 train_time:69866ms step_avg:124.76ms
step:571/1395 train_time:69994ms step_avg:124.77ms
step:572/1395 train_time:70123ms step_avg:124.77ms
step:573/1395 train_time:70252ms step_avg:124.78ms
step:574/1395 train_time:70380ms step_avg:124.79ms
step:575/1395 train_time:70509ms step_avg:124.79ms
step:576/1395 train_time:70637ms step_avg:124.80ms
step:577/1395 train_time:70766ms step_avg:124.81ms
step:578/1395 train_time:70894ms step_avg:124.81ms
step:579/1395 train_time:71022ms step_avg:124.82ms
step:580/1395 train_time:71150ms step_avg:124.83ms
step:581/1395 train_time:71278ms step_avg:124.83ms
step:582/1395 train_time:71406ms step_avg:124.84ms
step:583/1395 train_time:71534ms step_avg:124.84ms
step:584/1395 train_time:71663ms step_avg:124.85ms
step:585/1395 train_time:71791ms step_avg:124.85ms
step:586/1395 train_time:71919ms step_avg:124.86ms
step:587/1395 train_time:72047ms step_avg:124.86ms
step:588/1395 train_time:72175ms step_avg:124.87ms
step:589/1395 train_time:72304ms step_avg:124.88ms
step:590/1395 train_time:72434ms step_avg:124.89ms
step:591/1395 train_time:72562ms step_avg:124.89ms
step:592/1395 train_time:72692ms step_avg:124.90ms
step:593/1395 train_time:72820ms step_avg:124.91ms
step:594/1395 train_time:72947ms step_avg:124.91ms
step:595/1395 train_time:73076ms step_avg:124.92ms
step:596/1395 train_time:73205ms step_avg:124.92ms
step:597/1395 train_time:73333ms step_avg:124.93ms
step:598/1395 train_time:73462ms step_avg:124.94ms
step:599/1395 train_time:73590ms step_avg:124.94ms
step:600/1395 train_time:73718ms step_avg:124.95ms
step:601/1395 train_time:73847ms step_avg:124.95ms
step:602/1395 train_time:73974ms step_avg:124.96ms
step:603/1395 train_time:74103ms step_avg:124.96ms
step:604/1395 train_time:74231ms step_avg:124.97ms
step:605/1395 train_time:74360ms step_avg:124.97ms
step:606/1395 train_time:74488ms step_avg:124.98ms
step:607/1395 train_time:74616ms step_avg:124.98ms
step:608/1395 train_time:74745ms step_avg:124.99ms
step:609/1395 train_time:74873ms step_avg:125.00ms
step:610/1395 train_time:75001ms step_avg:125.00ms
step:611/1395 train_time:75129ms step_avg:125.01ms
step:612/1395 train_time:75257ms step_avg:125.01ms
step:613/1395 train_time:75387ms step_avg:125.02ms
step:614/1395 train_time:75515ms step_avg:125.02ms
step:615/1395 train_time:75643ms step_avg:125.03ms
step:616/1395 train_time:75771ms step_avg:125.04ms
step:617/1395 train_time:75898ms step_avg:125.04ms
step:618/1395 train_time:76027ms step_avg:125.04ms
step:619/1395 train_time:76155ms step_avg:125.05ms
step:620/1395 train_time:76284ms step_avg:125.06ms
step:621/1395 train_time:76412ms step_avg:125.06ms
step:622/1395 train_time:76541ms step_avg:125.07ms
step:623/1395 train_time:76670ms step_avg:125.07ms
step:624/1395 train_time:76800ms step_avg:125.08ms
step:625/1395 train_time:76929ms step_avg:125.09ms
step:625/1395 val_loss:3.5785 train_time:77056ms step_avg:125.29ms
step:626/1395 train_time:77076ms step_avg:125.12ms
step:627/1395 train_time:77200ms step_avg:125.12ms
step:628/1395 train_time:77330ms step_avg:125.13ms
step:629/1395 train_time:77459ms step_avg:125.14ms
step:630/1395 train_time:77587ms step_avg:125.14ms
step:631/1395 train_time:77715ms step_avg:125.14ms
step:632/1395 train_time:77843ms step_avg:125.15ms
step:633/1395 train_time:77971ms step_avg:125.15ms
step:634/1395 train_time:78099ms step_avg:125.16ms
step:635/1395 train_time:78229ms step_avg:125.17ms
step:636/1395 train_time:78360ms step_avg:125.18ms
step:637/1395 train_time:78488ms step_avg:125.18ms
step:638/1395 train_time:78617ms step_avg:125.19ms
step:639/1395 train_time:78746ms step_avg:125.19ms
step:640/1395 train_time:78875ms step_avg:125.20ms
step:641/1395 train_time:79003ms step_avg:125.20ms
step:642/1395 train_time:79131ms step_avg:125.21ms
step:643/1395 train_time:79260ms step_avg:125.21ms
step:644/1395 train_time:79389ms step_avg:125.22ms
step:645/1395 train_time:79518ms step_avg:125.23ms
step:646/1395 train_time:79649ms step_avg:125.23ms
step:647/1395 train_time:79777ms step_avg:125.24ms
step:648/1395 train_time:79906ms step_avg:125.24ms
step:649/1395 train_time:80035ms step_avg:125.25ms
step:650/1395 train_time:80164ms step_avg:125.26ms
step:651/1395 train_time:80293ms step_avg:125.26ms
step:652/1395 train_time:80422ms step_avg:125.27ms
step:653/1395 train_time:80550ms step_avg:125.27ms
step:654/1395 train_time:80680ms step_avg:125.28ms
step:655/1395 train_time:80808ms step_avg:125.28ms
step:656/1395 train_time:80938ms step_avg:125.29ms
step:657/1395 train_time:81066ms step_avg:125.30ms
step:658/1395 train_time:81195ms step_avg:125.30ms
step:659/1395 train_time:81323ms step_avg:125.31ms
step:660/1395 train_time:81452ms step_avg:125.31ms
step:661/1395 train_time:81581ms step_avg:125.32ms
step:662/1395 train_time:81711ms step_avg:125.32ms
step:663/1395 train_time:81840ms step_avg:125.33ms
step:664/1395 train_time:81969ms step_avg:125.33ms
step:665/1395 train_time:82098ms step_avg:125.34ms
step:666/1395 train_time:82227ms step_avg:125.35ms
step:667/1395 train_time:82356ms step_avg:125.35ms
step:668/1395 train_time:82484ms step_avg:125.36ms
step:669/1395 train_time:82613ms step_avg:125.36ms
step:670/1395 train_time:82742ms step_avg:125.37ms
step:671/1395 train_time:82872ms step_avg:125.37ms
step:672/1395 train_time:83001ms step_avg:125.38ms
step:673/1395 train_time:83129ms step_avg:125.38ms
step:674/1395 train_time:83258ms step_avg:125.39ms
step:675/1395 train_time:83386ms step_avg:125.39ms
step:676/1395 train_time:83515ms step_avg:125.40ms
step:677/1395 train_time:83643ms step_avg:125.40ms
step:678/1395 train_time:83772ms step_avg:125.41ms
step:679/1395 train_time:83901ms step_avg:125.41ms
step:680/1395 train_time:84030ms step_avg:125.42ms
step:681/1395 train_time:84159ms step_avg:125.42ms
step:682/1395 train_time:84288ms step_avg:125.43ms
step:683/1395 train_time:84416ms step_avg:125.43ms
step:684/1395 train_time:84545ms step_avg:125.44ms
step:685/1395 train_time:84674ms step_avg:125.44ms
step:686/1395 train_time:84803ms step_avg:125.45ms
step:687/1395 train_time:84932ms step_avg:125.45ms
step:688/1395 train_time:85061ms step_avg:125.46ms
step:689/1395 train_time:85190ms step_avg:125.46ms
step:690/1395 train_time:85319ms step_avg:125.47ms
step:691/1395 train_time:85448ms step_avg:125.47ms
step:692/1395 train_time:85577ms step_avg:125.48ms
step:693/1395 train_time:85705ms step_avg:125.48ms
step:694/1395 train_time:85836ms step_avg:125.49ms
step:695/1395 train_time:85964ms step_avg:125.50ms
step:696/1395 train_time:86092ms step_avg:125.50ms
step:697/1395 train_time:86222ms step_avg:125.50ms
step:698/1395 train_time:86350ms step_avg:125.51ms
step:699/1395 train_time:86479ms step_avg:125.51ms
step:700/1395 train_time:86608ms step_avg:125.52ms
step:701/1395 train_time:86736ms step_avg:125.52ms
step:702/1395 train_time:86866ms step_avg:125.53ms
step:703/1395 train_time:86994ms step_avg:125.53ms
step:704/1395 train_time:87123ms step_avg:125.54ms
step:705/1395 train_time:87252ms step_avg:125.54ms
step:706/1395 train_time:87381ms step_avg:125.55ms
step:707/1395 train_time:87509ms step_avg:125.55ms
step:708/1395 train_time:87639ms step_avg:125.56ms
step:709/1395 train_time:87768ms step_avg:125.56ms
step:710/1395 train_time:87898ms step_avg:125.57ms
step:711/1395 train_time:88027ms step_avg:125.57ms
step:712/1395 train_time:88156ms step_avg:125.58ms
step:713/1395 train_time:88284ms step_avg:125.58ms
step:714/1395 train_time:88414ms step_avg:125.59ms
step:715/1395 train_time:88542ms step_avg:125.59ms
step:716/1395 train_time:88671ms step_avg:125.60ms
step:717/1395 train_time:88800ms step_avg:125.60ms
step:718/1395 train_time:88930ms step_avg:125.61ms
step:719/1395 train_time:89059ms step_avg:125.61ms
step:720/1395 train_time:89188ms step_avg:125.62ms
step:721/1395 train_time:89317ms step_avg:125.62ms
step:722/1395 train_time:89446ms step_avg:125.63ms
step:723/1395 train_time:89575ms step_avg:125.63ms
step:724/1395 train_time:89703ms step_avg:125.63ms
step:725/1395 train_time:89832ms step_avg:125.64ms
step:726/1395 train_time:89963ms step_avg:125.65ms
step:727/1395 train_time:90095ms step_avg:125.66ms
step:728/1395 train_time:90227ms step_avg:125.66ms
step:729/1395 train_time:90357ms step_avg:125.67ms
step:730/1395 train_time:90489ms step_avg:125.68ms
step:731/1395 train_time:90620ms step_avg:125.69ms
step:732/1395 train_time:90749ms step_avg:125.69ms
step:733/1395 train_time:90880ms step_avg:125.70ms
step:734/1395 train_time:91010ms step_avg:125.70ms
step:735/1395 train_time:91141ms step_avg:125.71ms
step:736/1395 train_time:91271ms step_avg:125.72ms
step:737/1395 train_time:91403ms step_avg:125.73ms
step:738/1395 train_time:91533ms step_avg:125.73ms
step:739/1395 train_time:91664ms step_avg:125.74ms
step:740/1395 train_time:91794ms step_avg:125.75ms
step:741/1395 train_time:91926ms step_avg:125.75ms
step:742/1395 train_time:92056ms step_avg:125.76ms
step:743/1395 train_time:92187ms step_avg:125.77ms
step:744/1395 train_time:92317ms step_avg:125.77ms
step:745/1395 train_time:92449ms step_avg:125.78ms
step:746/1395 train_time:92580ms step_avg:125.79ms
step:747/1395 train_time:92711ms step_avg:125.80ms
step:748/1395 train_time:92841ms step_avg:125.80ms
step:749/1395 train_time:92972ms step_avg:125.81ms
step:750/1395 train_time:93103ms step_avg:125.82ms
step:750/1395 val_loss:3.5252 train_time:93233ms step_avg:125.99ms
step:751/1395 train_time:93254ms step_avg:125.85ms
step:752/1395 train_time:93377ms step_avg:125.85ms
step:753/1395 train_time:93508ms step_avg:125.85ms
step:754/1395 train_time:93638ms step_avg:125.86ms
step:755/1395 train_time:93768ms step_avg:125.86ms
step:756/1395 train_time:93898ms step_avg:125.87ms
step:757/1395 train_time:94030ms step_avg:125.88ms
step:758/1395 train_time:94161ms step_avg:125.88ms
step:759/1395 train_time:94292ms step_avg:125.89ms
step:760/1395 train_time:94424ms step_avg:125.90ms
step:761/1395 train_time:94555ms step_avg:125.91ms
step:762/1395 train_time:94687ms step_avg:125.91ms
step:763/1395 train_time:94817ms step_avg:125.92ms
step:764/1395 train_time:94947ms step_avg:125.92ms
step:765/1395 train_time:95078ms step_avg:125.93ms
step:766/1395 train_time:95208ms step_avg:125.94ms
step:767/1395 train_time:95338ms step_avg:125.94ms
step:768/1395 train_time:95470ms step_avg:125.95ms
step:769/1395 train_time:95601ms step_avg:125.96ms
step:770/1395 train_time:95733ms step_avg:125.96ms
step:771/1395 train_time:95863ms step_avg:125.97ms
step:772/1395 train_time:95994ms step_avg:125.98ms
step:773/1395 train_time:96126ms step_avg:125.98ms
step:774/1395 train_time:96256ms step_avg:125.99ms
step:775/1395 train_time:96386ms step_avg:125.99ms
step:776/1395 train_time:96517ms step_avg:126.00ms
step:777/1395 train_time:96647ms step_avg:126.01ms
step:778/1395 train_time:96779ms step_avg:126.01ms
step:779/1395 train_time:96910ms step_avg:126.02ms
step:780/1395 train_time:97041ms step_avg:126.03ms
step:781/1395 train_time:97171ms step_avg:126.03ms
step:782/1395 train_time:97302ms step_avg:126.04ms
step:783/1395 train_time:97432ms step_avg:126.04ms
step:784/1395 train_time:97563ms step_avg:126.05ms
step:785/1395 train_time:97695ms step_avg:126.06ms
step:786/1395 train_time:97825ms step_avg:126.06ms
step:787/1395 train_time:97956ms step_avg:126.07ms
step:788/1395 train_time:98086ms step_avg:126.07ms
step:789/1395 train_time:98216ms step_avg:126.08ms
step:790/1395 train_time:98347ms step_avg:126.09ms
step:791/1395 train_time:98478ms step_avg:126.09ms
step:792/1395 train_time:98608ms step_avg:126.10ms
step:793/1395 train_time:98739ms step_avg:126.10ms
step:794/1395 train_time:98870ms step_avg:126.11ms
step:795/1395 train_time:99003ms step_avg:126.12ms
step:796/1395 train_time:99135ms step_avg:126.13ms
step:797/1395 train_time:99266ms step_avg:126.13ms
step:798/1395 train_time:99397ms step_avg:126.14ms
step:799/1395 train_time:99528ms step_avg:126.14ms
step:800/1395 train_time:99658ms step_avg:126.15ms
step:801/1395 train_time:99790ms step_avg:126.16ms
step:802/1395 train_time:99921ms step_avg:126.16ms
step:803/1395 train_time:100051ms step_avg:126.17ms
step:804/1395 train_time:100181ms step_avg:126.17ms
step:805/1395 train_time:100313ms step_avg:126.18ms
step:806/1395 train_time:100443ms step_avg:126.19ms
step:807/1395 train_time:100573ms step_avg:126.19ms
step:808/1395 train_time:100705ms step_avg:126.20ms
step:809/1395 train_time:100835ms step_avg:126.20ms
step:810/1395 train_time:100966ms step_avg:126.21ms
step:811/1395 train_time:101096ms step_avg:126.21ms
step:812/1395 train_time:101226ms step_avg:126.22ms
step:813/1395 train_time:101356ms step_avg:126.22ms
step:814/1395 train_time:101487ms step_avg:126.23ms
step:815/1395 train_time:101618ms step_avg:126.23ms
step:816/1395 train_time:101751ms step_avg:126.24ms
step:817/1395 train_time:101881ms step_avg:126.25ms
step:818/1395 train_time:102012ms step_avg:126.25ms
step:819/1395 train_time:102143ms step_avg:126.26ms
step:820/1395 train_time:102273ms step_avg:126.26ms
step:821/1395 train_time:102404ms step_avg:126.27ms
step:822/1395 train_time:102534ms step_avg:126.27ms
step:823/1395 train_time:102664ms step_avg:126.28ms
step:824/1395 train_time:102795ms step_avg:126.28ms
step:825/1395 train_time:102927ms step_avg:126.29ms
step:826/1395 train_time:103057ms step_avg:126.30ms
step:827/1395 train_time:103187ms step_avg:126.30ms
step:828/1395 train_time:103317ms step_avg:126.30ms
step:829/1395 train_time:103447ms step_avg:126.31ms
step:830/1395 train_time:103579ms step_avg:126.32ms
step:831/1395 train_time:103710ms step_avg:126.32ms
step:832/1395 train_time:103841ms step_avg:126.33ms
step:833/1395 train_time:103972ms step_avg:126.33ms
step:834/1395 train_time:104103ms step_avg:126.34ms
step:835/1395 train_time:104234ms step_avg:126.34ms
step:836/1395 train_time:104367ms step_avg:126.35ms
step:837/1395 train_time:104498ms step_avg:126.36ms
step:838/1395 train_time:104628ms step_avg:126.36ms
step:839/1395 train_time:104759ms step_avg:126.37ms
step:840/1395 train_time:104890ms step_avg:126.37ms
step:841/1395 train_time:105021ms step_avg:126.38ms
step:842/1395 train_time:105153ms step_avg:126.39ms
step:843/1395 train_time:105284ms step_avg:126.39ms
step:844/1395 train_time:105415ms step_avg:126.40ms
step:845/1395 train_time:105545ms step_avg:126.40ms
step:846/1395 train_time:105676ms step_avg:126.41ms
step:847/1395 train_time:105807ms step_avg:126.41ms
step:848/1395 train_time:105939ms step_avg:126.42ms
step:849/1395 train_time:106069ms step_avg:126.42ms
step:850/1395 train_time:106201ms step_avg:126.43ms
step:851/1395 train_time:106332ms step_avg:126.44ms
step:852/1395 train_time:106463ms step_avg:126.44ms
step:853/1395 train_time:106594ms step_avg:126.45ms
step:854/1395 train_time:106725ms step_avg:126.45ms
step:855/1395 train_time:106856ms step_avg:126.46ms
step:856/1395 train_time:106987ms step_avg:126.46ms
step:857/1395 train_time:107118ms step_avg:126.47ms
step:858/1395 train_time:107250ms step_avg:126.47ms
step:859/1395 train_time:107381ms step_avg:126.48ms
step:860/1395 train_time:107512ms step_avg:126.49ms
step:861/1395 train_time:107643ms step_avg:126.49ms
step:862/1395 train_time:107775ms step_avg:126.50ms
step:863/1395 train_time:107906ms step_avg:126.50ms
step:864/1395 train_time:108036ms step_avg:126.51ms
step:865/1395 train_time:108167ms step_avg:126.51ms
step:866/1395 train_time:108299ms step_avg:126.52ms
step:867/1395 train_time:108431ms step_avg:126.52ms
step:868/1395 train_time:108561ms step_avg:126.53ms
step:869/1395 train_time:108692ms step_avg:126.53ms
step:870/1395 train_time:108824ms step_avg:126.54ms
step:871/1395 train_time:108955ms step_avg:126.54ms
step:872/1395 train_time:109086ms step_avg:126.55ms
step:873/1395 train_time:109216ms step_avg:126.55ms
step:874/1395 train_time:109347ms step_avg:126.56ms
step:875/1395 train_time:109479ms step_avg:126.57ms
step:875/1395 val_loss:3.4765 train_time:109609ms step_avg:126.72ms
step:876/1395 train_time:109630ms step_avg:126.59ms
step:877/1395 train_time:109754ms step_avg:126.59ms
step:878/1395 train_time:109886ms step_avg:126.60ms
step:879/1395 train_time:110016ms step_avg:126.60ms
step:880/1395 train_time:110147ms step_avg:126.61ms
step:881/1395 train_time:110278ms step_avg:126.61ms
step:882/1395 train_time:110408ms step_avg:126.61ms
step:883/1395 train_time:110538ms step_avg:126.62ms
step:884/1395 train_time:110670ms step_avg:126.63ms
step:885/1395 train_time:110802ms step_avg:126.63ms
step:886/1395 train_time:110935ms step_avg:126.64ms
step:887/1395 train_time:111065ms step_avg:126.64ms
step:888/1395 train_time:111196ms step_avg:126.65ms
step:889/1395 train_time:111328ms step_avg:126.65ms
step:890/1395 train_time:111460ms step_avg:126.66ms
step:891/1395 train_time:111590ms step_avg:126.66ms
step:892/1395 train_time:111722ms step_avg:126.67ms
step:893/1395 train_time:111853ms step_avg:126.67ms
step:894/1395 train_time:111984ms step_avg:126.68ms
step:895/1395 train_time:112116ms step_avg:126.68ms
step:896/1395 train_time:112246ms step_avg:126.69ms
step:897/1395 train_time:112376ms step_avg:126.69ms
step:898/1395 train_time:112508ms step_avg:126.70ms
step:899/1395 train_time:112640ms step_avg:126.70ms
step:900/1395 train_time:112772ms step_avg:126.71ms
step:901/1395 train_time:112903ms step_avg:126.72ms
step:902/1395 train_time:113033ms step_avg:126.72ms
step:903/1395 train_time:113165ms step_avg:126.72ms
step:904/1395 train_time:113295ms step_avg:126.73ms
step:905/1395 train_time:113427ms step_avg:126.73ms
step:906/1395 train_time:113558ms step_avg:126.74ms
step:907/1395 train_time:113690ms step_avg:126.74ms
step:908/1395 train_time:113821ms step_avg:126.75ms
step:909/1395 train_time:113953ms step_avg:126.75ms
step:910/1395 train_time:114086ms step_avg:126.76ms
step:911/1395 train_time:114217ms step_avg:126.77ms
step:912/1395 train_time:114348ms step_avg:126.77ms
step:913/1395 train_time:114479ms step_avg:126.78ms
step:914/1395 train_time:114610ms step_avg:126.78ms
step:915/1395 train_time:114742ms step_avg:126.79ms
step:916/1395 train_time:114873ms step_avg:126.79ms
step:917/1395 train_time:115004ms step_avg:126.80ms
step:918/1395 train_time:115135ms step_avg:126.80ms
step:919/1395 train_time:115267ms step_avg:126.81ms
step:920/1395 train_time:115398ms step_avg:126.81ms
step:921/1395 train_time:115528ms step_avg:126.81ms
step:922/1395 train_time:115659ms step_avg:126.82ms
step:923/1395 train_time:115790ms step_avg:126.82ms
step:924/1395 train_time:115920ms step_avg:126.83ms
step:925/1395 train_time:116051ms step_avg:126.83ms
step:926/1395 train_time:116183ms step_avg:126.84ms
step:927/1395 train_time:116315ms step_avg:126.84ms
step:928/1395 train_time:116445ms step_avg:126.85ms
step:929/1395 train_time:116577ms step_avg:126.85ms
step:930/1395 train_time:116708ms step_avg:126.86ms
step:931/1395 train_time:116838ms step_avg:126.86ms
step:932/1395 train_time:116970ms step_avg:126.87ms
step:933/1395 train_time:117103ms step_avg:126.87ms
step:934/1395 train_time:117235ms step_avg:126.88ms
step:935/1395 train_time:117369ms step_avg:126.89ms
step:936/1395 train_time:117501ms step_avg:126.89ms
step:937/1395 train_time:117636ms step_avg:126.90ms
step:938/1395 train_time:117768ms step_avg:126.90ms
step:939/1395 train_time:117900ms step_avg:126.91ms
step:940/1395 train_time:118034ms step_avg:126.92ms
step:941/1395 train_time:118167ms step_avg:126.92ms
step:942/1395 train_time:118299ms step_avg:126.93ms
step:943/1395 train_time:118432ms step_avg:126.94ms
step:944/1395 train_time:118566ms step_avg:126.94ms
step:945/1395 train_time:118699ms step_avg:126.95ms
step:946/1395 train_time:118833ms step_avg:126.96ms
step:947/1395 train_time:118966ms step_avg:126.96ms
step:948/1395 train_time:119098ms step_avg:126.97ms
step:949/1395 train_time:119231ms step_avg:126.98ms
step:950/1395 train_time:119363ms step_avg:126.98ms
step:951/1395 train_time:119497ms step_avg:126.99ms
step:952/1395 train_time:119630ms step_avg:127.00ms
step:953/1395 train_time:119763ms step_avg:127.00ms
step:954/1395 train_time:119896ms step_avg:127.01ms
step:955/1395 train_time:120028ms step_avg:127.01ms
step:956/1395 train_time:120163ms step_avg:127.02ms
step:957/1395 train_time:120295ms step_avg:127.03ms
step:958/1395 train_time:120429ms step_avg:127.03ms
step:959/1395 train_time:120562ms step_avg:127.04ms
step:960/1395 train_time:120695ms step_avg:127.05ms
step:961/1395 train_time:120827ms step_avg:127.05ms
step:962/1395 train_time:120960ms step_avg:127.06ms
step:963/1395 train_time:121094ms step_avg:127.07ms
step:964/1395 train_time:121226ms step_avg:127.07ms
step:965/1395 train_time:121360ms step_avg:127.08ms
step:966/1395 train_time:121493ms step_avg:127.08ms
step:967/1395 train_time:121625ms step_avg:127.09ms
step:968/1395 train_time:121758ms step_avg:127.10ms
step:969/1395 train_time:121891ms step_avg:127.10ms
step:970/1395 train_time:122023ms step_avg:127.11ms
step:971/1395 train_time:122156ms step_avg:127.11ms
step:972/1395 train_time:122289ms step_avg:127.12ms
step:973/1395 train_time:122422ms step_avg:127.13ms
step:974/1395 train_time:122554ms step_avg:127.13ms
step:975/1395 train_time:122686ms step_avg:127.14ms
step:976/1395 train_time:122819ms step_avg:127.14ms
step:977/1395 train_time:122952ms step_avg:127.15ms
step:978/1395 train_time:123085ms step_avg:127.15ms
step:979/1395 train_time:123218ms step_avg:127.16ms
step:980/1395 train_time:123351ms step_avg:127.17ms
step:981/1395 train_time:123482ms step_avg:127.17ms
step:982/1395 train_time:123615ms step_avg:127.18ms
step:983/1395 train_time:123746ms step_avg:127.18ms
step:984/1395 train_time:123879ms step_avg:127.19ms
step:985/1395 train_time:124013ms step_avg:127.19ms
step:986/1395 train_time:124147ms step_avg:127.20ms
step:987/1395 train_time:124279ms step_avg:127.21ms
step:988/1395 train_time:124412ms step_avg:127.21ms
step:989/1395 train_time:124546ms step_avg:127.22ms
step:990/1395 train_time:124679ms step_avg:127.22ms
step:991/1395 train_time:124811ms step_avg:127.23ms
step:992/1395 train_time:124945ms step_avg:127.24ms
step:993/1395 train_time:125082ms step_avg:127.24ms
step:994/1395 train_time:125213ms step_avg:127.25ms
step:995/1395 train_time:125345ms step_avg:127.25ms
step:996/1395 train_time:125477ms step_avg:127.26ms
step:997/1395 train_time:125609ms step_avg:127.26ms
step:998/1395 train_time:125742ms step_avg:127.27ms
step:999/1395 train_time:125874ms step_avg:127.27ms
step:1000/1395 train_time:126007ms step_avg:127.28ms
step:1000/1395 val_loss:3.4135 train_time:126137ms step_avg:127.41ms
step:1001/1395 train_time:126158ms step_avg:127.30ms
step:1002/1395 train_time:126280ms step_avg:127.30ms
step:1003/1395 train_time:126414ms step_avg:127.30ms
step:1004/1395 train_time:126546ms step_avg:127.31ms
step:1005/1395 train_time:126678ms step_avg:127.31ms
step:1006/1395 train_time:126810ms step_avg:127.32ms
step:1007/1395 train_time:126942ms step_avg:127.32ms
step:1008/1395 train_time:127074ms step_avg:127.33ms
step:1009/1395 train_time:127210ms step_avg:127.34ms
step:1010/1395 train_time:127344ms step_avg:127.34ms
step:1011/1395 train_time:127478ms step_avg:127.35ms
step:1012/1395 train_time:127612ms step_avg:127.36ms
step:1013/1395 train_time:127745ms step_avg:127.36ms
step:1014/1395 train_time:127876ms step_avg:127.37ms
step:1015/1395 train_time:128008ms step_avg:127.37ms
step:1016/1395 train_time:128139ms step_avg:127.37ms
step:1017/1395 train_time:128272ms step_avg:127.38ms
step:1018/1395 train_time:128406ms step_avg:127.39ms
step:1019/1395 train_time:128540ms step_avg:127.39ms
step:1020/1395 train_time:128673ms step_avg:127.40ms
step:1021/1395 train_time:128806ms step_avg:127.40ms
step:1022/1395 train_time:128938ms step_avg:127.41ms
step:1023/1395 train_time:129070ms step_avg:127.41ms
step:1024/1395 train_time:129203ms step_avg:127.42ms
step:1025/1395 train_time:129336ms step_avg:127.42ms
step:1026/1395 train_time:129469ms step_avg:127.43ms
step:1027/1395 train_time:129602ms step_avg:127.44ms
step:1028/1395 train_time:129734ms step_avg:127.44ms
step:1029/1395 train_time:129868ms step_avg:127.45ms
step:1030/1395 train_time:130001ms step_avg:127.45ms
step:1031/1395 train_time:130132ms step_avg:127.46ms
step:1032/1395 train_time:130265ms step_avg:127.46ms
step:1033/1395 train_time:130396ms step_avg:127.46ms
step:1034/1395 train_time:130529ms step_avg:127.47ms
step:1035/1395 train_time:130662ms step_avg:127.48ms
step:1036/1395 train_time:130796ms step_avg:127.48ms
step:1037/1395 train_time:130930ms step_avg:127.49ms
step:1038/1395 train_time:131063ms step_avg:127.49ms
step:1039/1395 train_time:131195ms step_avg:127.50ms
step:1040/1395 train_time:131327ms step_avg:127.50ms
step:1041/1395 train_time:131459ms step_avg:127.51ms
step:1042/1395 train_time:131592ms step_avg:127.51ms
step:1043/1395 train_time:131727ms step_avg:127.52ms
step:1044/1395 train_time:131861ms step_avg:127.53ms
step:1045/1395 train_time:131996ms step_avg:127.53ms
step:1046/1395 train_time:132129ms step_avg:127.54ms
step:1047/1395 train_time:132261ms step_avg:127.54ms
step:1048/1395 train_time:132395ms step_avg:127.55ms
step:1049/1395 train_time:132529ms step_avg:127.55ms
step:1050/1395 train_time:132662ms step_avg:127.56ms
step:1051/1395 train_time:132797ms step_avg:127.57ms
step:1052/1395 train_time:132931ms step_avg:127.57ms
step:1053/1395 train_time:133064ms step_avg:127.58ms
step:1054/1395 train_time:133196ms step_avg:127.58ms
step:1055/1395 train_time:133330ms step_avg:127.59ms
step:1056/1395 train_time:133461ms step_avg:127.59ms
step:1057/1395 train_time:133593ms step_avg:127.60ms
step:1058/1395 train_time:133727ms step_avg:127.60ms
step:1059/1395 train_time:133859ms step_avg:127.61ms
step:1060/1395 train_time:133993ms step_avg:127.61ms
step:1061/1395 train_time:134127ms step_avg:127.62ms
step:1062/1395 train_time:134260ms step_avg:127.62ms
step:1063/1395 train_time:134393ms step_avg:127.63ms
step:1064/1395 train_time:134526ms step_avg:127.63ms
step:1065/1395 train_time:134658ms step_avg:127.64ms
step:1066/1395 train_time:134791ms step_avg:127.64ms
step:1067/1395 train_time:134925ms step_avg:127.65ms
step:1068/1395 train_time:135059ms step_avg:127.65ms
step:1069/1395 train_time:135193ms step_avg:127.66ms
step:1070/1395 train_time:135327ms step_avg:127.67ms
step:1071/1395 train_time:135461ms step_avg:127.67ms
step:1072/1395 train_time:135593ms step_avg:127.68ms
step:1073/1395 train_time:135724ms step_avg:127.68ms
step:1074/1395 train_time:135856ms step_avg:127.68ms
step:1075/1395 train_time:135989ms step_avg:127.69ms
step:1076/1395 train_time:136121ms step_avg:127.69ms
step:1077/1395 train_time:136254ms step_avg:127.70ms
step:1078/1395 train_time:136387ms step_avg:127.70ms
step:1079/1395 train_time:136525ms step_avg:127.71ms
step:1080/1395 train_time:136657ms step_avg:127.72ms
step:1081/1395 train_time:136791ms step_avg:127.72ms
step:1082/1395 train_time:136923ms step_avg:127.73ms
step:1083/1395 train_time:137055ms step_avg:127.73ms
step:1084/1395 train_time:137189ms step_avg:127.74ms
step:1085/1395 train_time:137322ms step_avg:127.74ms
step:1086/1395 train_time:137455ms step_avg:127.75ms
step:1087/1395 train_time:137589ms step_avg:127.75ms
step:1088/1395 train_time:137722ms step_avg:127.76ms
step:1089/1395 train_time:137857ms step_avg:127.76ms
step:1090/1395 train_time:137990ms step_avg:127.77ms
step:1091/1395 train_time:138122ms step_avg:127.77ms
step:1092/1395 train_time:138255ms step_avg:127.78ms
step:1093/1395 train_time:138388ms step_avg:127.78ms
step:1094/1395 train_time:138521ms step_avg:127.79ms
step:1095/1395 train_time:138654ms step_avg:127.79ms
step:1096/1395 train_time:138787ms step_avg:127.80ms
step:1097/1395 train_time:138920ms step_avg:127.80ms
step:1098/1395 train_time:139052ms step_avg:127.80ms
step:1099/1395 train_time:139184ms step_avg:127.81ms
step:1100/1395 train_time:139316ms step_avg:127.81ms
step:1101/1395 train_time:139449ms step_avg:127.82ms
step:1102/1395 train_time:139582ms step_avg:127.82ms
step:1103/1395 train_time:139716ms step_avg:127.83ms
step:1104/1395 train_time:139850ms step_avg:127.83ms
step:1105/1395 train_time:139985ms step_avg:127.84ms
step:1106/1395 train_time:140117ms step_avg:127.84ms
step:1107/1395 train_time:140250ms step_avg:127.85ms
step:1108/1395 train_time:140384ms step_avg:127.85ms
step:1109/1395 train_time:140516ms step_avg:127.86ms
step:1110/1395 train_time:140648ms step_avg:127.86ms
step:1111/1395 train_time:140781ms step_avg:127.87ms
step:1112/1395 train_time:140914ms step_avg:127.87ms
step:1113/1395 train_time:141046ms step_avg:127.88ms
step:1114/1395 train_time:141178ms step_avg:127.88ms
step:1115/1395 train_time:141310ms step_avg:127.88ms
step:1116/1395 train_time:141446ms step_avg:127.89ms
step:1117/1395 train_time:141579ms step_avg:127.89ms
step:1118/1395 train_time:141714ms step_avg:127.90ms
step:1119/1395 train_time:141846ms step_avg:127.90ms
step:1120/1395 train_time:141981ms step_avg:127.91ms
step:1121/1395 train_time:142113ms step_avg:127.91ms
step:1122/1395 train_time:142246ms step_avg:127.92ms
step:1123/1395 train_time:142378ms step_avg:127.92ms
step:1124/1395 train_time:142512ms step_avg:127.93ms
step:1125/1395 train_time:142643ms step_avg:127.93ms
step:1125/1395 val_loss:3.3626 train_time:142776ms step_avg:128.05ms
step:1126/1395 train_time:142797ms step_avg:127.95ms
step:1127/1395 train_time:142919ms step_avg:127.95ms
step:1128/1395 train_time:143053ms step_avg:127.95ms
step:1129/1395 train_time:143185ms step_avg:127.96ms
step:1130/1395 train_time:143317ms step_avg:127.96ms
step:1131/1395 train_time:143450ms step_avg:127.97ms
step:1132/1395 train_time:143582ms step_avg:127.97ms
step:1133/1395 train_time:143714ms step_avg:127.97ms
step:1134/1395 train_time:143850ms step_avg:127.98ms
step:1135/1395 train_time:143983ms step_avg:127.99ms
step:1136/1395 train_time:144120ms step_avg:127.99ms
step:1137/1395 train_time:144251ms step_avg:128.00ms
step:1138/1395 train_time:144384ms step_avg:128.00ms
step:1139/1395 train_time:144519ms step_avg:128.01ms
step:1140/1395 train_time:144652ms step_avg:128.01ms
step:1141/1395 train_time:144786ms step_avg:128.02ms
step:1142/1395 train_time:144921ms step_avg:128.02ms
step:1143/1395 train_time:145057ms step_avg:128.03ms
step:1144/1395 train_time:145192ms step_avg:128.04ms
step:1145/1395 train_time:145325ms step_avg:128.04ms
step:1146/1395 train_time:145459ms step_avg:128.04ms
step:1147/1395 train_time:145593ms step_avg:128.05ms
step:1148/1395 train_time:145727ms step_avg:128.06ms
step:1149/1395 train_time:145862ms step_avg:128.06ms
step:1150/1395 train_time:145997ms step_avg:128.07ms
step:1151/1395 train_time:146134ms step_avg:128.08ms
step:1152/1395 train_time:146267ms step_avg:128.08ms
step:1153/1395 train_time:146403ms step_avg:128.09ms
step:1154/1395 train_time:146539ms step_avg:128.09ms
step:1155/1395 train_time:146672ms step_avg:128.10ms
step:1156/1395 train_time:146809ms step_avg:128.11ms
step:1157/1395 train_time:146945ms step_avg:128.11ms
step:1158/1395 train_time:147080ms step_avg:128.12ms
step:1159/1395 train_time:147213ms step_avg:128.12ms
step:1160/1395 train_time:147347ms step_avg:128.13ms
step:1161/1395 train_time:147481ms step_avg:128.13ms
step:1162/1395 train_time:147616ms step_avg:128.14ms
step:1163/1395 train_time:147750ms step_avg:128.14ms
step:1164/1395 train_time:147885ms step_avg:128.15ms
step:1165/1395 train_time:148018ms step_avg:128.15ms
step:1166/1395 train_time:148152ms step_avg:128.16ms
step:1167/1395 train_time:148286ms step_avg:128.16ms
step:1168/1395 train_time:148421ms step_avg:128.17ms
step:1169/1395 train_time:148554ms step_avg:128.17ms
step:1170/1395 train_time:148689ms step_avg:128.18ms
step:1171/1395 train_time:148824ms step_avg:128.19ms
step:1172/1395 train_time:148960ms step_avg:128.19ms
step:1173/1395 train_time:149093ms step_avg:128.20ms
step:1174/1395 train_time:149232ms step_avg:128.21ms
step:1175/1395 train_time:149366ms step_avg:128.21ms
step:1176/1395 train_time:149502ms step_avg:128.22ms
step:1177/1395 train_time:149639ms step_avg:128.23ms
step:1178/1395 train_time:149772ms step_avg:128.23ms
step:1179/1395 train_time:149907ms step_avg:128.23ms
step:1180/1395 train_time:150043ms step_avg:128.24ms
step:1181/1395 train_time:150179ms step_avg:128.25ms
step:1182/1395 train_time:150312ms step_avg:128.25ms
step:1183/1395 train_time:150446ms step_avg:128.26ms
step:1184/1395 train_time:150580ms step_avg:128.26ms
step:1185/1395 train_time:150714ms step_avg:128.27ms
step:1186/1395 train_time:150847ms step_avg:128.27ms
step:1187/1395 train_time:150986ms step_avg:128.28ms
step:1188/1395 train_time:151120ms step_avg:128.29ms
step:1189/1395 train_time:151254ms step_avg:128.29ms
step:1190/1395 train_time:151387ms step_avg:128.29ms
step:1191/1395 train_time:151522ms step_avg:128.30ms
step:1192/1395 train_time:151657ms step_avg:128.31ms
step:1193/1395 train_time:151791ms step_avg:128.31ms
step:1194/1395 train_time:151926ms step_avg:128.32ms
step:1195/1395 train_time:152060ms step_avg:128.32ms
step:1196/1395 train_time:152194ms step_avg:128.33ms
step:1197/1395 train_time:152329ms step_avg:128.33ms
step:1198/1395 train_time:152466ms step_avg:128.34ms
step:1199/1395 train_time:152599ms step_avg:128.34ms
step:1200/1395 train_time:152733ms step_avg:128.35ms
step:1201/1395 train_time:152866ms step_avg:128.35ms
step:1202/1395 train_time:153005ms step_avg:128.36ms
step:1203/1395 train_time:153143ms step_avg:128.37ms
step:1204/1395 train_time:153277ms step_avg:128.37ms
step:1205/1395 train_time:153413ms step_avg:128.38ms
step:1206/1395 train_time:153547ms step_avg:128.38ms
step:1207/1395 train_time:153680ms step_avg:128.39ms
step:1208/1395 train_time:153815ms step_avg:128.39ms
step:1209/1395 train_time:153948ms step_avg:128.40ms
step:1210/1395 train_time:154085ms step_avg:128.40ms
step:1211/1395 train_time:154219ms step_avg:128.41ms
step:1212/1395 train_time:154353ms step_avg:128.41ms
step:1213/1395 train_time:154487ms step_avg:128.42ms
step:1214/1395 train_time:154622ms step_avg:128.42ms
step:1215/1395 train_time:154758ms step_avg:128.43ms
step:1216/1395 train_time:154891ms step_avg:128.43ms
step:1217/1395 train_time:155026ms step_avg:128.44ms
step:1218/1395 train_time:155159ms step_avg:128.44ms
step:1219/1395 train_time:155292ms step_avg:128.45ms
step:1220/1395 train_time:155426ms step_avg:128.45ms
step:1221/1395 train_time:155560ms step_avg:128.46ms
step:1222/1395 train_time:155694ms step_avg:128.46ms
step:1223/1395 train_time:155829ms step_avg:128.47ms
step:1224/1395 train_time:155964ms step_avg:128.47ms
step:1225/1395 train_time:156100ms step_avg:128.48ms
step:1226/1395 train_time:156235ms step_avg:128.48ms
step:1227/1395 train_time:156369ms step_avg:128.49ms
step:1228/1395 train_time:156503ms step_avg:128.49ms
step:1229/1395 train_time:156636ms step_avg:128.50ms
step:1230/1395 train_time:156772ms step_avg:128.50ms
step:1231/1395 train_time:156907ms step_avg:128.51ms
step:1232/1395 train_time:157042ms step_avg:128.51ms
step:1233/1395 train_time:157176ms step_avg:128.52ms
step:1234/1395 train_time:157310ms step_avg:128.52ms
step:1235/1395 train_time:157444ms step_avg:128.53ms
step:1236/1395 train_time:157580ms step_avg:128.53ms
step:1237/1395 train_time:157713ms step_avg:128.54ms
step:1238/1395 train_time:157850ms step_avg:128.54ms
step:1239/1395 train_time:157984ms step_avg:128.55ms
step:1240/1395 train_time:158119ms step_avg:128.55ms
step:1241/1395 train_time:158255ms step_avg:128.56ms
step:1242/1395 train_time:158388ms step_avg:128.56ms
step:1243/1395 train_time:158523ms step_avg:128.57ms
step:1244/1395 train_time:158657ms step_avg:128.57ms
step:1245/1395 train_time:158791ms step_avg:128.58ms
step:1246/1395 train_time:158925ms step_avg:128.58ms
step:1247/1395 train_time:159060ms step_avg:128.59ms
step:1248/1395 train_time:159194ms step_avg:128.59ms
step:1249/1395 train_time:159327ms step_avg:128.59ms
step:1250/1395 train_time:159461ms step_avg:128.60ms
step:1250/1395 val_loss:3.3160 train_time:159595ms step_avg:128.71ms
step:1251/1395 train_time:159616ms step_avg:128.62ms
step:1252/1395 train_time:159745ms step_avg:128.62ms
step:1253/1395 train_time:159878ms step_avg:128.62ms
step:1254/1395 train_time:160011ms step_avg:128.63ms
step:1255/1395 train_time:160149ms step_avg:128.63ms
step:1256/1395 train_time:160283ms step_avg:128.64ms
step:1257/1395 train_time:160417ms step_avg:128.64ms
step:1258/1395 train_time:160551ms step_avg:128.65ms
step:1259/1395 train_time:160688ms step_avg:128.65ms
step:1260/1395 train_time:160823ms step_avg:128.66ms
step:1261/1395 train_time:160957ms step_avg:128.66ms
step:1262/1395 train_time:161092ms step_avg:128.67ms
step:1263/1395 train_time:161227ms step_avg:128.67ms
step:1264/1395 train_time:161360ms step_avg:128.68ms
step:1265/1395 train_time:161494ms step_avg:128.68ms
step:1266/1395 train_time:161629ms step_avg:128.69ms
step:1267/1395 train_time:161763ms step_avg:128.69ms
step:1268/1395 train_time:161897ms step_avg:128.69ms
step:1269/1395 train_time:162032ms step_avg:128.70ms
step:1270/1395 train_time:162167ms step_avg:128.70ms
step:1271/1395 train_time:162301ms step_avg:128.71ms
step:1272/1395 train_time:162436ms step_avg:128.71ms
step:1273/1395 train_time:162570ms step_avg:128.72ms
step:1274/1395 train_time:162703ms step_avg:128.72ms
step:1275/1395 train_time:162839ms step_avg:128.73ms
step:1276/1395 train_time:162973ms step_avg:128.73ms
step:1277/1395 train_time:163108ms step_avg:128.74ms
step:1278/1395 train_time:163243ms step_avg:128.74ms
step:1279/1395 train_time:163376ms step_avg:128.74ms
step:1280/1395 train_time:163512ms step_avg:128.75ms
step:1281/1395 train_time:163647ms step_avg:128.75ms
step:1282/1395 train_time:163780ms step_avg:128.76ms
step:1283/1395 train_time:163914ms step_avg:128.76ms
step:1284/1395 train_time:164048ms step_avg:128.77ms
step:1285/1395 train_time:164184ms step_avg:128.77ms
step:1286/1395 train_time:164319ms step_avg:128.78ms
step:1287/1395 train_time:164454ms step_avg:128.78ms
step:1288/1395 train_time:164588ms step_avg:128.79ms
step:1289/1395 train_time:164724ms step_avg:128.79ms
step:1290/1395 train_time:164860ms step_avg:128.80ms
step:1291/1395 train_time:164995ms step_avg:128.80ms
step:1292/1395 train_time:165129ms step_avg:128.81ms
step:1293/1395 train_time:165266ms step_avg:128.81ms
step:1294/1395 train_time:165399ms step_avg:128.82ms
step:1295/1395 train_time:165535ms step_avg:128.82ms
step:1296/1395 train_time:165670ms step_avg:128.83ms
step:1297/1395 train_time:165805ms step_avg:128.83ms
step:1298/1395 train_time:165939ms step_avg:128.83ms
step:1299/1395 train_time:166074ms step_avg:128.84ms
step:1300/1395 train_time:166208ms step_avg:128.84ms
step:1301/1395 train_time:166343ms step_avg:128.85ms
step:1302/1395 train_time:166477ms step_avg:128.85ms
step:1303/1395 train_time:166615ms step_avg:128.86ms
step:1304/1395 train_time:166751ms step_avg:128.86ms
step:1305/1395 train_time:166885ms step_avg:128.87ms
step:1306/1395 train_time:167020ms step_avg:128.87ms
step:1307/1395 train_time:167156ms step_avg:128.88ms
step:1308/1395 train_time:167291ms step_avg:128.88ms
step:1309/1395 train_time:167425ms step_avg:128.89ms
step:1310/1395 train_time:167562ms step_avg:128.89ms
step:1311/1395 train_time:167697ms step_avg:128.90ms
step:1312/1395 train_time:167830ms step_avg:128.90ms
step:1313/1395 train_time:167964ms step_avg:128.91ms
step:1314/1395 train_time:168099ms step_avg:128.91ms
step:1315/1395 train_time:168236ms step_avg:128.92ms
step:1316/1395 train_time:168369ms step_avg:128.92ms
step:1317/1395 train_time:168504ms step_avg:128.92ms
step:1318/1395 train_time:168640ms step_avg:128.93ms
step:1319/1395 train_time:168776ms step_avg:128.94ms
step:1320/1395 train_time:168909ms step_avg:128.94ms
step:1321/1395 train_time:169044ms step_avg:128.94ms
step:1322/1395 train_time:169180ms step_avg:128.95ms
step:1323/1395 train_time:169313ms step_avg:128.95ms
step:1324/1395 train_time:169447ms step_avg:128.96ms
step:1325/1395 train_time:169582ms step_avg:128.96ms
step:1326/1395 train_time:169717ms step_avg:128.96ms
step:1327/1395 train_time:169851ms step_avg:128.97ms
step:1328/1395 train_time:169984ms step_avg:128.97ms
step:1329/1395 train_time:170124ms step_avg:128.98ms
step:1330/1395 train_time:170259ms step_avg:128.98ms
step:1331/1395 train_time:170397ms step_avg:128.99ms
step:1332/1395 train_time:170534ms step_avg:129.00ms
step:1333/1395 train_time:170669ms step_avg:129.00ms
step:1334/1395 train_time:170803ms step_avg:129.01ms
step:1335/1395 train_time:170936ms step_avg:129.01ms
step:1336/1395 train_time:171074ms step_avg:129.01ms
step:1337/1395 train_time:171208ms step_avg:129.02ms
step:1338/1395 train_time:171343ms step_avg:129.02ms
step:1339/1395 train_time:171477ms step_avg:129.03ms
step:1340/1395 train_time:171615ms step_avg:129.03ms
step:1341/1395 train_time:171748ms step_avg:129.04ms
step:1342/1395 train_time:171882ms step_avg:129.04ms
step:1343/1395 train_time:172018ms step_avg:129.05ms
step:1344/1395 train_time:172151ms step_avg:129.05ms
step:1345/1395 train_time:172287ms step_avg:129.05ms
step:1346/1395 train_time:172422ms step_avg:129.06ms
step:1347/1395 train_time:172559ms step_avg:129.06ms
step:1348/1395 train_time:172693ms step_avg:129.07ms
step:1349/1395 train_time:172829ms step_avg:129.07ms
step:1350/1395 train_time:172964ms step_avg:129.08ms
step:1351/1395 train_time:173099ms step_avg:129.08ms
step:1352/1395 train_time:173238ms step_avg:129.09ms
step:1353/1395 train_time:173374ms step_avg:129.09ms
step:1354/1395 train_time:173510ms step_avg:129.10ms
step:1355/1395 train_time:173645ms step_avg:129.10ms
step:1356/1395 train_time:173778ms step_avg:129.11ms
step:1357/1395 train_time:173915ms step_avg:129.11ms
step:1358/1395 train_time:174052ms step_avg:129.12ms
step:1359/1395 train_time:174187ms step_avg:129.12ms
step:1360/1395 train_time:174324ms step_avg:129.13ms
step:1361/1395 train_time:174460ms step_avg:129.13ms
step:1362/1395 train_time:174599ms step_avg:129.14ms
step:1363/1395 train_time:174737ms step_avg:129.15ms
step:1364/1395 train_time:174873ms step_avg:129.15ms
step:1365/1395 train_time:175006ms step_avg:129.16ms
step:1366/1395 train_time:175142ms step_avg:129.16ms
step:1367/1395 train_time:175277ms step_avg:129.17ms
step:1368/1395 train_time:175413ms step_avg:129.17ms
step:1369/1395 train_time:175551ms step_avg:129.18ms
step:1370/1395 train_time:175689ms step_avg:129.18ms
step:1371/1395 train_time:175825ms step_avg:129.19ms
step:1372/1395 train_time:175963ms step_avg:129.19ms
step:1373/1395 train_time:176098ms step_avg:129.20ms
step:1374/1395 train_time:176236ms step_avg:129.20ms
step:1375/1395 train_time:176369ms step_avg:129.21ms
step:1375/1395 val_loss:3.2818 train_time:176503ms step_avg:129.31ms
step:1376/1395 train_time:176524ms step_avg:129.23ms
step:1377/1395 train_time:176647ms step_avg:129.22ms
step:1378/1395 train_time:176783ms step_avg:129.23ms
step:1379/1395 train_time:176919ms step_avg:129.23ms
step:1380/1395 train_time:177054ms step_avg:129.24ms
step:1381/1395 train_time:177191ms step_avg:129.24ms
step:1382/1395 train_time:177328ms step_avg:129.25ms
step:1383/1395 train_time:177463ms step_avg:129.25ms
step:1384/1395 train_time:177601ms step_avg:129.26ms
step:1385/1395 train_time:177736ms step_avg:129.26ms
step:1386/1395 train_time:177871ms step_avg:129.27ms
step:1387/1395 train_time:178008ms step_avg:129.27ms
step:1388/1395 train_time:178144ms step_avg:129.28ms
step:1389/1395 train_time:178279ms step_avg:129.28ms
step:1390/1395 train_time:178416ms step_avg:129.29ms
step:1391/1395 train_time:178551ms step_avg:129.29ms
step:1392/1395 train_time:178687ms step_avg:129.30ms
step:1393/1395 train_time:178821ms step_avg:129.30ms
step:1394/1395 train_time:178957ms step_avg:129.30ms
step:1395/1395 train_time:179092ms step_avg:129.31ms
step:1395/1395 val_loss:3.2776 train_time:179227ms step_avg:129.41ms
peak memory allocated: 37653 MiB reserved: 39236 MiB
