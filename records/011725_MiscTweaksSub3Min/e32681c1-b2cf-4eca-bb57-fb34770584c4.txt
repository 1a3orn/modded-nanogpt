import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True

# -----------------------------------------------------------------------------
# Custom operators

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

def lm_head_fp8(x: Tensor, w: Tensor) -> Tensor:
    _x = x.flatten(0, -2)
    out: Tensor = torch.ops.nanogpt.mm(_x, w, x_s=2.0, w_s=32.0, grad_s=2.0**29)[0]
    return out.reshape(*x.shape[:-1], -1)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev():
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int):
        super().__init__(in_features, out_features, bias=False)

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x):
        return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len=65536):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        # self.c_q = CastedLinear(dim, dim)
        # self.c_k = CastedLinear(dim, dim)
        # self.c_v = CastedLinear(dim, dim)
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std
        self.qkv_w = nn.Parameter(torch.empty(3, dim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads) # dim // num_heads = head_dim
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # Set attention scale such that the minimum attainable attention entropy
        # (but not necessary the attention entropy itself) is close to 0. By @leloykun
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        # q = self.c_q(x).view(B, T, self.num_heads, -1)
        # k = self.c_k(x).view(B, T, self.num_heads, -1)
        # v = self.c_v(x).view(B, T, self.num_heads, -1)
        # qkv_weight = torch.cat([self.c_q.weight, self.c_k.weight, self.c_v.weight], dim=0).type_as(x)
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3*self.num_heads, -1).chunk(3, dim=-2)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, model_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(model_dim, num_heads, layer_idx) if layer_idx != 7 else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, ve, x0, block_mask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, num_embeddings: int, embedding_dim: int):
        super().__init__()
        self.embed = nn.ModuleList([nn.Embedding(num_embeddings, embedding_dim) for _ in range(3)])

    def forward(self, input_seq) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2], None, None, None, None, None, None, ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual learning
        self.value_embeds = ValueEmbedding(vocab_size, model_dim)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128))
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        assert input_seq.ndim == 1
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        docs = (input_seq == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        def create_doc_swc_block_masks(sliding_window_num_blocks: Tensor):
            kv_idx = block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & document_bm
            full_bm  = causal_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            def build_bm(sw_num_blocks: Tensor) -> BlockMask:
                return BlockMask.from_kv_blocks(
                    torch.clamp_max(kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                    kv_indices,
                    torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                    full_kv_indices,
                    BLOCK_SIZE=BLOCK_SIZE,
                    mask_mod=document_causal,
                )
            return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

        # Long-short SWA block masks by @leloykun & @YouJiacheng
        long_bm, short_bm = create_doc_swc_block_masks(sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = lm_head_fp8(x, self.lm_head.weight) if self.training else self.lm_head(x)
        # @Grad62304977 added tanh softcapping, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use cycle(files) if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    batch_size = 8*64*1024 # batch size in tokens
    num_iterations = 1395 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 64*1024 # FlexAttention sequence length
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
             if console:
                 print(s)
             print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_loader = distributed_data_generator(args.train_files, args.batch_size, rank, world_size)

model = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim >= 2]
embed_params = [model.embed.weight, *model.value_embeds.parameters()]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), fused=True, eps=1e-10)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(it: int):
    t = 1 - it / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_bs = world_size * args.seq_len
        assert args.val_tokens % val_bs == 0
        val_steps = args.val_tokens // val_bs
        val_loader = distributed_data_generator(args.val_files, val_bs, rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION BEGIN -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB"
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250110+cu126 compiled for CUDA 12.6
Thu Jan 16 21:03:50 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   38C    P0             121W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   28C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   38C    P0             119W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0             112W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   36C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1395 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1395 train_time:23698ms step_avg:nanms
step:2/1395 train_time:24587ms step_avg:nanms
step:3/1395 train_time:24708ms step_avg:nanms
step:4/1395 train_time:24828ms step_avg:nanms
step:5/1395 train_time:24950ms step_avg:nanms
step:6/1395 train_time:25071ms step_avg:nanms
step:7/1395 train_time:25193ms step_avg:nanms
step:8/1395 train_time:25314ms step_avg:nanms
step:9/1395 train_time:25436ms step_avg:nanms
step:10/1395 train_time:25558ms step_avg:nanms
step:11/1395 train_time:123ms step_avg:nanms
step:12/1395 train_time:244ms step_avg:nanms
step:13/1395 train_time:366ms step_avg:121.90ms
step:14/1395 train_time:488ms step_avg:121.92ms
step:15/1395 train_time:610ms step_avg:122.01ms
step:16/1395 train_time:732ms step_avg:122.02ms
step:17/1395 train_time:854ms step_avg:121.95ms
step:18/1395 train_time:975ms step_avg:121.90ms
step:19/1395 train_time:1096ms step_avg:121.81ms
step:20/1395 train_time:1219ms step_avg:121.90ms
step:21/1395 train_time:1341ms step_avg:121.91ms
step:22/1395 train_time:1463ms step_avg:121.92ms
step:23/1395 train_time:1585ms step_avg:121.90ms
step:24/1395 train_time:1708ms step_avg:121.98ms
step:25/1395 train_time:1829ms step_avg:121.92ms
step:26/1395 train_time:1952ms step_avg:121.98ms
step:27/1395 train_time:2073ms step_avg:121.94ms
step:28/1395 train_time:2195ms step_avg:121.96ms
step:29/1395 train_time:2318ms step_avg:121.99ms
step:30/1395 train_time:2440ms step_avg:122.01ms
step:31/1395 train_time:2562ms step_avg:122.00ms
step:32/1395 train_time:2685ms step_avg:122.04ms
step:33/1395 train_time:2806ms step_avg:122.01ms
step:34/1395 train_time:2928ms step_avg:122.00ms
step:35/1395 train_time:3050ms step_avg:121.98ms
step:36/1395 train_time:3171ms step_avg:121.97ms
step:37/1395 train_time:3294ms step_avg:122.00ms
step:38/1395 train_time:3416ms step_avg:122.01ms
step:39/1395 train_time:3539ms step_avg:122.03ms
step:40/1395 train_time:3661ms step_avg:122.03ms
step:41/1395 train_time:3783ms step_avg:122.03ms
step:42/1395 train_time:3905ms step_avg:122.02ms
step:43/1395 train_time:4026ms step_avg:122.01ms
step:44/1395 train_time:4148ms step_avg:121.99ms
step:45/1395 train_time:4269ms step_avg:121.97ms
step:46/1395 train_time:4390ms step_avg:121.95ms
step:47/1395 train_time:4513ms step_avg:121.97ms
step:48/1395 train_time:4634ms step_avg:121.95ms
step:49/1395 train_time:4757ms step_avg:121.97ms
step:50/1395 train_time:4879ms step_avg:121.98ms
step:51/1395 train_time:5002ms step_avg:122.00ms
step:52/1395 train_time:5124ms step_avg:121.99ms
step:53/1395 train_time:5246ms step_avg:122.00ms
step:54/1395 train_time:5369ms step_avg:122.02ms
step:55/1395 train_time:5491ms step_avg:122.02ms
step:56/1395 train_time:5613ms step_avg:122.03ms
step:57/1395 train_time:5735ms step_avg:122.01ms
step:58/1395 train_time:5858ms step_avg:122.03ms
step:59/1395 train_time:5979ms step_avg:122.03ms
step:60/1395 train_time:6102ms step_avg:122.04ms
step:61/1395 train_time:6225ms step_avg:122.06ms
step:62/1395 train_time:6348ms step_avg:122.07ms
step:63/1395 train_time:6471ms step_avg:122.09ms
step:64/1395 train_time:6592ms step_avg:122.08ms
step:65/1395 train_time:6714ms step_avg:122.07ms
step:66/1395 train_time:6837ms step_avg:122.08ms
step:67/1395 train_time:6958ms step_avg:122.08ms
step:68/1395 train_time:7080ms step_avg:122.07ms
step:69/1395 train_time:7203ms step_avg:122.08ms
step:70/1395 train_time:7326ms step_avg:122.10ms
step:71/1395 train_time:7447ms step_avg:122.07ms
step:72/1395 train_time:7568ms step_avg:122.06ms
step:73/1395 train_time:7689ms step_avg:122.04ms
step:74/1395 train_time:7811ms step_avg:122.05ms
step:75/1395 train_time:7933ms step_avg:122.05ms
step:76/1395 train_time:8056ms step_avg:122.06ms
step:77/1395 train_time:8178ms step_avg:122.05ms
step:78/1395 train_time:8300ms step_avg:122.06ms
step:79/1395 train_time:8423ms step_avg:122.07ms
step:80/1395 train_time:8545ms step_avg:122.07ms
step:81/1395 train_time:8667ms step_avg:122.07ms
step:82/1395 train_time:8789ms step_avg:122.08ms
step:83/1395 train_time:8911ms step_avg:122.06ms
step:84/1395 train_time:9034ms step_avg:122.08ms
step:85/1395 train_time:9156ms step_avg:122.08ms
step:86/1395 train_time:9280ms step_avg:122.10ms
step:87/1395 train_time:9403ms step_avg:122.12ms
step:88/1395 train_time:9526ms step_avg:122.13ms
step:89/1395 train_time:9648ms step_avg:122.13ms
step:90/1395 train_time:9770ms step_avg:122.12ms
step:91/1395 train_time:9891ms step_avg:122.12ms
step:92/1395 train_time:10014ms step_avg:122.12ms
step:93/1395 train_time:10136ms step_avg:122.12ms
step:94/1395 train_time:10259ms step_avg:122.13ms
step:95/1395 train_time:10383ms step_avg:122.15ms
step:96/1395 train_time:10505ms step_avg:122.16ms
step:97/1395 train_time:10629ms step_avg:122.17ms
step:98/1395 train_time:10751ms step_avg:122.17ms
step:99/1395 train_time:10873ms step_avg:122.17ms
step:100/1395 train_time:10996ms step_avg:122.17ms
step:101/1395 train_time:11118ms step_avg:122.18ms
step:102/1395 train_time:11239ms step_avg:122.17ms
step:103/1395 train_time:11361ms step_avg:122.17ms
step:104/1395 train_time:11484ms step_avg:122.17ms
step:105/1395 train_time:11605ms step_avg:122.16ms
step:106/1395 train_time:11729ms step_avg:122.17ms
step:107/1395 train_time:11851ms step_avg:122.18ms
step:108/1395 train_time:11974ms step_avg:122.18ms
step:109/1395 train_time:12096ms step_avg:122.18ms
step:110/1395 train_time:12219ms step_avg:122.19ms
step:111/1395 train_time:12343ms step_avg:122.20ms
step:112/1395 train_time:12465ms step_avg:122.21ms
step:113/1395 train_time:12588ms step_avg:122.22ms
step:114/1395 train_time:12711ms step_avg:122.22ms
step:115/1395 train_time:12836ms step_avg:122.24ms
step:116/1395 train_time:12957ms step_avg:122.23ms
step:117/1395 train_time:13079ms step_avg:122.23ms
step:118/1395 train_time:13202ms step_avg:122.24ms
step:119/1395 train_time:13324ms step_avg:122.24ms
step:120/1395 train_time:13447ms step_avg:122.25ms
step:121/1395 train_time:13570ms step_avg:122.25ms
step:122/1395 train_time:13694ms step_avg:122.27ms
step:123/1395 train_time:13817ms step_avg:122.28ms
step:124/1395 train_time:13940ms step_avg:122.28ms
step:125/1395 train_time:14063ms step_avg:122.28ms
step:125/1395 val_loss:4.4003 train_time:14184ms step_avg:123.33ms
step:126/1395 train_time:14205ms step_avg:122.45ms
step:127/1395 train_time:14325ms step_avg:122.43ms
step:128/1395 train_time:14452ms step_avg:122.47ms
step:129/1395 train_time:14575ms step_avg:122.48ms
step:130/1395 train_time:14698ms step_avg:122.48ms
step:131/1395 train_time:14820ms step_avg:122.48ms
step:132/1395 train_time:14943ms step_avg:122.48ms
step:133/1395 train_time:15065ms step_avg:122.48ms
step:134/1395 train_time:15188ms step_avg:122.49ms
step:135/1395 train_time:15312ms step_avg:122.50ms
step:136/1395 train_time:15436ms step_avg:122.51ms
step:137/1395 train_time:15560ms step_avg:122.52ms
step:138/1395 train_time:15683ms step_avg:122.52ms
step:139/1395 train_time:15806ms step_avg:122.53ms
step:140/1395 train_time:15929ms step_avg:122.53ms
step:141/1395 train_time:16053ms step_avg:122.54ms
step:142/1395 train_time:16175ms step_avg:122.54ms
step:143/1395 train_time:16299ms step_avg:122.55ms
step:144/1395 train_time:16423ms step_avg:122.56ms
step:145/1395 train_time:16545ms step_avg:122.56ms
step:146/1395 train_time:16669ms step_avg:122.56ms
step:147/1395 train_time:16791ms step_avg:122.56ms
step:148/1395 train_time:16914ms step_avg:122.57ms
step:149/1395 train_time:17037ms step_avg:122.57ms
step:150/1395 train_time:17160ms step_avg:122.57ms
step:151/1395 train_time:17283ms step_avg:122.58ms
step:152/1395 train_time:17407ms step_avg:122.58ms
step:153/1395 train_time:17530ms step_avg:122.59ms
step:154/1395 train_time:17652ms step_avg:122.58ms
step:155/1395 train_time:17774ms step_avg:122.58ms
step:156/1395 train_time:17897ms step_avg:122.58ms
step:157/1395 train_time:18021ms step_avg:122.59ms
step:158/1395 train_time:18145ms step_avg:122.60ms
step:159/1395 train_time:18268ms step_avg:122.60ms
step:160/1395 train_time:18392ms step_avg:122.61ms
step:161/1395 train_time:18514ms step_avg:122.61ms
step:162/1395 train_time:18638ms step_avg:122.62ms
step:163/1395 train_time:18760ms step_avg:122.62ms
step:164/1395 train_time:18883ms step_avg:122.62ms
step:165/1395 train_time:19006ms step_avg:122.62ms
step:166/1395 train_time:19130ms step_avg:122.63ms
step:167/1395 train_time:19252ms step_avg:122.63ms
step:168/1395 train_time:19376ms step_avg:122.63ms
step:169/1395 train_time:19498ms step_avg:122.63ms
step:170/1395 train_time:19622ms step_avg:122.64ms
step:171/1395 train_time:19744ms step_avg:122.63ms
step:172/1395 train_time:19868ms step_avg:122.64ms
step:173/1395 train_time:19990ms step_avg:122.64ms
step:174/1395 train_time:20112ms step_avg:122.63ms
step:175/1395 train_time:20236ms step_avg:122.64ms
step:176/1395 train_time:20359ms step_avg:122.65ms
step:177/1395 train_time:20483ms step_avg:122.65ms
step:178/1395 train_time:20606ms step_avg:122.65ms
step:179/1395 train_time:20729ms step_avg:122.66ms
step:180/1395 train_time:20853ms step_avg:122.66ms
step:181/1395 train_time:20976ms step_avg:122.67ms
step:182/1395 train_time:21099ms step_avg:122.67ms
step:183/1395 train_time:21221ms step_avg:122.67ms
step:184/1395 train_time:21344ms step_avg:122.66ms
step:185/1395 train_time:21466ms step_avg:122.66ms
step:186/1395 train_time:21589ms step_avg:122.66ms
step:187/1395 train_time:21713ms step_avg:122.67ms
step:188/1395 train_time:21836ms step_avg:122.67ms
step:189/1395 train_time:21960ms step_avg:122.68ms
step:190/1395 train_time:22083ms step_avg:122.68ms
step:191/1395 train_time:22205ms step_avg:122.68ms
step:192/1395 train_time:22328ms step_avg:122.68ms
step:193/1395 train_time:22450ms step_avg:122.68ms
step:194/1395 train_time:22574ms step_avg:122.69ms
step:195/1395 train_time:22698ms step_avg:122.69ms
step:196/1395 train_time:22820ms step_avg:122.69ms
step:197/1395 train_time:22944ms step_avg:122.69ms
step:198/1395 train_time:23067ms step_avg:122.69ms
step:199/1395 train_time:23190ms step_avg:122.70ms
step:200/1395 train_time:23312ms step_avg:122.70ms
step:201/1395 train_time:23436ms step_avg:122.70ms
step:202/1395 train_time:23559ms step_avg:122.70ms
step:203/1395 train_time:23681ms step_avg:122.70ms
step:204/1395 train_time:23804ms step_avg:122.70ms
step:205/1395 train_time:23926ms step_avg:122.70ms
step:206/1395 train_time:24049ms step_avg:122.70ms
step:207/1395 train_time:24172ms step_avg:122.70ms
step:208/1395 train_time:24295ms step_avg:122.70ms
step:209/1395 train_time:24419ms step_avg:122.71ms
step:210/1395 train_time:24542ms step_avg:122.71ms
step:211/1395 train_time:24665ms step_avg:122.71ms
step:212/1395 train_time:24788ms step_avg:122.71ms
step:213/1395 train_time:24911ms step_avg:122.72ms
step:214/1395 train_time:25034ms step_avg:122.72ms
step:215/1395 train_time:25157ms step_avg:122.72ms
step:216/1395 train_time:25280ms step_avg:122.72ms
step:217/1395 train_time:25403ms step_avg:122.72ms
step:218/1395 train_time:25526ms step_avg:122.72ms
step:219/1395 train_time:25649ms step_avg:122.72ms
step:220/1395 train_time:25773ms step_avg:122.73ms
step:221/1395 train_time:25896ms step_avg:122.73ms
step:222/1395 train_time:26019ms step_avg:122.73ms
step:223/1395 train_time:26143ms step_avg:122.74ms
step:224/1395 train_time:26266ms step_avg:122.74ms
step:225/1395 train_time:26390ms step_avg:122.74ms
step:226/1395 train_time:26513ms step_avg:122.74ms
step:227/1395 train_time:26637ms step_avg:122.75ms
step:228/1395 train_time:26760ms step_avg:122.75ms
step:229/1395 train_time:26883ms step_avg:122.76ms
step:230/1395 train_time:27007ms step_avg:122.76ms
step:231/1395 train_time:27130ms step_avg:122.76ms
step:232/1395 train_time:27254ms step_avg:122.76ms
step:233/1395 train_time:27377ms step_avg:122.77ms
step:234/1395 train_time:27500ms step_avg:122.77ms
step:235/1395 train_time:27624ms step_avg:122.77ms
step:236/1395 train_time:27748ms step_avg:122.78ms
step:237/1395 train_time:27871ms step_avg:122.78ms
step:238/1395 train_time:27996ms step_avg:122.79ms
step:239/1395 train_time:28120ms step_avg:122.79ms
step:240/1395 train_time:28244ms step_avg:122.80ms
step:241/1395 train_time:28368ms step_avg:122.81ms
step:242/1395 train_time:28491ms step_avg:122.81ms
step:243/1395 train_time:28615ms step_avg:122.81ms
step:244/1395 train_time:28738ms step_avg:122.81ms
step:245/1395 train_time:28861ms step_avg:122.81ms
step:246/1395 train_time:28984ms step_avg:122.81ms
step:247/1395 train_time:29108ms step_avg:122.82ms
step:248/1395 train_time:29232ms step_avg:122.82ms
step:249/1395 train_time:29355ms step_avg:122.83ms
step:250/1395 train_time:29479ms step_avg:122.83ms
step:250/1395 val_loss:3.9816 train_time:29602ms step_avg:123.34ms
step:251/1395 train_time:29623ms step_avg:122.92ms
step:252/1395 train_time:29740ms step_avg:122.89ms
step:253/1395 train_time:29865ms step_avg:122.90ms
step:254/1395 train_time:29988ms step_avg:122.90ms
step:255/1395 train_time:30110ms step_avg:122.90ms
step:256/1395 train_time:30233ms step_avg:122.90ms
step:257/1395 train_time:30356ms step_avg:122.90ms
step:258/1395 train_time:30479ms step_avg:122.90ms
step:259/1395 train_time:30601ms step_avg:122.90ms
step:260/1395 train_time:30725ms step_avg:122.90ms
step:261/1395 train_time:30849ms step_avg:122.90ms
step:262/1395 train_time:30973ms step_avg:122.91ms
step:263/1395 train_time:31096ms step_avg:122.91ms
step:264/1395 train_time:31220ms step_avg:122.91ms
step:265/1395 train_time:31343ms step_avg:122.91ms
step:266/1395 train_time:31465ms step_avg:122.91ms
step:267/1395 train_time:31589ms step_avg:122.91ms
step:268/1395 train_time:31712ms step_avg:122.91ms
step:269/1395 train_time:31836ms step_avg:122.92ms
step:270/1395 train_time:31960ms step_avg:122.92ms
step:271/1395 train_time:32084ms step_avg:122.93ms
step:272/1395 train_time:32207ms step_avg:122.93ms
step:273/1395 train_time:32331ms step_avg:122.93ms
step:274/1395 train_time:32454ms step_avg:122.93ms
step:275/1395 train_time:32578ms step_avg:122.94ms
step:276/1395 train_time:32702ms step_avg:122.94ms
step:277/1395 train_time:32825ms step_avg:122.94ms
step:278/1395 train_time:32948ms step_avg:122.94ms
step:279/1395 train_time:33072ms step_avg:122.94ms
step:280/1395 train_time:33195ms step_avg:122.94ms
step:281/1395 train_time:33319ms step_avg:122.95ms
step:282/1395 train_time:33442ms step_avg:122.95ms
step:283/1395 train_time:33565ms step_avg:122.95ms
step:284/1395 train_time:33688ms step_avg:122.95ms
step:285/1395 train_time:33812ms step_avg:122.95ms
step:286/1395 train_time:33936ms step_avg:122.96ms
step:287/1395 train_time:34059ms step_avg:122.96ms
step:288/1395 train_time:34182ms step_avg:122.96ms
step:289/1395 train_time:34305ms step_avg:122.96ms
step:290/1395 train_time:34429ms step_avg:122.96ms
step:291/1395 train_time:34552ms step_avg:122.96ms
step:292/1395 train_time:34675ms step_avg:122.96ms
step:293/1395 train_time:34799ms step_avg:122.96ms
step:294/1395 train_time:34923ms step_avg:122.97ms
step:295/1395 train_time:35046ms step_avg:122.97ms
step:296/1395 train_time:35169ms step_avg:122.97ms
step:297/1395 train_time:35293ms step_avg:122.97ms
step:298/1395 train_time:35416ms step_avg:122.97ms
step:299/1395 train_time:35540ms step_avg:122.98ms
step:300/1395 train_time:35664ms step_avg:122.98ms
step:301/1395 train_time:35788ms step_avg:122.98ms
step:302/1395 train_time:35911ms step_avg:122.98ms
step:303/1395 train_time:36035ms step_avg:122.99ms
step:304/1395 train_time:36159ms step_avg:122.99ms
step:305/1395 train_time:36282ms step_avg:122.99ms
step:306/1395 train_time:36407ms step_avg:123.00ms
step:307/1395 train_time:36531ms step_avg:123.00ms
step:308/1395 train_time:36655ms step_avg:123.00ms
step:309/1395 train_time:36778ms step_avg:123.00ms
step:310/1395 train_time:36901ms step_avg:123.00ms
step:311/1395 train_time:37024ms step_avg:123.00ms
step:312/1395 train_time:37149ms step_avg:123.01ms
step:313/1395 train_time:37276ms step_avg:123.02ms
step:314/1395 train_time:37402ms step_avg:123.03ms
step:315/1395 train_time:37529ms step_avg:123.05ms
step:316/1395 train_time:37656ms step_avg:123.06ms
step:317/1395 train_time:37782ms step_avg:123.07ms
step:318/1395 train_time:37908ms step_avg:123.08ms
step:319/1395 train_time:38034ms step_avg:123.09ms
step:320/1395 train_time:38159ms step_avg:123.09ms
step:321/1395 train_time:38286ms step_avg:123.10ms
step:322/1395 train_time:38412ms step_avg:123.11ms
step:323/1395 train_time:38538ms step_avg:123.12ms
step:324/1395 train_time:38664ms step_avg:123.13ms
step:325/1395 train_time:38789ms step_avg:123.14ms
step:326/1395 train_time:38916ms step_avg:123.15ms
step:327/1395 train_time:39042ms step_avg:123.16ms
step:328/1395 train_time:39168ms step_avg:123.17ms
step:329/1395 train_time:39294ms step_avg:123.18ms
step:330/1395 train_time:39419ms step_avg:123.18ms
step:331/1395 train_time:39546ms step_avg:123.19ms
step:332/1395 train_time:39672ms step_avg:123.21ms
step:333/1395 train_time:39797ms step_avg:123.21ms
step:334/1395 train_time:39923ms step_avg:123.22ms
step:335/1395 train_time:40050ms step_avg:123.23ms
step:336/1395 train_time:40176ms step_avg:123.24ms
step:337/1395 train_time:40303ms step_avg:123.25ms
step:338/1395 train_time:40429ms step_avg:123.26ms
step:339/1395 train_time:40557ms step_avg:123.27ms
step:340/1395 train_time:40682ms step_avg:123.28ms
step:341/1395 train_time:40808ms step_avg:123.29ms
step:342/1395 train_time:40934ms step_avg:123.29ms
step:343/1395 train_time:41059ms step_avg:123.30ms
step:344/1395 train_time:41185ms step_avg:123.31ms
step:345/1395 train_time:41311ms step_avg:123.32ms
step:346/1395 train_time:41438ms step_avg:123.33ms
step:347/1395 train_time:41564ms step_avg:123.34ms
step:348/1395 train_time:41690ms step_avg:123.34ms
step:349/1395 train_time:41816ms step_avg:123.35ms
step:350/1395 train_time:41942ms step_avg:123.36ms
step:351/1395 train_time:42068ms step_avg:123.37ms
step:352/1395 train_time:42195ms step_avg:123.38ms
step:353/1395 train_time:42321ms step_avg:123.38ms
step:354/1395 train_time:42448ms step_avg:123.40ms
step:355/1395 train_time:42574ms step_avg:123.40ms
step:356/1395 train_time:42700ms step_avg:123.41ms
step:357/1395 train_time:42826ms step_avg:123.42ms
step:358/1395 train_time:42952ms step_avg:123.43ms
step:359/1395 train_time:43079ms step_avg:123.44ms
step:360/1395 train_time:43205ms step_avg:123.44ms
step:361/1395 train_time:43330ms step_avg:123.45ms
step:362/1395 train_time:43456ms step_avg:123.45ms
step:363/1395 train_time:43583ms step_avg:123.46ms
step:364/1395 train_time:43710ms step_avg:123.48ms
step:365/1395 train_time:43836ms step_avg:123.48ms
step:366/1395 train_time:43962ms step_avg:123.49ms
step:367/1395 train_time:44088ms step_avg:123.50ms
step:368/1395 train_time:44214ms step_avg:123.50ms
step:369/1395 train_time:44340ms step_avg:123.51ms
step:370/1395 train_time:44466ms step_avg:123.52ms
step:371/1395 train_time:44591ms step_avg:123.52ms
step:372/1395 train_time:44718ms step_avg:123.53ms
step:373/1395 train_time:44844ms step_avg:123.54ms
step:374/1395 train_time:44970ms step_avg:123.54ms
step:375/1395 train_time:45096ms step_avg:123.55ms
step:375/1395 val_loss:3.7844 train_time:45220ms step_avg:123.89ms
step:376/1395 train_time:45241ms step_avg:123.61ms
step:377/1395 train_time:45360ms step_avg:123.60ms
step:378/1395 train_time:45488ms step_avg:123.61ms
step:379/1395 train_time:45613ms step_avg:123.61ms
step:380/1395 train_time:45739ms step_avg:123.62ms
step:381/1395 train_time:45864ms step_avg:123.62ms
step:382/1395 train_time:45989ms step_avg:123.63ms
step:383/1395 train_time:46114ms step_avg:123.63ms
step:384/1395 train_time:46240ms step_avg:123.64ms
step:385/1395 train_time:46369ms step_avg:123.65ms
step:386/1395 train_time:46494ms step_avg:123.65ms
step:387/1395 train_time:46620ms step_avg:123.66ms
step:388/1395 train_time:46747ms step_avg:123.67ms
step:389/1395 train_time:46873ms step_avg:123.67ms
step:390/1395 train_time:46998ms step_avg:123.68ms
step:391/1395 train_time:47124ms step_avg:123.69ms
step:392/1395 train_time:47250ms step_avg:123.69ms
step:393/1395 train_time:47377ms step_avg:123.70ms
step:394/1395 train_time:47503ms step_avg:123.71ms
step:395/1395 train_time:47629ms step_avg:123.71ms
step:396/1395 train_time:47755ms step_avg:123.72ms
step:397/1395 train_time:47881ms step_avg:123.72ms
step:398/1395 train_time:48006ms step_avg:123.73ms
step:399/1395 train_time:48132ms step_avg:123.73ms
step:400/1395 train_time:48259ms step_avg:123.74ms
step:401/1395 train_time:48384ms step_avg:123.74ms
step:402/1395 train_time:48511ms step_avg:123.75ms
step:403/1395 train_time:48636ms step_avg:123.76ms
step:404/1395 train_time:48762ms step_avg:123.76ms
step:405/1395 train_time:48888ms step_avg:123.77ms
step:406/1395 train_time:49014ms step_avg:123.77ms
step:407/1395 train_time:49139ms step_avg:123.78ms
step:408/1395 train_time:49265ms step_avg:123.78ms
step:409/1395 train_time:49391ms step_avg:123.79ms
step:410/1395 train_time:49518ms step_avg:123.80ms
step:411/1395 train_time:49645ms step_avg:123.80ms
step:412/1395 train_time:49772ms step_avg:123.81ms
step:413/1395 train_time:49898ms step_avg:123.82ms
step:414/1395 train_time:50024ms step_avg:123.82ms
step:415/1395 train_time:50150ms step_avg:123.83ms
step:416/1395 train_time:50277ms step_avg:123.83ms
step:417/1395 train_time:50404ms step_avg:123.84ms
step:418/1395 train_time:50530ms step_avg:123.85ms
step:419/1395 train_time:50656ms step_avg:123.85ms
step:420/1395 train_time:50782ms step_avg:123.86ms
step:421/1395 train_time:50908ms step_avg:123.86ms
step:422/1395 train_time:51035ms step_avg:123.87ms
step:423/1395 train_time:51161ms step_avg:123.88ms
step:424/1395 train_time:51286ms step_avg:123.88ms
step:425/1395 train_time:51415ms step_avg:123.89ms
step:426/1395 train_time:51541ms step_avg:123.90ms
step:427/1395 train_time:51667ms step_avg:123.90ms
step:428/1395 train_time:51794ms step_avg:123.91ms
step:429/1395 train_time:51921ms step_avg:123.92ms
step:430/1395 train_time:52046ms step_avg:123.92ms
step:431/1395 train_time:52172ms step_avg:123.93ms
step:432/1395 train_time:52298ms step_avg:123.93ms
step:433/1395 train_time:52425ms step_avg:123.94ms
step:434/1395 train_time:52551ms step_avg:123.94ms
step:435/1395 train_time:52677ms step_avg:123.95ms
step:436/1395 train_time:52804ms step_avg:123.95ms
step:437/1395 train_time:52930ms step_avg:123.96ms
step:438/1395 train_time:53057ms step_avg:123.96ms
step:439/1395 train_time:53183ms step_avg:123.97ms
step:440/1395 train_time:53309ms step_avg:123.97ms
step:441/1395 train_time:53435ms step_avg:123.98ms
step:442/1395 train_time:53561ms step_avg:123.98ms
step:443/1395 train_time:53687ms step_avg:123.99ms
step:444/1395 train_time:53813ms step_avg:123.99ms
step:445/1395 train_time:53941ms step_avg:124.00ms
step:446/1395 train_time:54067ms step_avg:124.01ms
step:447/1395 train_time:54193ms step_avg:124.01ms
step:448/1395 train_time:54320ms step_avg:124.02ms
step:449/1395 train_time:54446ms step_avg:124.02ms
step:450/1395 train_time:54573ms step_avg:124.03ms
step:451/1395 train_time:54699ms step_avg:124.03ms
step:452/1395 train_time:54825ms step_avg:124.04ms
step:453/1395 train_time:54952ms step_avg:124.04ms
step:454/1395 train_time:55079ms step_avg:124.05ms
step:455/1395 train_time:55205ms step_avg:124.06ms
step:456/1395 train_time:55332ms step_avg:124.06ms
step:457/1395 train_time:55458ms step_avg:124.07ms
step:458/1395 train_time:55585ms step_avg:124.07ms
step:459/1395 train_time:55711ms step_avg:124.08ms
step:460/1395 train_time:55837ms step_avg:124.08ms
step:461/1395 train_time:55963ms step_avg:124.09ms
step:462/1395 train_time:56090ms step_avg:124.09ms
step:463/1395 train_time:56216ms step_avg:124.10ms
step:464/1395 train_time:56343ms step_avg:124.10ms
step:465/1395 train_time:56470ms step_avg:124.11ms
step:466/1395 train_time:56595ms step_avg:124.11ms
step:467/1395 train_time:56721ms step_avg:124.12ms
step:468/1395 train_time:56848ms step_avg:124.12ms
step:469/1395 train_time:56974ms step_avg:124.13ms
step:470/1395 train_time:57100ms step_avg:124.13ms
step:471/1395 train_time:57227ms step_avg:124.14ms
step:472/1395 train_time:57355ms step_avg:124.14ms
step:473/1395 train_time:57480ms step_avg:124.15ms
step:474/1395 train_time:57606ms step_avg:124.15ms
step:475/1395 train_time:57732ms step_avg:124.15ms
step:476/1395 train_time:57859ms step_avg:124.16ms
step:477/1395 train_time:57986ms step_avg:124.17ms
step:478/1395 train_time:58112ms step_avg:124.17ms
step:479/1395 train_time:58239ms step_avg:124.18ms
step:480/1395 train_time:58365ms step_avg:124.18ms
step:481/1395 train_time:58491ms step_avg:124.19ms
step:482/1395 train_time:58617ms step_avg:124.19ms
step:483/1395 train_time:58742ms step_avg:124.19ms
step:484/1395 train_time:58869ms step_avg:124.20ms
step:485/1395 train_time:58996ms step_avg:124.20ms
step:486/1395 train_time:59123ms step_avg:124.21ms
step:487/1395 train_time:59249ms step_avg:124.21ms
step:488/1395 train_time:59375ms step_avg:124.21ms
step:489/1395 train_time:59501ms step_avg:124.22ms
step:490/1395 train_time:59628ms step_avg:124.22ms
step:491/1395 train_time:59754ms step_avg:124.23ms
step:492/1395 train_time:59879ms step_avg:124.23ms
step:493/1395 train_time:60005ms step_avg:124.23ms
step:494/1395 train_time:60133ms step_avg:124.24ms
step:495/1395 train_time:60259ms step_avg:124.25ms
step:496/1395 train_time:60385ms step_avg:124.25ms
step:497/1395 train_time:60513ms step_avg:124.26ms
step:498/1395 train_time:60639ms step_avg:124.26ms
step:499/1395 train_time:60766ms step_avg:124.27ms
step:500/1395 train_time:60892ms step_avg:124.27ms
step:500/1395 val_loss:3.6652 train_time:61018ms step_avg:124.53ms
step:501/1395 train_time:61039ms step_avg:124.32ms
step:502/1395 train_time:61157ms step_avg:124.30ms
step:503/1395 train_time:61285ms step_avg:124.31ms
step:504/1395 train_time:61411ms step_avg:124.31ms
step:505/1395 train_time:61537ms step_avg:124.32ms
step:506/1395 train_time:61663ms step_avg:124.32ms
step:507/1395 train_time:61788ms step_avg:124.32ms
step:508/1395 train_time:61914ms step_avg:124.33ms
step:509/1395 train_time:62040ms step_avg:124.33ms
step:510/1395 train_time:62167ms step_avg:124.33ms
step:511/1395 train_time:62293ms step_avg:124.34ms
step:512/1395 train_time:62419ms step_avg:124.34ms
step:513/1395 train_time:62546ms step_avg:124.35ms
step:514/1395 train_time:62672ms step_avg:124.35ms
step:515/1395 train_time:62798ms step_avg:124.35ms
step:516/1395 train_time:62924ms step_avg:124.36ms
step:517/1395 train_time:63050ms step_avg:124.36ms
step:518/1395 train_time:63175ms step_avg:124.36ms
step:519/1395 train_time:63306ms step_avg:124.37ms
step:520/1395 train_time:63434ms step_avg:124.38ms
step:521/1395 train_time:63564ms step_avg:124.39ms
step:522/1395 train_time:63693ms step_avg:124.40ms
step:523/1395 train_time:63821ms step_avg:124.41ms
step:524/1395 train_time:63949ms step_avg:124.41ms
step:525/1395 train_time:64077ms step_avg:124.42ms
step:526/1395 train_time:64205ms step_avg:124.43ms
step:527/1395 train_time:64333ms step_avg:124.44ms
step:528/1395 train_time:64462ms step_avg:124.44ms
step:529/1395 train_time:64592ms step_avg:124.45ms
step:530/1395 train_time:64721ms step_avg:124.46ms
step:531/1395 train_time:64849ms step_avg:124.47ms
step:532/1395 train_time:64977ms step_avg:124.48ms
step:533/1395 train_time:65106ms step_avg:124.49ms
step:534/1395 train_time:65234ms step_avg:124.49ms
step:535/1395 train_time:65364ms step_avg:124.50ms
step:536/1395 train_time:65492ms step_avg:124.51ms
step:537/1395 train_time:65621ms step_avg:124.52ms
step:538/1395 train_time:65750ms step_avg:124.53ms
step:539/1395 train_time:65880ms step_avg:124.54ms
step:540/1395 train_time:66008ms step_avg:124.54ms
step:541/1395 train_time:66135ms step_avg:124.55ms
step:542/1395 train_time:66265ms step_avg:124.56ms
step:543/1395 train_time:66394ms step_avg:124.57ms
step:544/1395 train_time:66521ms step_avg:124.57ms
step:545/1395 train_time:66650ms step_avg:124.58ms
step:546/1395 train_time:66778ms step_avg:124.59ms
step:547/1395 train_time:66906ms step_avg:124.59ms
step:548/1395 train_time:67035ms step_avg:124.60ms
step:549/1395 train_time:67163ms step_avg:124.61ms
step:550/1395 train_time:67291ms step_avg:124.61ms
step:551/1395 train_time:67419ms step_avg:124.62ms
step:552/1395 train_time:67547ms step_avg:124.63ms
step:553/1395 train_time:67676ms step_avg:124.63ms
step:554/1395 train_time:67805ms step_avg:124.64ms
step:555/1395 train_time:67934ms step_avg:124.65ms
step:556/1395 train_time:68063ms step_avg:124.66ms
step:557/1395 train_time:68191ms step_avg:124.66ms
step:558/1395 train_time:68319ms step_avg:124.67ms
step:559/1395 train_time:68447ms step_avg:124.68ms
step:560/1395 train_time:68575ms step_avg:124.68ms
step:561/1395 train_time:68703ms step_avg:124.69ms
step:562/1395 train_time:68832ms step_avg:124.70ms
step:563/1395 train_time:68961ms step_avg:124.70ms
step:564/1395 train_time:69089ms step_avg:124.71ms
step:565/1395 train_time:69218ms step_avg:124.72ms
step:566/1395 train_time:69346ms step_avg:124.72ms
step:567/1395 train_time:69474ms step_avg:124.73ms
step:568/1395 train_time:69601ms step_avg:124.73ms
step:569/1395 train_time:69731ms step_avg:124.74ms
step:570/1395 train_time:69860ms step_avg:124.75ms
step:571/1395 train_time:69988ms step_avg:124.76ms
step:572/1395 train_time:70116ms step_avg:124.76ms
step:573/1395 train_time:70244ms step_avg:124.77ms
step:574/1395 train_time:70374ms step_avg:124.78ms
step:575/1395 train_time:70503ms step_avg:124.78ms
step:576/1395 train_time:70631ms step_avg:124.79ms
step:577/1395 train_time:70759ms step_avg:124.79ms
step:578/1395 train_time:70887ms step_avg:124.80ms
step:579/1395 train_time:71015ms step_avg:124.81ms
step:580/1395 train_time:71144ms step_avg:124.81ms
step:581/1395 train_time:71273ms step_avg:124.82ms
step:582/1395 train_time:71401ms step_avg:124.83ms
step:583/1395 train_time:71531ms step_avg:124.84ms
step:584/1395 train_time:71659ms step_avg:124.84ms
step:585/1395 train_time:71787ms step_avg:124.85ms
step:586/1395 train_time:71915ms step_avg:124.85ms
step:587/1395 train_time:72045ms step_avg:124.86ms
step:588/1395 train_time:72173ms step_avg:124.87ms
step:589/1395 train_time:72300ms step_avg:124.87ms
step:590/1395 train_time:72429ms step_avg:124.88ms
step:591/1395 train_time:72557ms step_avg:124.88ms
step:592/1395 train_time:72687ms step_avg:124.89ms
step:593/1395 train_time:72815ms step_avg:124.90ms
step:594/1395 train_time:72943ms step_avg:124.90ms
step:595/1395 train_time:73072ms step_avg:124.91ms
step:596/1395 train_time:73201ms step_avg:124.92ms
step:597/1395 train_time:73329ms step_avg:124.92ms
step:598/1395 train_time:73457ms step_avg:124.93ms
step:599/1395 train_time:73587ms step_avg:124.93ms
step:600/1395 train_time:73715ms step_avg:124.94ms
step:601/1395 train_time:73842ms step_avg:124.94ms
step:602/1395 train_time:73971ms step_avg:124.95ms
step:603/1395 train_time:74100ms step_avg:124.96ms
step:604/1395 train_time:74229ms step_avg:124.96ms
step:605/1395 train_time:74357ms step_avg:124.97ms
step:606/1395 train_time:74485ms step_avg:124.97ms
step:607/1395 train_time:74613ms step_avg:124.98ms
step:608/1395 train_time:74742ms step_avg:124.99ms
step:609/1395 train_time:74870ms step_avg:124.99ms
step:610/1395 train_time:75000ms step_avg:125.00ms
step:611/1395 train_time:75128ms step_avg:125.00ms
step:612/1395 train_time:75256ms step_avg:125.01ms
step:613/1395 train_time:75384ms step_avg:125.01ms
step:614/1395 train_time:75512ms step_avg:125.02ms
step:615/1395 train_time:75640ms step_avg:125.02ms
step:616/1395 train_time:75769ms step_avg:125.03ms
step:617/1395 train_time:75897ms step_avg:125.04ms
step:618/1395 train_time:76027ms step_avg:125.04ms
step:619/1395 train_time:76155ms step_avg:125.05ms
step:620/1395 train_time:76283ms step_avg:125.05ms
step:621/1395 train_time:76412ms step_avg:125.06ms
step:622/1395 train_time:76540ms step_avg:125.06ms
step:623/1395 train_time:76668ms step_avg:125.07ms
step:624/1395 train_time:76797ms step_avg:125.08ms
step:625/1395 train_time:76926ms step_avg:125.08ms
step:625/1395 val_loss:3.5837 train_time:77053ms step_avg:125.29ms
step:626/1395 train_time:77074ms step_avg:125.12ms
step:627/1395 train_time:77197ms step_avg:125.12ms
step:628/1395 train_time:77326ms step_avg:125.12ms
step:629/1395 train_time:77454ms step_avg:125.13ms
step:630/1395 train_time:77582ms step_avg:125.13ms
step:631/1395 train_time:77710ms step_avg:125.14ms
step:632/1395 train_time:77838ms step_avg:125.14ms
step:633/1395 train_time:77967ms step_avg:125.15ms
step:634/1395 train_time:78096ms step_avg:125.15ms
step:635/1395 train_time:78227ms step_avg:125.16ms
step:636/1395 train_time:78356ms step_avg:125.17ms
step:637/1395 train_time:78486ms step_avg:125.18ms
step:638/1395 train_time:78615ms step_avg:125.18ms
step:639/1395 train_time:78743ms step_avg:125.19ms
step:640/1395 train_time:78871ms step_avg:125.19ms
step:641/1395 train_time:78999ms step_avg:125.20ms
step:642/1395 train_time:79128ms step_avg:125.20ms
step:643/1395 train_time:79256ms step_avg:125.21ms
step:644/1395 train_time:79386ms step_avg:125.21ms
step:645/1395 train_time:79515ms step_avg:125.22ms
step:646/1395 train_time:79644ms step_avg:125.23ms
step:647/1395 train_time:79773ms step_avg:125.23ms
step:648/1395 train_time:79902ms step_avg:125.24ms
step:649/1395 train_time:80030ms step_avg:125.24ms
step:650/1395 train_time:80158ms step_avg:125.25ms
step:651/1395 train_time:80288ms step_avg:125.25ms
step:652/1395 train_time:80417ms step_avg:125.26ms
step:653/1395 train_time:80546ms step_avg:125.27ms
step:654/1395 train_time:80674ms step_avg:125.27ms
step:655/1395 train_time:80803ms step_avg:125.28ms
step:656/1395 train_time:80931ms step_avg:125.28ms
step:657/1395 train_time:81060ms step_avg:125.29ms
step:658/1395 train_time:81189ms step_avg:125.29ms
step:659/1395 train_time:81317ms step_avg:125.30ms
step:660/1395 train_time:81447ms step_avg:125.30ms
step:661/1395 train_time:81576ms step_avg:125.31ms
step:662/1395 train_time:81705ms step_avg:125.31ms
step:663/1395 train_time:81834ms step_avg:125.32ms
step:664/1395 train_time:81962ms step_avg:125.32ms
step:665/1395 train_time:82091ms step_avg:125.33ms
step:666/1395 train_time:82220ms step_avg:125.34ms
step:667/1395 train_time:82348ms step_avg:125.34ms
step:668/1395 train_time:82477ms step_avg:125.34ms
step:669/1395 train_time:82605ms step_avg:125.35ms
step:670/1395 train_time:82734ms step_avg:125.35ms
step:671/1395 train_time:82863ms step_avg:125.36ms
step:672/1395 train_time:82993ms step_avg:125.37ms
step:673/1395 train_time:83121ms step_avg:125.37ms
step:674/1395 train_time:83250ms step_avg:125.38ms
step:675/1395 train_time:83378ms step_avg:125.38ms
step:676/1395 train_time:83507ms step_avg:125.39ms
step:677/1395 train_time:83635ms step_avg:125.39ms
step:678/1395 train_time:83764ms step_avg:125.40ms
step:679/1395 train_time:83893ms step_avg:125.40ms
step:680/1395 train_time:84023ms step_avg:125.41ms
step:681/1395 train_time:84152ms step_avg:125.41ms
step:682/1395 train_time:84279ms step_avg:125.42ms
step:683/1395 train_time:84408ms step_avg:125.42ms
step:684/1395 train_time:84536ms step_avg:125.42ms
step:685/1395 train_time:84665ms step_avg:125.43ms
step:686/1395 train_time:84794ms step_avg:125.43ms
step:687/1395 train_time:84924ms step_avg:125.44ms
step:688/1395 train_time:85053ms step_avg:125.45ms
step:689/1395 train_time:85183ms step_avg:125.45ms
step:690/1395 train_time:85311ms step_avg:125.46ms
step:691/1395 train_time:85439ms step_avg:125.46ms
step:692/1395 train_time:85568ms step_avg:125.47ms
step:693/1395 train_time:85696ms step_avg:125.47ms
step:694/1395 train_time:85825ms step_avg:125.47ms
step:695/1395 train_time:85954ms step_avg:125.48ms
step:696/1395 train_time:86082ms step_avg:125.48ms
step:697/1395 train_time:86212ms step_avg:125.49ms
step:698/1395 train_time:86341ms step_avg:125.50ms
step:699/1395 train_time:86469ms step_avg:125.50ms
step:700/1395 train_time:86598ms step_avg:125.50ms
step:701/1395 train_time:86727ms step_avg:125.51ms
step:702/1395 train_time:86855ms step_avg:125.51ms
step:703/1395 train_time:86984ms step_avg:125.52ms
step:704/1395 train_time:87113ms step_avg:125.52ms
step:705/1395 train_time:87241ms step_avg:125.53ms
step:706/1395 train_time:87371ms step_avg:125.53ms
step:707/1395 train_time:87500ms step_avg:125.54ms
step:708/1395 train_time:87629ms step_avg:125.54ms
step:709/1395 train_time:87758ms step_avg:125.55ms
step:710/1395 train_time:87888ms step_avg:125.55ms
step:711/1395 train_time:88017ms step_avg:125.56ms
step:712/1395 train_time:88145ms step_avg:125.56ms
step:713/1395 train_time:88273ms step_avg:125.57ms
step:714/1395 train_time:88402ms step_avg:125.57ms
step:715/1395 train_time:88531ms step_avg:125.58ms
step:716/1395 train_time:88660ms step_avg:125.58ms
step:717/1395 train_time:88789ms step_avg:125.59ms
step:718/1395 train_time:88918ms step_avg:125.59ms
step:719/1395 train_time:89046ms step_avg:125.59ms
step:720/1395 train_time:89174ms step_avg:125.60ms
step:721/1395 train_time:89304ms step_avg:125.60ms
step:722/1395 train_time:89433ms step_avg:125.61ms
step:723/1395 train_time:89561ms step_avg:125.61ms
step:724/1395 train_time:89691ms step_avg:125.62ms
step:725/1395 train_time:89819ms step_avg:125.62ms
step:726/1395 train_time:89951ms step_avg:125.63ms
step:727/1395 train_time:90081ms step_avg:125.64ms
step:728/1395 train_time:90212ms step_avg:125.64ms
step:729/1395 train_time:90342ms step_avg:125.65ms
step:730/1395 train_time:90472ms step_avg:125.66ms
step:731/1395 train_time:90603ms step_avg:125.66ms
step:732/1395 train_time:90734ms step_avg:125.67ms
step:733/1395 train_time:90864ms step_avg:125.68ms
step:734/1395 train_time:90994ms step_avg:125.68ms
step:735/1395 train_time:91126ms step_avg:125.69ms
step:736/1395 train_time:91257ms step_avg:125.70ms
step:737/1395 train_time:91387ms step_avg:125.70ms
step:738/1395 train_time:91516ms step_avg:125.71ms
step:739/1395 train_time:91646ms step_avg:125.71ms
step:740/1395 train_time:91776ms step_avg:125.72ms
step:741/1395 train_time:91908ms step_avg:125.73ms
step:742/1395 train_time:92039ms step_avg:125.74ms
step:743/1395 train_time:92169ms step_avg:125.74ms
step:744/1395 train_time:92300ms step_avg:125.75ms
step:745/1395 train_time:92431ms step_avg:125.76ms
step:746/1395 train_time:92561ms step_avg:125.76ms
step:747/1395 train_time:92692ms step_avg:125.77ms
step:748/1395 train_time:92823ms step_avg:125.78ms
step:749/1395 train_time:92953ms step_avg:125.78ms
step:750/1395 train_time:93085ms step_avg:125.79ms
step:750/1395 val_loss:3.5301 train_time:93215ms step_avg:125.97ms
step:751/1395 train_time:93236ms step_avg:125.82ms
step:752/1395 train_time:93359ms step_avg:125.82ms
step:753/1395 train_time:93491ms step_avg:125.83ms
step:754/1395 train_time:93622ms step_avg:125.84ms
step:755/1395 train_time:93751ms step_avg:125.84ms
step:756/1395 train_time:93880ms step_avg:125.84ms
step:757/1395 train_time:94012ms step_avg:125.85ms
step:758/1395 train_time:94142ms step_avg:125.86ms
step:759/1395 train_time:94272ms step_avg:125.86ms
step:760/1395 train_time:94403ms step_avg:125.87ms
step:761/1395 train_time:94535ms step_avg:125.88ms
step:762/1395 train_time:94666ms step_avg:125.89ms
step:763/1395 train_time:94796ms step_avg:125.89ms
step:764/1395 train_time:94927ms step_avg:125.90ms
step:765/1395 train_time:95056ms step_avg:125.90ms
step:766/1395 train_time:95187ms step_avg:125.91ms
step:767/1395 train_time:95317ms step_avg:125.91ms
step:768/1395 train_time:95448ms step_avg:125.92ms
step:769/1395 train_time:95579ms step_avg:125.93ms
step:770/1395 train_time:95710ms step_avg:125.93ms
step:771/1395 train_time:95840ms step_avg:125.94ms
step:772/1395 train_time:95971ms step_avg:125.95ms
step:773/1395 train_time:96101ms step_avg:125.95ms
step:774/1395 train_time:96231ms step_avg:125.96ms
step:775/1395 train_time:96362ms step_avg:125.96ms
step:776/1395 train_time:96493ms step_avg:125.97ms
step:777/1395 train_time:96624ms step_avg:125.98ms
step:778/1395 train_time:96755ms step_avg:125.98ms
step:779/1395 train_time:96885ms step_avg:125.99ms
step:780/1395 train_time:97016ms step_avg:125.99ms
step:781/1395 train_time:97147ms step_avg:126.00ms
step:782/1395 train_time:97277ms step_avg:126.01ms
step:783/1395 train_time:97407ms step_avg:126.01ms
step:784/1395 train_time:97538ms step_avg:126.02ms
step:785/1395 train_time:97668ms step_avg:126.02ms
step:786/1395 train_time:97799ms step_avg:126.03ms
step:787/1395 train_time:97929ms step_avg:126.04ms
step:788/1395 train_time:98060ms step_avg:126.04ms
step:789/1395 train_time:98190ms step_avg:126.05ms
step:790/1395 train_time:98320ms step_avg:126.05ms
step:791/1395 train_time:98451ms step_avg:126.06ms
step:792/1395 train_time:98581ms step_avg:126.06ms
step:793/1395 train_time:98711ms step_avg:126.07ms
step:794/1395 train_time:98842ms step_avg:126.07ms
step:795/1395 train_time:98973ms step_avg:126.08ms
step:796/1395 train_time:99106ms step_avg:126.09ms
step:797/1395 train_time:99236ms step_avg:126.09ms
step:798/1395 train_time:99367ms step_avg:126.10ms
step:799/1395 train_time:99498ms step_avg:126.11ms
step:800/1395 train_time:99629ms step_avg:126.11ms
step:801/1395 train_time:99760ms step_avg:126.12ms
step:802/1395 train_time:99891ms step_avg:126.12ms
step:803/1395 train_time:100021ms step_avg:126.13ms
step:804/1395 train_time:100152ms step_avg:126.14ms
step:805/1395 train_time:100283ms step_avg:126.14ms
step:806/1395 train_time:100414ms step_avg:126.15ms
step:807/1395 train_time:100545ms step_avg:126.15ms
step:808/1395 train_time:100676ms step_avg:126.16ms
step:809/1395 train_time:100805ms step_avg:126.16ms
step:810/1395 train_time:100936ms step_avg:126.17ms
step:811/1395 train_time:101067ms step_avg:126.18ms
step:812/1395 train_time:101198ms step_avg:126.18ms
step:813/1395 train_time:101328ms step_avg:126.19ms
step:814/1395 train_time:101459ms step_avg:126.19ms
step:815/1395 train_time:101589ms step_avg:126.20ms
step:816/1395 train_time:101721ms step_avg:126.20ms
step:817/1395 train_time:101851ms step_avg:126.21ms
step:818/1395 train_time:101981ms step_avg:126.21ms
step:819/1395 train_time:102112ms step_avg:126.22ms
step:820/1395 train_time:102243ms step_avg:126.23ms
step:821/1395 train_time:102373ms step_avg:126.23ms
step:822/1395 train_time:102503ms step_avg:126.24ms
step:823/1395 train_time:102633ms step_avg:126.24ms
step:824/1395 train_time:102764ms step_avg:126.25ms
step:825/1395 train_time:102894ms step_avg:126.25ms
step:826/1395 train_time:103025ms step_avg:126.26ms
step:827/1395 train_time:103156ms step_avg:126.26ms
step:828/1395 train_time:103287ms step_avg:126.27ms
step:829/1395 train_time:103417ms step_avg:126.27ms
step:830/1395 train_time:103548ms step_avg:126.28ms
step:831/1395 train_time:103680ms step_avg:126.28ms
step:832/1395 train_time:103811ms step_avg:126.29ms
step:833/1395 train_time:103941ms step_avg:126.30ms
step:834/1395 train_time:104072ms step_avg:126.30ms
step:835/1395 train_time:104204ms step_avg:126.31ms
step:836/1395 train_time:104335ms step_avg:126.31ms
step:837/1395 train_time:104467ms step_avg:126.32ms
step:838/1395 train_time:104598ms step_avg:126.33ms
step:839/1395 train_time:104729ms step_avg:126.33ms
step:840/1395 train_time:104860ms step_avg:126.34ms
step:841/1395 train_time:104990ms step_avg:126.34ms
step:842/1395 train_time:105120ms step_avg:126.35ms
step:843/1395 train_time:105250ms step_avg:126.35ms
step:844/1395 train_time:105381ms step_avg:126.36ms
step:845/1395 train_time:105511ms step_avg:126.36ms
step:846/1395 train_time:105642ms step_avg:126.37ms
step:847/1395 train_time:105773ms step_avg:126.37ms
step:848/1395 train_time:105904ms step_avg:126.38ms
step:849/1395 train_time:106036ms step_avg:126.38ms
step:850/1395 train_time:106167ms step_avg:126.39ms
step:851/1395 train_time:106298ms step_avg:126.39ms
step:852/1395 train_time:106429ms step_avg:126.40ms
step:853/1395 train_time:106559ms step_avg:126.40ms
step:854/1395 train_time:106690ms step_avg:126.41ms
step:855/1395 train_time:106821ms step_avg:126.42ms
step:856/1395 train_time:106951ms step_avg:126.42ms
step:857/1395 train_time:107082ms step_avg:126.43ms
step:858/1395 train_time:107214ms step_avg:126.43ms
step:859/1395 train_time:107345ms step_avg:126.44ms
step:860/1395 train_time:107477ms step_avg:126.44ms
step:861/1395 train_time:107607ms step_avg:126.45ms
step:862/1395 train_time:107738ms step_avg:126.45ms
step:863/1395 train_time:107870ms step_avg:126.46ms
step:864/1395 train_time:108001ms step_avg:126.46ms
step:865/1395 train_time:108132ms step_avg:126.47ms
step:866/1395 train_time:108264ms step_avg:126.48ms
step:867/1395 train_time:108395ms step_avg:126.48ms
step:868/1395 train_time:108525ms step_avg:126.49ms
step:869/1395 train_time:108656ms step_avg:126.49ms
step:870/1395 train_time:108787ms step_avg:126.50ms
step:871/1395 train_time:108918ms step_avg:126.50ms
step:872/1395 train_time:109049ms step_avg:126.51ms
step:873/1395 train_time:109181ms step_avg:126.51ms
step:874/1395 train_time:109310ms step_avg:126.52ms
step:875/1395 train_time:109442ms step_avg:126.52ms
step:875/1395 val_loss:3.4774 train_time:109571ms step_avg:126.67ms
step:876/1395 train_time:109592ms step_avg:126.55ms
step:877/1395 train_time:109714ms step_avg:126.54ms
step:878/1395 train_time:109845ms step_avg:126.55ms
step:879/1395 train_time:109976ms step_avg:126.55ms
step:880/1395 train_time:110107ms step_avg:126.56ms
step:881/1395 train_time:110237ms step_avg:126.56ms
step:882/1395 train_time:110367ms step_avg:126.57ms
step:883/1395 train_time:110498ms step_avg:126.57ms
step:884/1395 train_time:110628ms step_avg:126.58ms
step:885/1395 train_time:110760ms step_avg:126.58ms
step:886/1395 train_time:110892ms step_avg:126.59ms
step:887/1395 train_time:111023ms step_avg:126.59ms
step:888/1395 train_time:111154ms step_avg:126.60ms
step:889/1395 train_time:111286ms step_avg:126.61ms
step:890/1395 train_time:111416ms step_avg:126.61ms
step:891/1395 train_time:111546ms step_avg:126.61ms
step:892/1395 train_time:111677ms step_avg:126.62ms
step:893/1395 train_time:111809ms step_avg:126.62ms
step:894/1395 train_time:111940ms step_avg:126.63ms
step:895/1395 train_time:112072ms step_avg:126.63ms
step:896/1395 train_time:112202ms step_avg:126.64ms
step:897/1395 train_time:112333ms step_avg:126.64ms
step:898/1395 train_time:112464ms step_avg:126.65ms
step:899/1395 train_time:112595ms step_avg:126.65ms
step:900/1395 train_time:112726ms step_avg:126.66ms
step:901/1395 train_time:112857ms step_avg:126.66ms
step:902/1395 train_time:112987ms step_avg:126.67ms
step:903/1395 train_time:113119ms step_avg:126.67ms
step:904/1395 train_time:113250ms step_avg:126.68ms
step:905/1395 train_time:113381ms step_avg:126.68ms
step:906/1395 train_time:113511ms step_avg:126.69ms
step:907/1395 train_time:113643ms step_avg:126.69ms
step:908/1395 train_time:113774ms step_avg:126.70ms
step:909/1395 train_time:113905ms step_avg:126.70ms
step:910/1395 train_time:114039ms step_avg:126.71ms
step:911/1395 train_time:114169ms step_avg:126.71ms
step:912/1395 train_time:114300ms step_avg:126.72ms
step:913/1395 train_time:114431ms step_avg:126.72ms
step:914/1395 train_time:114562ms step_avg:126.73ms
step:915/1395 train_time:114694ms step_avg:126.73ms
step:916/1395 train_time:114824ms step_avg:126.74ms
step:917/1395 train_time:114956ms step_avg:126.74ms
step:918/1395 train_time:115087ms step_avg:126.75ms
step:919/1395 train_time:115220ms step_avg:126.75ms
step:920/1395 train_time:115351ms step_avg:126.76ms
step:921/1395 train_time:115481ms step_avg:126.76ms
step:922/1395 train_time:115613ms step_avg:126.77ms
step:923/1395 train_time:115743ms step_avg:126.77ms
step:924/1395 train_time:115874ms step_avg:126.78ms
step:925/1395 train_time:116006ms step_avg:126.78ms
step:926/1395 train_time:116136ms step_avg:126.79ms
step:927/1395 train_time:116266ms step_avg:126.79ms
step:928/1395 train_time:116398ms step_avg:126.79ms
step:929/1395 train_time:116528ms step_avg:126.80ms
step:930/1395 train_time:116659ms step_avg:126.80ms
step:931/1395 train_time:116790ms step_avg:126.81ms
step:932/1395 train_time:116922ms step_avg:126.81ms
step:933/1395 train_time:117055ms step_avg:126.82ms
step:934/1395 train_time:117188ms step_avg:126.83ms
step:935/1395 train_time:117320ms step_avg:126.83ms
step:936/1395 train_time:117453ms step_avg:126.84ms
step:937/1395 train_time:117586ms step_avg:126.85ms
step:938/1395 train_time:117719ms step_avg:126.85ms
step:939/1395 train_time:117851ms step_avg:126.86ms
step:940/1395 train_time:117984ms step_avg:126.86ms
step:941/1395 train_time:118116ms step_avg:126.87ms
step:942/1395 train_time:118248ms step_avg:126.88ms
step:943/1395 train_time:118380ms step_avg:126.88ms
step:944/1395 train_time:118515ms step_avg:126.89ms
step:945/1395 train_time:118648ms step_avg:126.90ms
step:946/1395 train_time:118781ms step_avg:126.90ms
step:947/1395 train_time:118916ms step_avg:126.91ms
step:948/1395 train_time:119048ms step_avg:126.92ms
step:949/1395 train_time:119181ms step_avg:126.92ms
step:950/1395 train_time:119313ms step_avg:126.93ms
step:951/1395 train_time:119447ms step_avg:126.94ms
step:952/1395 train_time:119579ms step_avg:126.94ms
step:953/1395 train_time:119711ms step_avg:126.95ms
step:954/1395 train_time:119844ms step_avg:126.95ms
step:955/1395 train_time:119977ms step_avg:126.96ms
step:956/1395 train_time:120110ms step_avg:126.97ms
step:957/1395 train_time:120242ms step_avg:126.97ms
step:958/1395 train_time:120375ms step_avg:126.98ms
step:959/1395 train_time:120507ms step_avg:126.98ms
step:960/1395 train_time:120640ms step_avg:126.99ms
step:961/1395 train_time:120773ms step_avg:127.00ms
step:962/1395 train_time:120905ms step_avg:127.00ms
step:963/1395 train_time:121038ms step_avg:127.01ms
step:964/1395 train_time:121171ms step_avg:127.01ms
step:965/1395 train_time:121305ms step_avg:127.02ms
step:966/1395 train_time:121438ms step_avg:127.03ms
step:967/1395 train_time:121571ms step_avg:127.03ms
step:968/1395 train_time:121703ms step_avg:127.04ms
step:969/1395 train_time:121835ms step_avg:127.04ms
step:970/1395 train_time:121968ms step_avg:127.05ms
step:971/1395 train_time:122102ms step_avg:127.06ms
step:972/1395 train_time:122235ms step_avg:127.06ms
step:973/1395 train_time:122366ms step_avg:127.07ms
step:974/1395 train_time:122499ms step_avg:127.07ms
step:975/1395 train_time:122631ms step_avg:127.08ms
step:976/1395 train_time:122764ms step_avg:127.08ms
step:977/1395 train_time:122896ms step_avg:127.09ms
step:978/1395 train_time:123028ms step_avg:127.10ms
step:979/1395 train_time:123161ms step_avg:127.10ms
step:980/1395 train_time:123293ms step_avg:127.11ms
step:981/1395 train_time:123426ms step_avg:127.11ms
step:982/1395 train_time:123560ms step_avg:127.12ms
step:983/1395 train_time:123693ms step_avg:127.13ms
step:984/1395 train_time:123825ms step_avg:127.13ms
step:985/1395 train_time:123958ms step_avg:127.14ms
step:986/1395 train_time:124092ms step_avg:127.14ms
step:987/1395 train_time:124223ms step_avg:127.15ms
step:988/1395 train_time:124355ms step_avg:127.15ms
step:989/1395 train_time:124486ms step_avg:127.16ms
step:990/1395 train_time:124618ms step_avg:127.16ms
step:991/1395 train_time:124752ms step_avg:127.17ms
step:992/1395 train_time:124887ms step_avg:127.18ms
step:993/1395 train_time:125023ms step_avg:127.19ms
step:994/1395 train_time:125156ms step_avg:127.19ms
step:995/1395 train_time:125289ms step_avg:127.20ms
step:996/1395 train_time:125421ms step_avg:127.20ms
step:997/1395 train_time:125552ms step_avg:127.21ms
step:998/1395 train_time:125684ms step_avg:127.21ms
step:999/1395 train_time:125816ms step_avg:127.22ms
step:1000/1395 train_time:125949ms step_avg:127.22ms
step:1000/1395 val_loss:3.4154 train_time:126079ms step_avg:127.35ms
step:1001/1395 train_time:126100ms step_avg:127.25ms
step:1002/1395 train_time:126223ms step_avg:127.24ms
step:1003/1395 train_time:126358ms step_avg:127.25ms
step:1004/1395 train_time:126490ms step_avg:127.25ms
step:1005/1395 train_time:126623ms step_avg:127.26ms
step:1006/1395 train_time:126755ms step_avg:127.26ms
step:1007/1395 train_time:126886ms step_avg:127.27ms
step:1008/1395 train_time:127019ms step_avg:127.27ms
step:1009/1395 train_time:127154ms step_avg:127.28ms
step:1010/1395 train_time:127288ms step_avg:127.29ms
step:1011/1395 train_time:127423ms step_avg:127.30ms
step:1012/1395 train_time:127555ms step_avg:127.30ms
step:1013/1395 train_time:127687ms step_avg:127.30ms
step:1014/1395 train_time:127819ms step_avg:127.31ms
step:1015/1395 train_time:127951ms step_avg:127.31ms
step:1016/1395 train_time:128083ms step_avg:127.32ms
step:1017/1395 train_time:128215ms step_avg:127.32ms
step:1018/1395 train_time:128347ms step_avg:127.33ms
step:1019/1395 train_time:128480ms step_avg:127.33ms
step:1020/1395 train_time:128613ms step_avg:127.34ms
step:1021/1395 train_time:128745ms step_avg:127.34ms
step:1022/1395 train_time:128877ms step_avg:127.35ms
step:1023/1395 train_time:129011ms step_avg:127.36ms
step:1024/1395 train_time:129144ms step_avg:127.36ms
step:1025/1395 train_time:129276ms step_avg:127.37ms
step:1026/1395 train_time:129409ms step_avg:127.37ms
step:1027/1395 train_time:129543ms step_avg:127.38ms
step:1028/1395 train_time:129677ms step_avg:127.38ms
step:1029/1395 train_time:129810ms step_avg:127.39ms
step:1030/1395 train_time:129943ms step_avg:127.40ms
step:1031/1395 train_time:130075ms step_avg:127.40ms
step:1032/1395 train_time:130207ms step_avg:127.40ms
step:1033/1395 train_time:130340ms step_avg:127.41ms
step:1034/1395 train_time:130472ms step_avg:127.41ms
step:1035/1395 train_time:130606ms step_avg:127.42ms
step:1036/1395 train_time:130739ms step_avg:127.43ms
step:1037/1395 train_time:130873ms step_avg:127.43ms
step:1038/1395 train_time:131005ms step_avg:127.44ms
step:1039/1395 train_time:131136ms step_avg:127.44ms
step:1040/1395 train_time:131269ms step_avg:127.45ms
step:1041/1395 train_time:131402ms step_avg:127.45ms
step:1042/1395 train_time:131534ms step_avg:127.46ms
step:1043/1395 train_time:131668ms step_avg:127.46ms
step:1044/1395 train_time:131802ms step_avg:127.47ms
step:1045/1395 train_time:131935ms step_avg:127.47ms
step:1046/1395 train_time:132068ms step_avg:127.48ms
step:1047/1395 train_time:132200ms step_avg:127.48ms
step:1048/1395 train_time:132333ms step_avg:127.49ms
step:1049/1395 train_time:132467ms step_avg:127.49ms
step:1050/1395 train_time:132600ms step_avg:127.50ms
step:1051/1395 train_time:132735ms step_avg:127.51ms
step:1052/1395 train_time:132867ms step_avg:127.51ms
step:1053/1395 train_time:133000ms step_avg:127.52ms
step:1054/1395 train_time:133133ms step_avg:127.52ms
step:1055/1395 train_time:133266ms step_avg:127.53ms
step:1056/1395 train_time:133399ms step_avg:127.53ms
step:1057/1395 train_time:133531ms step_avg:127.54ms
step:1058/1395 train_time:133665ms step_avg:127.54ms
step:1059/1395 train_time:133798ms step_avg:127.55ms
step:1060/1395 train_time:133932ms step_avg:127.55ms
step:1061/1395 train_time:134065ms step_avg:127.56ms
step:1062/1395 train_time:134198ms step_avg:127.56ms
step:1063/1395 train_time:134331ms step_avg:127.57ms
step:1064/1395 train_time:134463ms step_avg:127.57ms
step:1065/1395 train_time:134595ms step_avg:127.58ms
step:1066/1395 train_time:134729ms step_avg:127.58ms
step:1067/1395 train_time:134861ms step_avg:127.59ms
step:1068/1395 train_time:134995ms step_avg:127.59ms
step:1069/1395 train_time:135129ms step_avg:127.60ms
step:1070/1395 train_time:135261ms step_avg:127.60ms
step:1071/1395 train_time:135396ms step_avg:127.61ms
step:1072/1395 train_time:135527ms step_avg:127.62ms
step:1073/1395 train_time:135659ms step_avg:127.62ms
step:1074/1395 train_time:135792ms step_avg:127.62ms
step:1075/1395 train_time:135924ms step_avg:127.63ms
step:1076/1395 train_time:136056ms step_avg:127.63ms
step:1077/1395 train_time:136188ms step_avg:127.64ms
step:1078/1395 train_time:136321ms step_avg:127.64ms
step:1079/1395 train_time:136458ms step_avg:127.65ms
step:1080/1395 train_time:136591ms step_avg:127.66ms
step:1081/1395 train_time:136724ms step_avg:127.66ms
step:1082/1395 train_time:136855ms step_avg:127.66ms
step:1083/1395 train_time:136987ms step_avg:127.67ms
step:1084/1395 train_time:137122ms step_avg:127.67ms
step:1085/1395 train_time:137254ms step_avg:127.68ms
step:1086/1395 train_time:137388ms step_avg:127.68ms
step:1087/1395 train_time:137522ms step_avg:127.69ms
step:1088/1395 train_time:137655ms step_avg:127.69ms
step:1089/1395 train_time:137789ms step_avg:127.70ms
step:1090/1395 train_time:137923ms step_avg:127.71ms
step:1091/1395 train_time:138056ms step_avg:127.71ms
step:1092/1395 train_time:138188ms step_avg:127.72ms
step:1093/1395 train_time:138321ms step_avg:127.72ms
step:1094/1395 train_time:138453ms step_avg:127.72ms
step:1095/1395 train_time:138586ms step_avg:127.73ms
step:1096/1395 train_time:138720ms step_avg:127.73ms
step:1097/1395 train_time:138853ms step_avg:127.74ms
step:1098/1395 train_time:138985ms step_avg:127.74ms
step:1099/1395 train_time:139117ms step_avg:127.75ms
step:1100/1395 train_time:139249ms step_avg:127.75ms
step:1101/1395 train_time:139382ms step_avg:127.76ms
step:1102/1395 train_time:139514ms step_avg:127.76ms
step:1103/1395 train_time:139648ms step_avg:127.77ms
step:1104/1395 train_time:139784ms step_avg:127.77ms
step:1105/1395 train_time:139917ms step_avg:127.78ms
step:1106/1395 train_time:140050ms step_avg:127.78ms
step:1107/1395 train_time:140182ms step_avg:127.79ms
step:1108/1395 train_time:140316ms step_avg:127.79ms
step:1109/1395 train_time:140448ms step_avg:127.80ms
step:1110/1395 train_time:140581ms step_avg:127.80ms
step:1111/1395 train_time:140714ms step_avg:127.81ms
step:1112/1395 train_time:140847ms step_avg:127.81ms
step:1113/1395 train_time:140980ms step_avg:127.81ms
step:1114/1395 train_time:141112ms step_avg:127.82ms
step:1115/1395 train_time:141245ms step_avg:127.82ms
step:1116/1395 train_time:141378ms step_avg:127.83ms
step:1117/1395 train_time:141511ms step_avg:127.83ms
step:1118/1395 train_time:141646ms step_avg:127.84ms
step:1119/1395 train_time:141778ms step_avg:127.84ms
step:1120/1395 train_time:141911ms step_avg:127.85ms
step:1121/1395 train_time:142043ms step_avg:127.85ms
step:1122/1395 train_time:142176ms step_avg:127.86ms
step:1123/1395 train_time:142308ms step_avg:127.86ms
step:1124/1395 train_time:142442ms step_avg:127.87ms
step:1125/1395 train_time:142573ms step_avg:127.87ms
step:1125/1395 val_loss:3.3655 train_time:142707ms step_avg:127.99ms
step:1126/1395 train_time:142729ms step_avg:127.89ms
step:1127/1395 train_time:142850ms step_avg:127.89ms
step:1128/1395 train_time:142985ms step_avg:127.89ms
step:1129/1395 train_time:143118ms step_avg:127.90ms
step:1130/1395 train_time:143250ms step_avg:127.90ms
step:1131/1395 train_time:143383ms step_avg:127.91ms
step:1132/1395 train_time:143515ms step_avg:127.91ms
step:1133/1395 train_time:143646ms step_avg:127.91ms
step:1134/1395 train_time:143781ms step_avg:127.92ms
step:1135/1395 train_time:143914ms step_avg:127.92ms
step:1136/1395 train_time:144049ms step_avg:127.93ms
step:1137/1395 train_time:144181ms step_avg:127.93ms
step:1138/1395 train_time:144314ms step_avg:127.94ms
step:1139/1395 train_time:144448ms step_avg:127.94ms
step:1140/1395 train_time:144584ms step_avg:127.95ms
step:1141/1395 train_time:144718ms step_avg:127.96ms
step:1142/1395 train_time:144853ms step_avg:127.96ms
step:1143/1395 train_time:144989ms step_avg:127.97ms
step:1144/1395 train_time:145124ms step_avg:127.98ms
step:1145/1395 train_time:145256ms step_avg:127.98ms
step:1146/1395 train_time:145391ms step_avg:127.99ms
step:1147/1395 train_time:145526ms step_avg:127.99ms
step:1148/1395 train_time:145660ms step_avg:128.00ms
step:1149/1395 train_time:145792ms step_avg:128.00ms
step:1150/1395 train_time:145926ms step_avg:128.01ms
step:1151/1395 train_time:146063ms step_avg:128.01ms
step:1152/1395 train_time:146197ms step_avg:128.02ms
step:1153/1395 train_time:146333ms step_avg:128.03ms
step:1154/1395 train_time:146468ms step_avg:128.03ms
step:1155/1395 train_time:146601ms step_avg:128.04ms
step:1156/1395 train_time:146738ms step_avg:128.04ms
step:1157/1395 train_time:146874ms step_avg:128.05ms
step:1158/1395 train_time:147008ms step_avg:128.06ms
step:1159/1395 train_time:147141ms step_avg:128.06ms
step:1160/1395 train_time:147275ms step_avg:128.07ms
step:1161/1395 train_time:147410ms step_avg:128.07ms
step:1162/1395 train_time:147546ms step_avg:128.08ms
step:1163/1395 train_time:147679ms step_avg:128.08ms
step:1164/1395 train_time:147814ms step_avg:128.09ms
step:1165/1395 train_time:147948ms step_avg:128.09ms
step:1166/1395 train_time:148081ms step_avg:128.10ms
step:1167/1395 train_time:148215ms step_avg:128.10ms
step:1168/1395 train_time:148350ms step_avg:128.11ms
step:1169/1395 train_time:148485ms step_avg:128.11ms
step:1170/1395 train_time:148619ms step_avg:128.12ms
step:1171/1395 train_time:148753ms step_avg:128.13ms
step:1172/1395 train_time:148889ms step_avg:128.13ms
step:1173/1395 train_time:149021ms step_avg:128.14ms
step:1174/1395 train_time:149160ms step_avg:128.14ms
step:1175/1395 train_time:149294ms step_avg:128.15ms
step:1176/1395 train_time:149429ms step_avg:128.16ms
step:1177/1395 train_time:149566ms step_avg:128.16ms
step:1178/1395 train_time:149699ms step_avg:128.17ms
step:1179/1395 train_time:149833ms step_avg:128.17ms
step:1180/1395 train_time:149969ms step_avg:128.18ms
step:1181/1395 train_time:150105ms step_avg:128.19ms
step:1182/1395 train_time:150238ms step_avg:128.19ms
step:1183/1395 train_time:150374ms step_avg:128.20ms
step:1184/1395 train_time:150509ms step_avg:128.20ms
step:1185/1395 train_time:150645ms step_avg:128.21ms
step:1186/1395 train_time:150778ms step_avg:128.21ms
step:1187/1395 train_time:150918ms step_avg:128.22ms
step:1188/1395 train_time:151052ms step_avg:128.23ms
step:1189/1395 train_time:151186ms step_avg:128.23ms
step:1190/1395 train_time:151319ms step_avg:128.24ms
step:1191/1395 train_time:151453ms step_avg:128.24ms
step:1192/1395 train_time:151587ms step_avg:128.25ms
step:1193/1395 train_time:151720ms step_avg:128.25ms
step:1194/1395 train_time:151854ms step_avg:128.26ms
step:1195/1395 train_time:151988ms step_avg:128.26ms
step:1196/1395 train_time:152123ms step_avg:128.27ms
step:1197/1395 train_time:152258ms step_avg:128.27ms
step:1198/1395 train_time:152395ms step_avg:128.28ms
step:1199/1395 train_time:152529ms step_avg:128.28ms
step:1200/1395 train_time:152664ms step_avg:128.29ms
step:1201/1395 train_time:152797ms step_avg:128.29ms
step:1202/1395 train_time:152936ms step_avg:128.30ms
step:1203/1395 train_time:153073ms step_avg:128.31ms
step:1204/1395 train_time:153206ms step_avg:128.31ms
step:1205/1395 train_time:153341ms step_avg:128.32ms
step:1206/1395 train_time:153476ms step_avg:128.32ms
step:1207/1395 train_time:153610ms step_avg:128.33ms
step:1208/1395 train_time:153744ms step_avg:128.33ms
step:1209/1395 train_time:153878ms step_avg:128.34ms
step:1210/1395 train_time:154014ms step_avg:128.35ms
step:1211/1395 train_time:154149ms step_avg:128.35ms
step:1212/1395 train_time:154282ms step_avg:128.35ms
step:1213/1395 train_time:154416ms step_avg:128.36ms
step:1214/1395 train_time:154550ms step_avg:128.36ms
step:1215/1395 train_time:154686ms step_avg:128.37ms
step:1216/1395 train_time:154818ms step_avg:128.37ms
step:1217/1395 train_time:154954ms step_avg:128.38ms
step:1218/1395 train_time:155087ms step_avg:128.38ms
step:1219/1395 train_time:155220ms step_avg:128.39ms
step:1220/1395 train_time:155354ms step_avg:128.39ms
step:1221/1395 train_time:155487ms step_avg:128.40ms
step:1222/1395 train_time:155621ms step_avg:128.40ms
step:1223/1395 train_time:155755ms step_avg:128.40ms
step:1224/1395 train_time:155890ms step_avg:128.41ms
step:1225/1395 train_time:156026ms step_avg:128.42ms
step:1226/1395 train_time:156159ms step_avg:128.42ms
step:1227/1395 train_time:156293ms step_avg:128.42ms
step:1228/1395 train_time:156427ms step_avg:128.43ms
step:1229/1395 train_time:156560ms step_avg:128.43ms
step:1230/1395 train_time:156695ms step_avg:128.44ms
step:1231/1395 train_time:156831ms step_avg:128.44ms
step:1232/1395 train_time:156966ms step_avg:128.45ms
step:1233/1395 train_time:157100ms step_avg:128.45ms
step:1234/1395 train_time:157234ms step_avg:128.46ms
step:1235/1395 train_time:157368ms step_avg:128.46ms
step:1236/1395 train_time:157502ms step_avg:128.47ms
step:1237/1395 train_time:157635ms step_avg:128.47ms
step:1238/1395 train_time:157773ms step_avg:128.48ms
step:1239/1395 train_time:157907ms step_avg:128.48ms
step:1240/1395 train_time:158042ms step_avg:128.49ms
step:1241/1395 train_time:158178ms step_avg:128.50ms
step:1242/1395 train_time:158312ms step_avg:128.50ms
step:1243/1395 train_time:158448ms step_avg:128.51ms
step:1244/1395 train_time:158581ms step_avg:128.51ms
step:1245/1395 train_time:158715ms step_avg:128.51ms
step:1246/1395 train_time:158849ms step_avg:128.52ms
step:1247/1395 train_time:158984ms step_avg:128.52ms
step:1248/1395 train_time:159118ms step_avg:128.53ms
step:1249/1395 train_time:159252ms step_avg:128.53ms
step:1250/1395 train_time:159387ms step_avg:128.54ms
step:1250/1395 val_loss:3.3179 train_time:159521ms step_avg:128.65ms
step:1251/1395 train_time:159541ms step_avg:128.56ms
step:1252/1395 train_time:159668ms step_avg:128.56ms
step:1253/1395 train_time:159802ms step_avg:128.56ms
step:1254/1395 train_time:159935ms step_avg:128.57ms
step:1255/1395 train_time:160072ms step_avg:128.57ms
step:1256/1395 train_time:160207ms step_avg:128.58ms
step:1257/1395 train_time:160341ms step_avg:128.58ms
step:1258/1395 train_time:160475ms step_avg:128.59ms
step:1259/1395 train_time:160610ms step_avg:128.59ms
step:1260/1395 train_time:160744ms step_avg:128.60ms
step:1261/1395 train_time:160878ms step_avg:128.60ms
step:1262/1395 train_time:161014ms step_avg:128.61ms
step:1263/1395 train_time:161148ms step_avg:128.61ms
step:1264/1395 train_time:161282ms step_avg:128.61ms
step:1265/1395 train_time:161415ms step_avg:128.62ms
step:1266/1395 train_time:161550ms step_avg:128.62ms
step:1267/1395 train_time:161684ms step_avg:128.63ms
step:1268/1395 train_time:161818ms step_avg:128.63ms
step:1269/1395 train_time:161953ms step_avg:128.64ms
step:1270/1395 train_time:162088ms step_avg:128.64ms
step:1271/1395 train_time:162223ms step_avg:128.65ms
step:1272/1395 train_time:162357ms step_avg:128.65ms
step:1273/1395 train_time:162490ms step_avg:128.65ms
step:1274/1395 train_time:162624ms step_avg:128.66ms
step:1275/1395 train_time:162760ms step_avg:128.66ms
step:1276/1395 train_time:162894ms step_avg:128.67ms
step:1277/1395 train_time:163028ms step_avg:128.67ms
step:1278/1395 train_time:163163ms step_avg:128.68ms
step:1279/1395 train_time:163296ms step_avg:128.68ms
step:1280/1395 train_time:163432ms step_avg:128.69ms
step:1281/1395 train_time:163566ms step_avg:128.69ms
step:1282/1395 train_time:163700ms step_avg:128.69ms
step:1283/1395 train_time:163834ms step_avg:128.70ms
step:1284/1395 train_time:163969ms step_avg:128.70ms
step:1285/1395 train_time:164103ms step_avg:128.71ms
step:1286/1395 train_time:164238ms step_avg:128.71ms
step:1287/1395 train_time:164373ms step_avg:128.72ms
step:1288/1395 train_time:164507ms step_avg:128.72ms
step:1289/1395 train_time:164643ms step_avg:128.73ms
step:1290/1395 train_time:164779ms step_avg:128.73ms
step:1291/1395 train_time:164915ms step_avg:128.74ms
step:1292/1395 train_time:165049ms step_avg:128.74ms
step:1293/1395 train_time:165185ms step_avg:128.75ms
step:1294/1395 train_time:165319ms step_avg:128.75ms
step:1295/1395 train_time:165454ms step_avg:128.76ms
step:1296/1395 train_time:165587ms step_avg:128.76ms
step:1297/1395 train_time:165723ms step_avg:128.77ms
step:1298/1395 train_time:165856ms step_avg:128.77ms
step:1299/1395 train_time:165990ms step_avg:128.77ms
step:1300/1395 train_time:166125ms step_avg:128.78ms
step:1301/1395 train_time:166258ms step_avg:128.78ms
step:1302/1395 train_time:166393ms step_avg:128.79ms
step:1303/1395 train_time:166528ms step_avg:128.79ms
step:1304/1395 train_time:166665ms step_avg:128.80ms
step:1305/1395 train_time:166800ms step_avg:128.80ms
step:1306/1395 train_time:166934ms step_avg:128.81ms
step:1307/1395 train_time:167069ms step_avg:128.81ms
step:1308/1395 train_time:167204ms step_avg:128.82ms
step:1309/1395 train_time:167339ms step_avg:128.82ms
step:1310/1395 train_time:167472ms step_avg:128.82ms
step:1311/1395 train_time:167606ms step_avg:128.83ms
step:1312/1395 train_time:167739ms step_avg:128.83ms
step:1313/1395 train_time:167874ms step_avg:128.84ms
step:1314/1395 train_time:168007ms step_avg:128.84ms
step:1315/1395 train_time:168142ms step_avg:128.84ms
step:1316/1395 train_time:168277ms step_avg:128.85ms
step:1317/1395 train_time:168410ms step_avg:128.85ms
step:1318/1395 train_time:168546ms step_avg:128.86ms
step:1319/1395 train_time:168682ms step_avg:128.86ms
step:1320/1395 train_time:168816ms step_avg:128.87ms
step:1321/1395 train_time:168950ms step_avg:128.87ms
step:1322/1395 train_time:169087ms step_avg:128.88ms
step:1323/1395 train_time:169221ms step_avg:128.88ms
step:1324/1395 train_time:169356ms step_avg:128.89ms
step:1325/1395 train_time:169489ms step_avg:128.89ms
step:1326/1395 train_time:169624ms step_avg:128.89ms
step:1327/1395 train_time:169759ms step_avg:128.90ms
step:1328/1395 train_time:169892ms step_avg:128.90ms
step:1329/1395 train_time:170031ms step_avg:128.91ms
step:1330/1395 train_time:170166ms step_avg:128.91ms
step:1331/1395 train_time:170304ms step_avg:128.92ms
step:1332/1395 train_time:170441ms step_avg:128.93ms
step:1333/1395 train_time:170576ms step_avg:128.93ms
step:1334/1395 train_time:170710ms step_avg:128.93ms
step:1335/1395 train_time:170843ms step_avg:128.94ms
step:1336/1395 train_time:170980ms step_avg:128.94ms
step:1337/1395 train_time:171115ms step_avg:128.95ms
step:1338/1395 train_time:171249ms step_avg:128.95ms
step:1339/1395 train_time:171385ms step_avg:128.96ms
step:1340/1395 train_time:171522ms step_avg:128.96ms
step:1341/1395 train_time:171655ms step_avg:128.97ms
step:1342/1395 train_time:171789ms step_avg:128.97ms
step:1343/1395 train_time:171924ms step_avg:128.98ms
step:1344/1395 train_time:172058ms step_avg:128.98ms
step:1345/1395 train_time:172192ms step_avg:128.98ms
step:1346/1395 train_time:172328ms step_avg:128.99ms
step:1347/1395 train_time:172464ms step_avg:128.99ms
step:1348/1395 train_time:172598ms step_avg:129.00ms
step:1349/1395 train_time:172735ms step_avg:129.00ms
step:1350/1395 train_time:172869ms step_avg:129.01ms
step:1351/1395 train_time:173005ms step_avg:129.01ms
step:1352/1395 train_time:173143ms step_avg:129.02ms
step:1353/1395 train_time:173280ms step_avg:129.02ms
step:1354/1395 train_time:173415ms step_avg:129.03ms
step:1355/1395 train_time:173549ms step_avg:129.03ms
step:1356/1395 train_time:173683ms step_avg:129.04ms
step:1357/1395 train_time:173820ms step_avg:129.04ms
step:1358/1395 train_time:173956ms step_avg:129.05ms
step:1359/1395 train_time:174092ms step_avg:129.05ms
step:1360/1395 train_time:174231ms step_avg:129.06ms
step:1361/1395 train_time:174366ms step_avg:129.06ms
step:1362/1395 train_time:174503ms step_avg:129.07ms
step:1363/1395 train_time:174640ms step_avg:129.08ms
step:1364/1395 train_time:174775ms step_avg:129.08ms
step:1365/1395 train_time:174909ms step_avg:129.08ms
step:1366/1395 train_time:175044ms step_avg:129.09ms
step:1367/1395 train_time:175181ms step_avg:129.09ms
step:1368/1395 train_time:175316ms step_avg:129.10ms
step:1369/1395 train_time:175453ms step_avg:129.10ms
step:1370/1395 train_time:175593ms step_avg:129.11ms
step:1371/1395 train_time:175729ms step_avg:129.12ms
step:1372/1395 train_time:175867ms step_avg:129.12ms
step:1373/1395 train_time:176002ms step_avg:129.13ms
step:1374/1395 train_time:176139ms step_avg:129.13ms
step:1375/1395 train_time:176273ms step_avg:129.14ms
step:1375/1395 val_loss:3.2836 train_time:176406ms step_avg:129.24ms
step:1376/1395 train_time:176427ms step_avg:129.16ms
step:1377/1395 train_time:176549ms step_avg:129.15ms
step:1378/1395 train_time:176685ms step_avg:129.16ms
step:1379/1395 train_time:176820ms step_avg:129.16ms
step:1380/1395 train_time:176956ms step_avg:129.17ms
step:1381/1395 train_time:177092ms step_avg:129.17ms
step:1382/1395 train_time:177228ms step_avg:129.17ms
step:1383/1395 train_time:177363ms step_avg:129.18ms
step:1384/1395 train_time:177500ms step_avg:129.18ms
step:1385/1395 train_time:177636ms step_avg:129.19ms
step:1386/1395 train_time:177771ms step_avg:129.19ms
step:1387/1395 train_time:177908ms step_avg:129.20ms
step:1388/1395 train_time:178044ms step_avg:129.20ms
step:1389/1395 train_time:178180ms step_avg:129.21ms
step:1390/1395 train_time:178315ms step_avg:129.21ms
step:1391/1395 train_time:178450ms step_avg:129.22ms
step:1392/1395 train_time:178587ms step_avg:129.22ms
step:1393/1395 train_time:178722ms step_avg:129.23ms
step:1394/1395 train_time:178858ms step_avg:129.23ms
step:1395/1395 train_time:178992ms step_avg:129.24ms
step:1395/1395 val_loss:3.2792 train_time:179128ms step_avg:129.33ms
peak memory allocated: 37653 MiB reserved: 39236 MiB
