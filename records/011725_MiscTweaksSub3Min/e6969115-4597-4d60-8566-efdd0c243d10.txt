import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True

# -----------------------------------------------------------------------------
# Custom operators

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

def lm_head_fp8(x: Tensor, w: Tensor) -> Tensor:
    _x = x.flatten(0, -2)
    out: Tensor = torch.ops.nanogpt.mm(_x, w, x_s=2.0, w_s=32.0, grad_s=2.0**29)[0]
    return out.reshape(*x.shape[:-1], -1)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev():
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int):
        super().__init__(in_features, out_features, bias=False)

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x):
        return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len=65536):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        # self.c_q = CastedLinear(dim, dim)
        # self.c_k = CastedLinear(dim, dim)
        # self.c_v = CastedLinear(dim, dim)
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std
        self.qkv_w = nn.Parameter(torch.empty(3, dim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads) # dim // num_heads = head_dim
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # Set attention scale such that the minimum attainable attention entropy
        # (but not necessary the attention entropy itself) is close to 0. By @leloykun
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        # q = self.c_q(x).view(B, T, self.num_heads, -1)
        # k = self.c_k(x).view(B, T, self.num_heads, -1)
        # v = self.c_v(x).view(B, T, self.num_heads, -1)
        # qkv_weight = torch.cat([self.c_q.weight, self.c_k.weight, self.c_v.weight], dim=0).type_as(x)
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3*self.num_heads, -1).chunk(3, dim=-2)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, model_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(model_dim, num_heads, layer_idx) if layer_idx != 7 else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, ve, x0, block_mask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, num_embeddings: int, embedding_dim: int):
        super().__init__()
        self.embed = nn.ModuleList([nn.Embedding(num_embeddings, embedding_dim) for _ in range(3)])

    def forward(self, input_seq) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2], None, None, None, None, None, None, ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual learning
        self.value_embeds = ValueEmbedding(vocab_size, model_dim)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128))
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        assert input_seq.ndim == 1
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        docs = (input_seq == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        def create_doc_swc_block_masks(sliding_window_num_blocks: Tensor):
            kv_idx = block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & document_bm
            full_bm  = causal_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            def build_bm(sw_num_blocks: Tensor) -> BlockMask:
                return BlockMask.from_kv_blocks(
                    torch.clamp_max(kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                    kv_indices,
                    torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                    full_kv_indices,
                    BLOCK_SIZE=BLOCK_SIZE,
                    mask_mod=document_causal,
                )
            return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

        # Long-short SWA block masks by @leloykun & @YouJiacheng
        long_bm, short_bm = create_doc_swc_block_masks(sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = lm_head_fp8(x, self.lm_head.weight) if self.training else self.lm_head(x)
        # @Grad62304977 added tanh softcapping, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use cycle(files) if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    batch_size = 8*64*1024 # batch size in tokens
    num_iterations = 1395 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 64*1024 # FlexAttention sequence length
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
             if console:
                 print(s)
             print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_loader = distributed_data_generator(args.train_files, args.batch_size, rank, world_size)

model = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim >= 2]
embed_params = [model.embed.weight, *model.value_embeds.parameters()]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), fused=True, eps=1e-10)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(it: int):
    t = 1 - it / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_bs = world_size * args.seq_len
        assert args.val_tokens % val_bs == 0
        val_steps = args.val_tokens // val_bs
        val_loader = distributed_data_generator(args.val_files, val_bs, rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION BEGIN -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB"
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250110+cu126 compiled for CUDA 12.6
Thu Jan 16 21:21:27 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   38C    P0             121W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   29C    P0             115W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0             118W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   38C    P0             120W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   37C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1395 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1395 train_time:23133ms step_avg:nanms
step:2/1395 train_time:24039ms step_avg:nanms
step:3/1395 train_time:24160ms step_avg:nanms
step:4/1395 train_time:24280ms step_avg:nanms
step:5/1395 train_time:24401ms step_avg:nanms
step:6/1395 train_time:24522ms step_avg:nanms
step:7/1395 train_time:24643ms step_avg:nanms
step:8/1395 train_time:24764ms step_avg:nanms
step:9/1395 train_time:24886ms step_avg:nanms
step:10/1395 train_time:25008ms step_avg:nanms
step:11/1395 train_time:124ms step_avg:nanms
step:12/1395 train_time:247ms step_avg:nanms
step:13/1395 train_time:370ms step_avg:123.38ms
step:14/1395 train_time:493ms step_avg:123.21ms
step:15/1395 train_time:614ms step_avg:122.82ms
step:16/1395 train_time:736ms step_avg:122.72ms
step:17/1395 train_time:858ms step_avg:122.63ms
step:18/1395 train_time:980ms step_avg:122.47ms
step:19/1395 train_time:1103ms step_avg:122.51ms
step:20/1395 train_time:1224ms step_avg:122.44ms
step:21/1395 train_time:1346ms step_avg:122.41ms
step:22/1395 train_time:1468ms step_avg:122.35ms
step:23/1395 train_time:1590ms step_avg:122.28ms
step:24/1395 train_time:1711ms step_avg:122.19ms
step:25/1395 train_time:1833ms step_avg:122.19ms
step:26/1395 train_time:1954ms step_avg:122.13ms
step:27/1395 train_time:2076ms step_avg:122.13ms
step:28/1395 train_time:2199ms step_avg:122.15ms
step:29/1395 train_time:2321ms step_avg:122.16ms
step:30/1395 train_time:2444ms step_avg:122.18ms
step:31/1395 train_time:2566ms step_avg:122.18ms
step:32/1395 train_time:2687ms step_avg:122.15ms
step:33/1395 train_time:2810ms step_avg:122.18ms
step:34/1395 train_time:2931ms step_avg:122.12ms
step:35/1395 train_time:3055ms step_avg:122.19ms
step:36/1395 train_time:3178ms step_avg:122.23ms
step:37/1395 train_time:3301ms step_avg:122.25ms
step:38/1395 train_time:3423ms step_avg:122.24ms
step:39/1395 train_time:3546ms step_avg:122.28ms
step:40/1395 train_time:3669ms step_avg:122.30ms
step:41/1395 train_time:3792ms step_avg:122.32ms
step:42/1395 train_time:3915ms step_avg:122.36ms
step:43/1395 train_time:4038ms step_avg:122.35ms
step:44/1395 train_time:4160ms step_avg:122.34ms
step:45/1395 train_time:4282ms step_avg:122.34ms
step:46/1395 train_time:4405ms step_avg:122.37ms
step:47/1395 train_time:4527ms step_avg:122.36ms
step:48/1395 train_time:4648ms step_avg:122.31ms
step:49/1395 train_time:4769ms step_avg:122.29ms
step:50/1395 train_time:4893ms step_avg:122.31ms
step:51/1395 train_time:5014ms step_avg:122.30ms
step:52/1395 train_time:5136ms step_avg:122.28ms
step:53/1395 train_time:5259ms step_avg:122.29ms
step:54/1395 train_time:5381ms step_avg:122.29ms
step:55/1395 train_time:5503ms step_avg:122.28ms
step:56/1395 train_time:5625ms step_avg:122.29ms
step:57/1395 train_time:5747ms step_avg:122.29ms
step:58/1395 train_time:5870ms step_avg:122.30ms
step:59/1395 train_time:5995ms step_avg:122.34ms
step:60/1395 train_time:6116ms step_avg:122.31ms
step:61/1395 train_time:6238ms step_avg:122.31ms
step:62/1395 train_time:6360ms step_avg:122.31ms
step:63/1395 train_time:6481ms step_avg:122.29ms
step:64/1395 train_time:6603ms step_avg:122.27ms
step:65/1395 train_time:6725ms step_avg:122.27ms
step:66/1395 train_time:6847ms step_avg:122.27ms
step:67/1395 train_time:6968ms step_avg:122.25ms
step:68/1395 train_time:7090ms step_avg:122.24ms
step:69/1395 train_time:7212ms step_avg:122.24ms
step:70/1395 train_time:7335ms step_avg:122.25ms
step:71/1395 train_time:7457ms step_avg:122.25ms
step:72/1395 train_time:7579ms step_avg:122.24ms
step:73/1395 train_time:7701ms step_avg:122.23ms
step:74/1395 train_time:7822ms step_avg:122.22ms
step:75/1395 train_time:7944ms step_avg:122.22ms
step:76/1395 train_time:8067ms step_avg:122.22ms
step:77/1395 train_time:8188ms step_avg:122.21ms
step:78/1395 train_time:8310ms step_avg:122.20ms
step:79/1395 train_time:8433ms step_avg:122.22ms
step:80/1395 train_time:8555ms step_avg:122.22ms
step:81/1395 train_time:8678ms step_avg:122.22ms
step:82/1395 train_time:8800ms step_avg:122.22ms
step:83/1395 train_time:8921ms step_avg:122.21ms
step:84/1395 train_time:9044ms step_avg:122.22ms
step:85/1395 train_time:9166ms step_avg:122.21ms
step:86/1395 train_time:9288ms step_avg:122.21ms
step:87/1395 train_time:9410ms step_avg:122.21ms
step:88/1395 train_time:9532ms step_avg:122.20ms
step:89/1395 train_time:9653ms step_avg:122.20ms
step:90/1395 train_time:9776ms step_avg:122.20ms
step:91/1395 train_time:9898ms step_avg:122.20ms
step:92/1395 train_time:10022ms step_avg:122.22ms
step:93/1395 train_time:10144ms step_avg:122.22ms
step:94/1395 train_time:10266ms step_avg:122.22ms
step:95/1395 train_time:10388ms step_avg:122.21ms
step:96/1395 train_time:10510ms step_avg:122.21ms
step:97/1395 train_time:10631ms step_avg:122.20ms
step:98/1395 train_time:10754ms step_avg:122.20ms
step:99/1395 train_time:10877ms step_avg:122.21ms
step:100/1395 train_time:10999ms step_avg:122.21ms
step:101/1395 train_time:11121ms step_avg:122.21ms
step:102/1395 train_time:11243ms step_avg:122.21ms
step:103/1395 train_time:11365ms step_avg:122.21ms
step:104/1395 train_time:11488ms step_avg:122.21ms
step:105/1395 train_time:11610ms step_avg:122.21ms
step:106/1395 train_time:11732ms step_avg:122.21ms
step:107/1395 train_time:11855ms step_avg:122.22ms
step:108/1395 train_time:11978ms step_avg:122.23ms
step:109/1395 train_time:12100ms step_avg:122.23ms
step:110/1395 train_time:12224ms step_avg:122.24ms
step:111/1395 train_time:12347ms step_avg:122.24ms
step:112/1395 train_time:12469ms step_avg:122.25ms
step:113/1395 train_time:12592ms step_avg:122.25ms
step:114/1395 train_time:12714ms step_avg:122.25ms
step:115/1395 train_time:12836ms step_avg:122.25ms
step:116/1395 train_time:12958ms step_avg:122.25ms
step:117/1395 train_time:13082ms step_avg:122.26ms
step:118/1395 train_time:13204ms step_avg:122.26ms
step:119/1395 train_time:13327ms step_avg:122.27ms
step:120/1395 train_time:13451ms step_avg:122.28ms
step:121/1395 train_time:13574ms step_avg:122.29ms
step:122/1395 train_time:13697ms step_avg:122.29ms
step:123/1395 train_time:13819ms step_avg:122.29ms
step:124/1395 train_time:13942ms step_avg:122.29ms
step:125/1395 train_time:14065ms step_avg:122.30ms
step:125/1395 val_loss:4.3861 train_time:14186ms step_avg:123.36ms
step:126/1395 train_time:14208ms step_avg:122.48ms
step:127/1395 train_time:14329ms step_avg:122.47ms
step:128/1395 train_time:14457ms step_avg:122.52ms
step:129/1395 train_time:14580ms step_avg:122.52ms
step:130/1395 train_time:14702ms step_avg:122.52ms
step:131/1395 train_time:14825ms step_avg:122.52ms
step:132/1395 train_time:14947ms step_avg:122.52ms
step:133/1395 train_time:15070ms step_avg:122.52ms
step:134/1395 train_time:15192ms step_avg:122.51ms
step:135/1395 train_time:15317ms step_avg:122.54ms
step:136/1395 train_time:15441ms step_avg:122.54ms
step:137/1395 train_time:15564ms step_avg:122.55ms
step:138/1395 train_time:15687ms step_avg:122.55ms
step:139/1395 train_time:15810ms step_avg:122.56ms
step:140/1395 train_time:15932ms step_avg:122.55ms
step:141/1395 train_time:16055ms step_avg:122.56ms
step:142/1395 train_time:16177ms step_avg:122.55ms
step:143/1395 train_time:16301ms step_avg:122.56ms
step:144/1395 train_time:16424ms step_avg:122.57ms
step:145/1395 train_time:16547ms step_avg:122.57ms
step:146/1395 train_time:16670ms step_avg:122.58ms
step:147/1395 train_time:16793ms step_avg:122.58ms
step:148/1395 train_time:16915ms step_avg:122.58ms
step:149/1395 train_time:17038ms step_avg:122.58ms
step:150/1395 train_time:17161ms step_avg:122.58ms
step:151/1395 train_time:17282ms step_avg:122.57ms
step:152/1395 train_time:17406ms step_avg:122.57ms
step:153/1395 train_time:17528ms step_avg:122.57ms
step:154/1395 train_time:17651ms step_avg:122.58ms
step:155/1395 train_time:17774ms step_avg:122.58ms
step:156/1395 train_time:17896ms step_avg:122.58ms
step:157/1395 train_time:18019ms step_avg:122.58ms
step:158/1395 train_time:18142ms step_avg:122.58ms
step:159/1395 train_time:18264ms step_avg:122.58ms
step:160/1395 train_time:18388ms step_avg:122.58ms
step:161/1395 train_time:18510ms step_avg:122.58ms
step:162/1395 train_time:18634ms step_avg:122.59ms
step:163/1395 train_time:18756ms step_avg:122.59ms
step:164/1395 train_time:18879ms step_avg:122.59ms
step:165/1395 train_time:19002ms step_avg:122.59ms
step:166/1395 train_time:19125ms step_avg:122.60ms
step:167/1395 train_time:19248ms step_avg:122.60ms
step:168/1395 train_time:19371ms step_avg:122.60ms
step:169/1395 train_time:19494ms step_avg:122.61ms
step:170/1395 train_time:19617ms step_avg:122.61ms
step:171/1395 train_time:19741ms step_avg:122.61ms
step:172/1395 train_time:19864ms step_avg:122.61ms
step:173/1395 train_time:19986ms step_avg:122.61ms
step:174/1395 train_time:20108ms step_avg:122.61ms
step:175/1395 train_time:20231ms step_avg:122.61ms
step:176/1395 train_time:20353ms step_avg:122.61ms
step:177/1395 train_time:20475ms step_avg:122.61ms
step:178/1395 train_time:20598ms step_avg:122.61ms
step:179/1395 train_time:20721ms step_avg:122.61ms
step:180/1395 train_time:20844ms step_avg:122.61ms
step:181/1395 train_time:20967ms step_avg:122.61ms
step:182/1395 train_time:21090ms step_avg:122.62ms
step:183/1395 train_time:21212ms step_avg:122.62ms
step:184/1395 train_time:21335ms step_avg:122.62ms
step:185/1395 train_time:21458ms step_avg:122.62ms
step:186/1395 train_time:21581ms step_avg:122.62ms
step:187/1395 train_time:21704ms step_avg:122.62ms
step:188/1395 train_time:21828ms step_avg:122.63ms
step:189/1395 train_time:21950ms step_avg:122.63ms
step:190/1395 train_time:22073ms step_avg:122.63ms
step:191/1395 train_time:22195ms step_avg:122.63ms
step:192/1395 train_time:22319ms step_avg:122.63ms
step:193/1395 train_time:22442ms step_avg:122.63ms
step:194/1395 train_time:22565ms step_avg:122.63ms
step:195/1395 train_time:22687ms step_avg:122.63ms
step:196/1395 train_time:22810ms step_avg:122.64ms
step:197/1395 train_time:22933ms step_avg:122.64ms
step:198/1395 train_time:23056ms step_avg:122.64ms
step:199/1395 train_time:23180ms step_avg:122.65ms
step:200/1395 train_time:23302ms step_avg:122.64ms
step:201/1395 train_time:23425ms step_avg:122.64ms
step:202/1395 train_time:23549ms step_avg:122.65ms
step:203/1395 train_time:23671ms step_avg:122.65ms
step:204/1395 train_time:23794ms step_avg:122.65ms
step:205/1395 train_time:23918ms step_avg:122.66ms
step:206/1395 train_time:24042ms step_avg:122.66ms
step:207/1395 train_time:24164ms step_avg:122.66ms
step:208/1395 train_time:24288ms step_avg:122.67ms
step:209/1395 train_time:24411ms step_avg:122.67ms
step:210/1395 train_time:24534ms step_avg:122.67ms
step:211/1395 train_time:24658ms step_avg:122.68ms
step:212/1395 train_time:24782ms step_avg:122.68ms
step:213/1395 train_time:24904ms step_avg:122.68ms
step:214/1395 train_time:25027ms step_avg:122.68ms
step:215/1395 train_time:25151ms step_avg:122.69ms
step:216/1395 train_time:25275ms step_avg:122.69ms
step:217/1395 train_time:25399ms step_avg:122.70ms
step:218/1395 train_time:25523ms step_avg:122.71ms
step:219/1395 train_time:25646ms step_avg:122.71ms
step:220/1395 train_time:25769ms step_avg:122.71ms
step:221/1395 train_time:25892ms step_avg:122.71ms
step:222/1395 train_time:26016ms step_avg:122.72ms
step:223/1395 train_time:26139ms step_avg:122.72ms
step:224/1395 train_time:26262ms step_avg:122.72ms
step:225/1395 train_time:26386ms step_avg:122.72ms
step:226/1395 train_time:26509ms step_avg:122.73ms
step:227/1395 train_time:26632ms step_avg:122.73ms
step:228/1395 train_time:26755ms step_avg:122.73ms
step:229/1395 train_time:26879ms step_avg:122.74ms
step:230/1395 train_time:27003ms step_avg:122.74ms
step:231/1395 train_time:27126ms step_avg:122.74ms
step:232/1395 train_time:27250ms step_avg:122.75ms
step:233/1395 train_time:27373ms step_avg:122.75ms
step:234/1395 train_time:27496ms step_avg:122.75ms
step:235/1395 train_time:27619ms step_avg:122.75ms
step:236/1395 train_time:27743ms step_avg:122.76ms
step:237/1395 train_time:27866ms step_avg:122.76ms
step:238/1395 train_time:27989ms step_avg:122.76ms
step:239/1395 train_time:28113ms step_avg:122.76ms
step:240/1395 train_time:28237ms step_avg:122.77ms
step:241/1395 train_time:28360ms step_avg:122.77ms
step:242/1395 train_time:28483ms step_avg:122.77ms
step:243/1395 train_time:28608ms step_avg:122.78ms
step:244/1395 train_time:28731ms step_avg:122.78ms
step:245/1395 train_time:28854ms step_avg:122.79ms
step:246/1395 train_time:28978ms step_avg:122.79ms
step:247/1395 train_time:29102ms step_avg:122.79ms
step:248/1395 train_time:29224ms step_avg:122.79ms
step:249/1395 train_time:29348ms step_avg:122.79ms
step:250/1395 train_time:29471ms step_avg:122.79ms
step:250/1395 val_loss:3.9716 train_time:29592ms step_avg:123.30ms
step:251/1395 train_time:29613ms step_avg:122.87ms
step:252/1395 train_time:29729ms step_avg:122.85ms
step:253/1395 train_time:29857ms step_avg:122.87ms
step:254/1395 train_time:29980ms step_avg:122.87ms
step:255/1395 train_time:30102ms step_avg:122.86ms
step:256/1395 train_time:30224ms step_avg:122.86ms
step:257/1395 train_time:30347ms step_avg:122.86ms
step:258/1395 train_time:30470ms step_avg:122.86ms
step:259/1395 train_time:30593ms step_avg:122.86ms
step:260/1395 train_time:30717ms step_avg:122.87ms
step:261/1395 train_time:30842ms step_avg:122.88ms
step:262/1395 train_time:30967ms step_avg:122.88ms
step:263/1395 train_time:31091ms step_avg:122.89ms
step:264/1395 train_time:31214ms step_avg:122.89ms
step:265/1395 train_time:31336ms step_avg:122.89ms
step:266/1395 train_time:31461ms step_avg:122.89ms
step:267/1395 train_time:31583ms step_avg:122.89ms
step:268/1395 train_time:31707ms step_avg:122.89ms
step:269/1395 train_time:31830ms step_avg:122.90ms
step:270/1395 train_time:31953ms step_avg:122.90ms
step:271/1395 train_time:32076ms step_avg:122.90ms
step:272/1395 train_time:32199ms step_avg:122.90ms
step:273/1395 train_time:32323ms step_avg:122.90ms
step:274/1395 train_time:32447ms step_avg:122.90ms
step:275/1395 train_time:32571ms step_avg:122.91ms
step:276/1395 train_time:32694ms step_avg:122.91ms
step:277/1395 train_time:32817ms step_avg:122.91ms
step:278/1395 train_time:32941ms step_avg:122.91ms
step:279/1395 train_time:33064ms step_avg:122.91ms
step:280/1395 train_time:33188ms step_avg:122.92ms
step:281/1395 train_time:33312ms step_avg:122.92ms
step:282/1395 train_time:33436ms step_avg:122.93ms
step:283/1395 train_time:33559ms step_avg:122.93ms
step:284/1395 train_time:33682ms step_avg:122.93ms
step:285/1395 train_time:33805ms step_avg:122.93ms
step:286/1395 train_time:33929ms step_avg:122.93ms
step:287/1395 train_time:34053ms step_avg:122.93ms
step:288/1395 train_time:34176ms step_avg:122.94ms
step:289/1395 train_time:34300ms step_avg:122.94ms
step:290/1395 train_time:34423ms step_avg:122.94ms
step:291/1395 train_time:34547ms step_avg:122.94ms
step:292/1395 train_time:34671ms step_avg:122.95ms
step:293/1395 train_time:34794ms step_avg:122.95ms
step:294/1395 train_time:34916ms step_avg:122.94ms
step:295/1395 train_time:35039ms step_avg:122.94ms
step:296/1395 train_time:35162ms step_avg:122.94ms
step:297/1395 train_time:35284ms step_avg:122.94ms
step:298/1395 train_time:35410ms step_avg:122.95ms
step:299/1395 train_time:35534ms step_avg:122.95ms
step:300/1395 train_time:35656ms step_avg:122.95ms
step:301/1395 train_time:35780ms step_avg:122.96ms
step:302/1395 train_time:35903ms step_avg:122.95ms
step:303/1395 train_time:36026ms step_avg:122.95ms
step:304/1395 train_time:36149ms step_avg:122.96ms
step:305/1395 train_time:36273ms step_avg:122.96ms
step:306/1395 train_time:36398ms step_avg:122.97ms
step:307/1395 train_time:36521ms step_avg:122.97ms
step:308/1395 train_time:36645ms step_avg:122.97ms
step:309/1395 train_time:36768ms step_avg:122.97ms
step:310/1395 train_time:36891ms step_avg:122.97ms
step:311/1395 train_time:37015ms step_avg:122.97ms
step:312/1395 train_time:37138ms step_avg:122.97ms
step:313/1395 train_time:37265ms step_avg:122.99ms
step:314/1395 train_time:37390ms step_avg:122.99ms
step:315/1395 train_time:37517ms step_avg:123.01ms
step:316/1395 train_time:37643ms step_avg:123.02ms
step:317/1395 train_time:37769ms step_avg:123.03ms
step:318/1395 train_time:37894ms step_avg:123.03ms
step:319/1395 train_time:38021ms step_avg:123.04ms
step:320/1395 train_time:38147ms step_avg:123.06ms
step:321/1395 train_time:38274ms step_avg:123.07ms
step:322/1395 train_time:38400ms step_avg:123.08ms
step:323/1395 train_time:38526ms step_avg:123.08ms
step:324/1395 train_time:38652ms step_avg:123.09ms
step:325/1395 train_time:38778ms step_avg:123.10ms
step:326/1395 train_time:38904ms step_avg:123.11ms
step:327/1395 train_time:39030ms step_avg:123.12ms
step:328/1395 train_time:39155ms step_avg:123.13ms
step:329/1395 train_time:39281ms step_avg:123.14ms
step:330/1395 train_time:39407ms step_avg:123.15ms
step:331/1395 train_time:39532ms step_avg:123.15ms
step:332/1395 train_time:39658ms step_avg:123.16ms
step:333/1395 train_time:39785ms step_avg:123.17ms
step:334/1395 train_time:39910ms step_avg:123.18ms
step:335/1395 train_time:40037ms step_avg:123.19ms
step:336/1395 train_time:40162ms step_avg:123.20ms
step:337/1395 train_time:40288ms step_avg:123.20ms
step:338/1395 train_time:40413ms step_avg:123.21ms
step:339/1395 train_time:40538ms step_avg:123.22ms
step:340/1395 train_time:40664ms step_avg:123.22ms
step:341/1395 train_time:40789ms step_avg:123.23ms
step:342/1395 train_time:40915ms step_avg:123.24ms
step:343/1395 train_time:41041ms step_avg:123.25ms
step:344/1395 train_time:41166ms step_avg:123.25ms
step:345/1395 train_time:41294ms step_avg:123.27ms
step:346/1395 train_time:41419ms step_avg:123.27ms
step:347/1395 train_time:41544ms step_avg:123.28ms
step:348/1395 train_time:41672ms step_avg:123.29ms
step:349/1395 train_time:41797ms step_avg:123.30ms
step:350/1395 train_time:41923ms step_avg:123.30ms
step:351/1395 train_time:42049ms step_avg:123.31ms
step:352/1395 train_time:42175ms step_avg:123.32ms
step:353/1395 train_time:42300ms step_avg:123.32ms
step:354/1395 train_time:42426ms step_avg:123.33ms
step:355/1395 train_time:42552ms step_avg:123.34ms
step:356/1395 train_time:42678ms step_avg:123.35ms
step:357/1395 train_time:42804ms step_avg:123.35ms
step:358/1395 train_time:42930ms step_avg:123.36ms
step:359/1395 train_time:43056ms step_avg:123.37ms
step:360/1395 train_time:43181ms step_avg:123.37ms
step:361/1395 train_time:43306ms step_avg:123.38ms
step:362/1395 train_time:43431ms step_avg:123.38ms
step:363/1395 train_time:43557ms step_avg:123.39ms
step:364/1395 train_time:43684ms step_avg:123.40ms
step:365/1395 train_time:43810ms step_avg:123.41ms
step:366/1395 train_time:43936ms step_avg:123.42ms
step:367/1395 train_time:44062ms step_avg:123.42ms
step:368/1395 train_time:44187ms step_avg:123.43ms
step:369/1395 train_time:44313ms step_avg:123.43ms
step:370/1395 train_time:44438ms step_avg:123.44ms
step:371/1395 train_time:44564ms step_avg:123.45ms
step:372/1395 train_time:44690ms step_avg:123.45ms
step:373/1395 train_time:44816ms step_avg:123.46ms
step:374/1395 train_time:44942ms step_avg:123.47ms
step:375/1395 train_time:45067ms step_avg:123.47ms
step:375/1395 val_loss:3.7797 train_time:45192ms step_avg:123.81ms
step:376/1395 train_time:45216ms step_avg:123.54ms
step:377/1395 train_time:45329ms step_avg:123.51ms
step:378/1395 train_time:45458ms step_avg:123.53ms
step:379/1395 train_time:45583ms step_avg:123.53ms
step:380/1395 train_time:45708ms step_avg:123.53ms
step:381/1395 train_time:45833ms step_avg:123.54ms
step:382/1395 train_time:45958ms step_avg:123.54ms
step:383/1395 train_time:46083ms step_avg:123.55ms
step:384/1395 train_time:46209ms step_avg:123.55ms
step:385/1395 train_time:46336ms step_avg:123.56ms
step:386/1395 train_time:46462ms step_avg:123.57ms
step:387/1395 train_time:46588ms step_avg:123.58ms
step:388/1395 train_time:46714ms step_avg:123.58ms
step:389/1395 train_time:46841ms step_avg:123.59ms
step:390/1395 train_time:46967ms step_avg:123.60ms
step:391/1395 train_time:47092ms step_avg:123.60ms
step:392/1395 train_time:47218ms step_avg:123.61ms
step:393/1395 train_time:47343ms step_avg:123.61ms
step:394/1395 train_time:47469ms step_avg:123.62ms
step:395/1395 train_time:47594ms step_avg:123.62ms
step:396/1395 train_time:47721ms step_avg:123.63ms
step:397/1395 train_time:47847ms step_avg:123.64ms
step:398/1395 train_time:47973ms step_avg:123.64ms
step:399/1395 train_time:48099ms step_avg:123.65ms
step:400/1395 train_time:48224ms step_avg:123.65ms
step:401/1395 train_time:48349ms step_avg:123.65ms
step:402/1395 train_time:48475ms step_avg:123.66ms
step:403/1395 train_time:48600ms step_avg:123.67ms
step:404/1395 train_time:48725ms step_avg:123.67ms
step:405/1395 train_time:48852ms step_avg:123.68ms
step:406/1395 train_time:48978ms step_avg:123.68ms
step:407/1395 train_time:49103ms step_avg:123.69ms
step:408/1395 train_time:49229ms step_avg:123.69ms
step:409/1395 train_time:49355ms step_avg:123.70ms
step:410/1395 train_time:49480ms step_avg:123.70ms
step:411/1395 train_time:49606ms step_avg:123.71ms
step:412/1395 train_time:49732ms step_avg:123.71ms
step:413/1395 train_time:49858ms step_avg:123.72ms
step:414/1395 train_time:49984ms step_avg:123.72ms
step:415/1395 train_time:50110ms step_avg:123.73ms
step:416/1395 train_time:50236ms step_avg:123.73ms
step:417/1395 train_time:50363ms step_avg:123.74ms
step:418/1395 train_time:50489ms step_avg:123.75ms
step:419/1395 train_time:50614ms step_avg:123.75ms
step:420/1395 train_time:50740ms step_avg:123.76ms
step:421/1395 train_time:50866ms step_avg:123.76ms
step:422/1395 train_time:50992ms step_avg:123.77ms
step:423/1395 train_time:51118ms step_avg:123.77ms
step:424/1395 train_time:51244ms step_avg:123.78ms
step:425/1395 train_time:51371ms step_avg:123.79ms
step:426/1395 train_time:51497ms step_avg:123.79ms
step:427/1395 train_time:51624ms step_avg:123.80ms
step:428/1395 train_time:51750ms step_avg:123.80ms
step:429/1395 train_time:51876ms step_avg:123.81ms
step:430/1395 train_time:52003ms step_avg:123.82ms
step:431/1395 train_time:52128ms step_avg:123.82ms
step:432/1395 train_time:52254ms step_avg:123.82ms
step:433/1395 train_time:52381ms step_avg:123.83ms
step:434/1395 train_time:52506ms step_avg:123.83ms
step:435/1395 train_time:52632ms step_avg:123.84ms
step:436/1395 train_time:52758ms step_avg:123.85ms
step:437/1395 train_time:52884ms step_avg:123.85ms
step:438/1395 train_time:53011ms step_avg:123.86ms
step:439/1395 train_time:53137ms step_avg:123.86ms
step:440/1395 train_time:53263ms step_avg:123.87ms
step:441/1395 train_time:53390ms step_avg:123.87ms
step:442/1395 train_time:53516ms step_avg:123.88ms
step:443/1395 train_time:53641ms step_avg:123.88ms
step:444/1395 train_time:53768ms step_avg:123.89ms
step:445/1395 train_time:53895ms step_avg:123.90ms
step:446/1395 train_time:54021ms step_avg:123.90ms
step:447/1395 train_time:54148ms step_avg:123.91ms
step:448/1395 train_time:54274ms step_avg:123.91ms
step:449/1395 train_time:54400ms step_avg:123.92ms
step:450/1395 train_time:54526ms step_avg:123.92ms
step:451/1395 train_time:54653ms step_avg:123.93ms
step:452/1395 train_time:54779ms step_avg:123.93ms
step:453/1395 train_time:54905ms step_avg:123.94ms
step:454/1395 train_time:55032ms step_avg:123.95ms
step:455/1395 train_time:55159ms step_avg:123.95ms
step:456/1395 train_time:55284ms step_avg:123.96ms
step:457/1395 train_time:55410ms step_avg:123.96ms
step:458/1395 train_time:55537ms step_avg:123.97ms
step:459/1395 train_time:55663ms step_avg:123.97ms
step:460/1395 train_time:55790ms step_avg:123.98ms
step:461/1395 train_time:55916ms step_avg:123.98ms
step:462/1395 train_time:56042ms step_avg:123.99ms
step:463/1395 train_time:56168ms step_avg:123.99ms
step:464/1395 train_time:56294ms step_avg:124.00ms
step:465/1395 train_time:56421ms step_avg:124.00ms
step:466/1395 train_time:56547ms step_avg:124.01ms
step:467/1395 train_time:56672ms step_avg:124.01ms
step:468/1395 train_time:56797ms step_avg:124.01ms
step:469/1395 train_time:56924ms step_avg:124.02ms
step:470/1395 train_time:57050ms step_avg:124.02ms
step:471/1395 train_time:57177ms step_avg:124.03ms
step:472/1395 train_time:57304ms step_avg:124.03ms
step:473/1395 train_time:57431ms step_avg:124.04ms
step:474/1395 train_time:57557ms step_avg:124.05ms
step:475/1395 train_time:57683ms step_avg:124.05ms
step:476/1395 train_time:57810ms step_avg:124.06ms
step:477/1395 train_time:57936ms step_avg:124.06ms
step:478/1395 train_time:58062ms step_avg:124.06ms
step:479/1395 train_time:58188ms step_avg:124.07ms
step:480/1395 train_time:58314ms step_avg:124.07ms
step:481/1395 train_time:58441ms step_avg:124.08ms
step:482/1395 train_time:58566ms step_avg:124.08ms
step:483/1395 train_time:58692ms step_avg:124.09ms
step:484/1395 train_time:58819ms step_avg:124.09ms
step:485/1395 train_time:58945ms step_avg:124.09ms
step:486/1395 train_time:59072ms step_avg:124.10ms
step:487/1395 train_time:59198ms step_avg:124.10ms
step:488/1395 train_time:59324ms step_avg:124.11ms
step:489/1395 train_time:59450ms step_avg:124.11ms
step:490/1395 train_time:59576ms step_avg:124.12ms
step:491/1395 train_time:59702ms step_avg:124.12ms
step:492/1395 train_time:59828ms step_avg:124.12ms
step:493/1395 train_time:59954ms step_avg:124.13ms
step:494/1395 train_time:60080ms step_avg:124.13ms
step:495/1395 train_time:60207ms step_avg:124.14ms
step:496/1395 train_time:60333ms step_avg:124.14ms
step:497/1395 train_time:60459ms step_avg:124.15ms
step:498/1395 train_time:60585ms step_avg:124.15ms
step:499/1395 train_time:60711ms step_avg:124.15ms
step:500/1395 train_time:60838ms step_avg:124.16ms
step:500/1395 val_loss:3.6617 train_time:60962ms step_avg:124.41ms
step:501/1395 train_time:60986ms step_avg:124.21ms
step:502/1395 train_time:61102ms step_avg:124.19ms
step:503/1395 train_time:61229ms step_avg:124.20ms
step:504/1395 train_time:61354ms step_avg:124.20ms
step:505/1395 train_time:61480ms step_avg:124.20ms
step:506/1395 train_time:61606ms step_avg:124.21ms
step:507/1395 train_time:61732ms step_avg:124.21ms
step:508/1395 train_time:61858ms step_avg:124.21ms
step:509/1395 train_time:61984ms step_avg:124.22ms
step:510/1395 train_time:62111ms step_avg:124.22ms
step:511/1395 train_time:62238ms step_avg:124.23ms
step:512/1395 train_time:62365ms step_avg:124.23ms
step:513/1395 train_time:62490ms step_avg:124.24ms
step:514/1395 train_time:62616ms step_avg:124.24ms
step:515/1395 train_time:62742ms step_avg:124.24ms
step:516/1395 train_time:62868ms step_avg:124.25ms
step:517/1395 train_time:62994ms step_avg:124.25ms
step:518/1395 train_time:63120ms step_avg:124.25ms
step:519/1395 train_time:63251ms step_avg:124.26ms
step:520/1395 train_time:63380ms step_avg:124.27ms
step:521/1395 train_time:63509ms step_avg:124.28ms
step:522/1395 train_time:63637ms step_avg:124.29ms
step:523/1395 train_time:63765ms step_avg:124.30ms
step:524/1395 train_time:63893ms step_avg:124.31ms
step:525/1395 train_time:64021ms step_avg:124.31ms
step:526/1395 train_time:64149ms step_avg:124.32ms
step:527/1395 train_time:64279ms step_avg:124.33ms
step:528/1395 train_time:64408ms step_avg:124.34ms
step:529/1395 train_time:64537ms step_avg:124.35ms
step:530/1395 train_time:64665ms step_avg:124.36ms
step:531/1395 train_time:64793ms step_avg:124.36ms
step:532/1395 train_time:64920ms step_avg:124.37ms
step:533/1395 train_time:65049ms step_avg:124.38ms
step:534/1395 train_time:65178ms step_avg:124.39ms
step:535/1395 train_time:65306ms step_avg:124.39ms
step:536/1395 train_time:65434ms step_avg:124.40ms
step:537/1395 train_time:65562ms step_avg:124.41ms
step:538/1395 train_time:65690ms step_avg:124.41ms
step:539/1395 train_time:65819ms step_avg:124.42ms
step:540/1395 train_time:65947ms step_avg:124.43ms
step:541/1395 train_time:66077ms step_avg:124.44ms
step:542/1395 train_time:66205ms step_avg:124.45ms
step:543/1395 train_time:66335ms step_avg:124.46ms
step:544/1395 train_time:66463ms step_avg:124.46ms
step:545/1395 train_time:66592ms step_avg:124.47ms
step:546/1395 train_time:66720ms step_avg:124.48ms
step:547/1395 train_time:66849ms step_avg:124.49ms
step:548/1395 train_time:66978ms step_avg:124.49ms
step:549/1395 train_time:67107ms step_avg:124.50ms
step:550/1395 train_time:67235ms step_avg:124.51ms
step:551/1395 train_time:67363ms step_avg:124.52ms
step:552/1395 train_time:67492ms step_avg:124.52ms
step:553/1395 train_time:67620ms step_avg:124.53ms
step:554/1395 train_time:67749ms step_avg:124.54ms
step:555/1395 train_time:67878ms step_avg:124.55ms
step:556/1395 train_time:68006ms step_avg:124.55ms
step:557/1395 train_time:68135ms step_avg:124.56ms
step:558/1395 train_time:68262ms step_avg:124.57ms
step:559/1395 train_time:68391ms step_avg:124.57ms
step:560/1395 train_time:68519ms step_avg:124.58ms
step:561/1395 train_time:68647ms step_avg:124.59ms
step:562/1395 train_time:68774ms step_avg:124.59ms
step:563/1395 train_time:68903ms step_avg:124.60ms
step:564/1395 train_time:69031ms step_avg:124.60ms
step:565/1395 train_time:69160ms step_avg:124.61ms
step:566/1395 train_time:69288ms step_avg:124.62ms
step:567/1395 train_time:69416ms step_avg:124.63ms
step:568/1395 train_time:69544ms step_avg:124.63ms
step:569/1395 train_time:69673ms step_avg:124.64ms
step:570/1395 train_time:69801ms step_avg:124.64ms
step:571/1395 train_time:69929ms step_avg:124.65ms
step:572/1395 train_time:70057ms step_avg:124.66ms
step:573/1395 train_time:70186ms step_avg:124.66ms
step:574/1395 train_time:70315ms step_avg:124.67ms
step:575/1395 train_time:70444ms step_avg:124.68ms
step:576/1395 train_time:70572ms step_avg:124.69ms
step:577/1395 train_time:70701ms step_avg:124.69ms
step:578/1395 train_time:70829ms step_avg:124.70ms
step:579/1395 train_time:70958ms step_avg:124.71ms
step:580/1395 train_time:71086ms step_avg:124.71ms
step:581/1395 train_time:71215ms step_avg:124.72ms
step:582/1395 train_time:71343ms step_avg:124.73ms
step:583/1395 train_time:71472ms step_avg:124.73ms
step:584/1395 train_time:71601ms step_avg:124.74ms
step:585/1395 train_time:71728ms step_avg:124.74ms
step:586/1395 train_time:71857ms step_avg:124.75ms
step:587/1395 train_time:71986ms step_avg:124.76ms
step:588/1395 train_time:72114ms step_avg:124.76ms
step:589/1395 train_time:72242ms step_avg:124.77ms
step:590/1395 train_time:72372ms step_avg:124.78ms
step:591/1395 train_time:72500ms step_avg:124.79ms
step:592/1395 train_time:72629ms step_avg:124.79ms
step:593/1395 train_time:72757ms step_avg:124.80ms
step:594/1395 train_time:72885ms step_avg:124.80ms
step:595/1395 train_time:73014ms step_avg:124.81ms
step:596/1395 train_time:73143ms step_avg:124.82ms
step:597/1395 train_time:73271ms step_avg:124.82ms
step:598/1395 train_time:73401ms step_avg:124.83ms
step:599/1395 train_time:73529ms step_avg:124.84ms
step:600/1395 train_time:73657ms step_avg:124.84ms
step:601/1395 train_time:73786ms step_avg:124.85ms
step:602/1395 train_time:73913ms step_avg:124.85ms
step:603/1395 train_time:74042ms step_avg:124.86ms
step:604/1395 train_time:74170ms step_avg:124.87ms
step:605/1395 train_time:74299ms step_avg:124.87ms
step:606/1395 train_time:74426ms step_avg:124.88ms
step:607/1395 train_time:74555ms step_avg:124.88ms
step:608/1395 train_time:74683ms step_avg:124.89ms
step:609/1395 train_time:74812ms step_avg:124.89ms
step:610/1395 train_time:74940ms step_avg:124.90ms
step:611/1395 train_time:75068ms step_avg:124.91ms
step:612/1395 train_time:75197ms step_avg:124.91ms
step:613/1395 train_time:75325ms step_avg:124.92ms
step:614/1395 train_time:75454ms step_avg:124.92ms
step:615/1395 train_time:75582ms step_avg:124.93ms
step:616/1395 train_time:75711ms step_avg:124.94ms
step:617/1395 train_time:75840ms step_avg:124.94ms
step:618/1395 train_time:75969ms step_avg:124.95ms
step:619/1395 train_time:76098ms step_avg:124.96ms
step:620/1395 train_time:76226ms step_avg:124.96ms
step:621/1395 train_time:76354ms step_avg:124.97ms
step:622/1395 train_time:76483ms step_avg:124.97ms
step:623/1395 train_time:76612ms step_avg:124.98ms
step:624/1395 train_time:76741ms step_avg:124.99ms
step:625/1395 train_time:76869ms step_avg:124.99ms
step:625/1395 val_loss:3.5774 train_time:76996ms step_avg:125.20ms
step:626/1395 train_time:77018ms step_avg:125.03ms
step:627/1395 train_time:77140ms step_avg:125.02ms
step:628/1395 train_time:77269ms step_avg:125.03ms
step:629/1395 train_time:77397ms step_avg:125.04ms
step:630/1395 train_time:77526ms step_avg:125.04ms
step:631/1395 train_time:77652ms step_avg:125.04ms
step:632/1395 train_time:77780ms step_avg:125.05ms
step:633/1395 train_time:77909ms step_avg:125.05ms
step:634/1395 train_time:78037ms step_avg:125.06ms
step:635/1395 train_time:78168ms step_avg:125.07ms
step:636/1395 train_time:78297ms step_avg:125.08ms
step:637/1395 train_time:78427ms step_avg:125.08ms
step:638/1395 train_time:78556ms step_avg:125.09ms
step:639/1395 train_time:78683ms step_avg:125.09ms
step:640/1395 train_time:78812ms step_avg:125.10ms
step:641/1395 train_time:78940ms step_avg:125.10ms
step:642/1395 train_time:79068ms step_avg:125.11ms
step:643/1395 train_time:79197ms step_avg:125.11ms
step:644/1395 train_time:79326ms step_avg:125.12ms
step:645/1395 train_time:79457ms step_avg:125.13ms
step:646/1395 train_time:79586ms step_avg:125.13ms
step:647/1395 train_time:79713ms step_avg:125.14ms
step:648/1395 train_time:79843ms step_avg:125.15ms
step:649/1395 train_time:79971ms step_avg:125.15ms
step:650/1395 train_time:80101ms step_avg:125.16ms
step:651/1395 train_time:80229ms step_avg:125.16ms
step:652/1395 train_time:80358ms step_avg:125.17ms
step:653/1395 train_time:80487ms step_avg:125.17ms
step:654/1395 train_time:80616ms step_avg:125.18ms
step:655/1395 train_time:80744ms step_avg:125.18ms
step:656/1395 train_time:80872ms step_avg:125.19ms
step:657/1395 train_time:81002ms step_avg:125.20ms
step:658/1395 train_time:81131ms step_avg:125.20ms
step:659/1395 train_time:81260ms step_avg:125.21ms
step:660/1395 train_time:81389ms step_avg:125.21ms
step:661/1395 train_time:81519ms step_avg:125.22ms
step:662/1395 train_time:81649ms step_avg:125.23ms
step:663/1395 train_time:81776ms step_avg:125.23ms
step:664/1395 train_time:81905ms step_avg:125.24ms
step:665/1395 train_time:82033ms step_avg:125.24ms
step:666/1395 train_time:82162ms step_avg:125.25ms
step:667/1395 train_time:82292ms step_avg:125.25ms
step:668/1395 train_time:82421ms step_avg:125.26ms
step:669/1395 train_time:82550ms step_avg:125.27ms
step:670/1395 train_time:82678ms step_avg:125.27ms
step:671/1395 train_time:82807ms step_avg:125.28ms
step:672/1395 train_time:82935ms step_avg:125.28ms
step:673/1395 train_time:83064ms step_avg:125.29ms
step:674/1395 train_time:83192ms step_avg:125.29ms
step:675/1395 train_time:83322ms step_avg:125.30ms
step:676/1395 train_time:83452ms step_avg:125.30ms
step:677/1395 train_time:83580ms step_avg:125.31ms
step:678/1395 train_time:83708ms step_avg:125.31ms
step:679/1395 train_time:83836ms step_avg:125.32ms
step:680/1395 train_time:83964ms step_avg:125.32ms
step:681/1395 train_time:84092ms step_avg:125.32ms
step:682/1395 train_time:84221ms step_avg:125.33ms
step:683/1395 train_time:84350ms step_avg:125.33ms
step:684/1395 train_time:84479ms step_avg:125.34ms
step:685/1395 train_time:84608ms step_avg:125.35ms
step:686/1395 train_time:84738ms step_avg:125.35ms
step:687/1395 train_time:84865ms step_avg:125.36ms
step:688/1395 train_time:84995ms step_avg:125.36ms
step:689/1395 train_time:85124ms step_avg:125.37ms
step:690/1395 train_time:85252ms step_avg:125.37ms
step:691/1395 train_time:85381ms step_avg:125.38ms
step:692/1395 train_time:85510ms step_avg:125.38ms
step:693/1395 train_time:85638ms step_avg:125.39ms
step:694/1395 train_time:85767ms step_avg:125.39ms
step:695/1395 train_time:85896ms step_avg:125.40ms
step:696/1395 train_time:86024ms step_avg:125.40ms
step:697/1395 train_time:86152ms step_avg:125.40ms
step:698/1395 train_time:86280ms step_avg:125.41ms
step:699/1395 train_time:86409ms step_avg:125.41ms
step:700/1395 train_time:86538ms step_avg:125.42ms
step:701/1395 train_time:86667ms step_avg:125.42ms
step:702/1395 train_time:86796ms step_avg:125.43ms
step:703/1395 train_time:86925ms step_avg:125.43ms
step:704/1395 train_time:87053ms step_avg:125.44ms
step:705/1395 train_time:87182ms step_avg:125.44ms
step:706/1395 train_time:87311ms step_avg:125.45ms
step:707/1395 train_time:87440ms step_avg:125.45ms
step:708/1395 train_time:87568ms step_avg:125.46ms
step:709/1395 train_time:87698ms step_avg:125.46ms
step:710/1395 train_time:87827ms step_avg:125.47ms
step:711/1395 train_time:87956ms step_avg:125.47ms
step:712/1395 train_time:88085ms step_avg:125.48ms
step:713/1395 train_time:88213ms step_avg:125.48ms
step:714/1395 train_time:88341ms step_avg:125.48ms
step:715/1395 train_time:88470ms step_avg:125.49ms
step:716/1395 train_time:88599ms step_avg:125.49ms
step:717/1395 train_time:88729ms step_avg:125.50ms
step:718/1395 train_time:88858ms step_avg:125.51ms
step:719/1395 train_time:88986ms step_avg:125.51ms
step:720/1395 train_time:89115ms step_avg:125.51ms
step:721/1395 train_time:89244ms step_avg:125.52ms
step:722/1395 train_time:89373ms step_avg:125.52ms
step:723/1395 train_time:89501ms step_avg:125.53ms
step:724/1395 train_time:89630ms step_avg:125.53ms
step:725/1395 train_time:89759ms step_avg:125.54ms
step:726/1395 train_time:89889ms step_avg:125.54ms
step:727/1395 train_time:90020ms step_avg:125.55ms
step:728/1395 train_time:90150ms step_avg:125.56ms
step:729/1395 train_time:90281ms step_avg:125.56ms
step:730/1395 train_time:90411ms step_avg:125.57ms
step:731/1395 train_time:90543ms step_avg:125.58ms
step:732/1395 train_time:90674ms step_avg:125.59ms
step:733/1395 train_time:90805ms step_avg:125.59ms
step:734/1395 train_time:90935ms step_avg:125.60ms
step:735/1395 train_time:91065ms step_avg:125.61ms
step:736/1395 train_time:91196ms step_avg:125.61ms
step:737/1395 train_time:91327ms step_avg:125.62ms
step:738/1395 train_time:91457ms step_avg:125.63ms
step:739/1395 train_time:91588ms step_avg:125.64ms
step:740/1395 train_time:91718ms step_avg:125.64ms
step:741/1395 train_time:91852ms step_avg:125.65ms
step:742/1395 train_time:91982ms step_avg:125.66ms
step:743/1395 train_time:92112ms step_avg:125.66ms
step:744/1395 train_time:92242ms step_avg:125.67ms
step:745/1395 train_time:92374ms step_avg:125.68ms
step:746/1395 train_time:92504ms step_avg:125.68ms
step:747/1395 train_time:92635ms step_avg:125.69ms
step:748/1395 train_time:92765ms step_avg:125.70ms
step:749/1395 train_time:92897ms step_avg:125.71ms
step:750/1395 train_time:93029ms step_avg:125.72ms
step:750/1395 val_loss:3.5264 train_time:93159ms step_avg:125.89ms
step:751/1395 train_time:93180ms step_avg:125.75ms
step:752/1395 train_time:93300ms step_avg:125.74ms
step:753/1395 train_time:93431ms step_avg:125.75ms
step:754/1395 train_time:93561ms step_avg:125.75ms
step:755/1395 train_time:93691ms step_avg:125.76ms
step:756/1395 train_time:93821ms step_avg:125.77ms
step:757/1395 train_time:93953ms step_avg:125.77ms
step:758/1395 train_time:94083ms step_avg:125.78ms
step:759/1395 train_time:94215ms step_avg:125.79ms
step:760/1395 train_time:94346ms step_avg:125.79ms
step:761/1395 train_time:94476ms step_avg:125.80ms
step:762/1395 train_time:94606ms step_avg:125.81ms
step:763/1395 train_time:94736ms step_avg:125.81ms
step:764/1395 train_time:94868ms step_avg:125.82ms
step:765/1395 train_time:94998ms step_avg:125.83ms
step:766/1395 train_time:95129ms step_avg:125.83ms
step:767/1395 train_time:95260ms step_avg:125.84ms
step:768/1395 train_time:95391ms step_avg:125.85ms
step:769/1395 train_time:95522ms step_avg:125.85ms
step:770/1395 train_time:95652ms step_avg:125.86ms
step:771/1395 train_time:95783ms step_avg:125.86ms
step:772/1395 train_time:95915ms step_avg:125.87ms
step:773/1395 train_time:96046ms step_avg:125.88ms
step:774/1395 train_time:96176ms step_avg:125.88ms
step:775/1395 train_time:96306ms step_avg:125.89ms
step:776/1395 train_time:96438ms step_avg:125.90ms
step:777/1395 train_time:96569ms step_avg:125.90ms
step:778/1395 train_time:96699ms step_avg:125.91ms
step:779/1395 train_time:96829ms step_avg:125.92ms
step:780/1395 train_time:96961ms step_avg:125.92ms
step:781/1395 train_time:97091ms step_avg:125.93ms
step:782/1395 train_time:97222ms step_avg:125.93ms
step:783/1395 train_time:97352ms step_avg:125.94ms
step:784/1395 train_time:97484ms step_avg:125.95ms
step:785/1395 train_time:97615ms step_avg:125.96ms
step:786/1395 train_time:97745ms step_avg:125.96ms
step:787/1395 train_time:97876ms step_avg:125.97ms
step:788/1395 train_time:98007ms step_avg:125.97ms
step:789/1395 train_time:98139ms step_avg:125.98ms
step:790/1395 train_time:98269ms step_avg:125.99ms
step:791/1395 train_time:98399ms step_avg:125.99ms
step:792/1395 train_time:98530ms step_avg:126.00ms
step:793/1395 train_time:98661ms step_avg:126.00ms
step:794/1395 train_time:98792ms step_avg:126.01ms
step:795/1395 train_time:98924ms step_avg:126.02ms
step:796/1395 train_time:99055ms step_avg:126.02ms
step:797/1395 train_time:99186ms step_avg:126.03ms
step:798/1395 train_time:99318ms step_avg:126.04ms
step:799/1395 train_time:99450ms step_avg:126.05ms
step:800/1395 train_time:99581ms step_avg:126.05ms
step:801/1395 train_time:99711ms step_avg:126.06ms
step:802/1395 train_time:99842ms step_avg:126.06ms
step:803/1395 train_time:99973ms step_avg:126.07ms
step:804/1395 train_time:100104ms step_avg:126.08ms
step:805/1395 train_time:100235ms step_avg:126.08ms
step:806/1395 train_time:100366ms step_avg:126.09ms
step:807/1395 train_time:100495ms step_avg:126.09ms
step:808/1395 train_time:100626ms step_avg:126.10ms
step:809/1395 train_time:100755ms step_avg:126.10ms
step:810/1395 train_time:100886ms step_avg:126.11ms
step:811/1395 train_time:101016ms step_avg:126.11ms
step:812/1395 train_time:101147ms step_avg:126.12ms
step:813/1395 train_time:101277ms step_avg:126.12ms
step:814/1395 train_time:101408ms step_avg:126.13ms
step:815/1395 train_time:101538ms step_avg:126.13ms
step:816/1395 train_time:101669ms step_avg:126.14ms
step:817/1395 train_time:101800ms step_avg:126.15ms
step:818/1395 train_time:101931ms step_avg:126.15ms
step:819/1395 train_time:102063ms step_avg:126.16ms
step:820/1395 train_time:102193ms step_avg:126.16ms
step:821/1395 train_time:102323ms step_avg:126.17ms
step:822/1395 train_time:102454ms step_avg:126.17ms
step:823/1395 train_time:102584ms step_avg:126.18ms
step:824/1395 train_time:102715ms step_avg:126.19ms
step:825/1395 train_time:102846ms step_avg:126.19ms
step:826/1395 train_time:102977ms step_avg:126.20ms
step:827/1395 train_time:103108ms step_avg:126.20ms
step:828/1395 train_time:103238ms step_avg:126.21ms
step:829/1395 train_time:103368ms step_avg:126.21ms
step:830/1395 train_time:103500ms step_avg:126.22ms
step:831/1395 train_time:103630ms step_avg:126.22ms
step:832/1395 train_time:103761ms step_avg:126.23ms
step:833/1395 train_time:103893ms step_avg:126.24ms
step:834/1395 train_time:104024ms step_avg:126.24ms
step:835/1395 train_time:104155ms step_avg:126.25ms
step:836/1395 train_time:104286ms step_avg:126.25ms
step:837/1395 train_time:104416ms step_avg:126.26ms
step:838/1395 train_time:104546ms step_avg:126.26ms
step:839/1395 train_time:104677ms step_avg:126.27ms
step:840/1395 train_time:104808ms step_avg:126.27ms
step:841/1395 train_time:104938ms step_avg:126.28ms
step:842/1395 train_time:105069ms step_avg:126.28ms
step:843/1395 train_time:105201ms step_avg:126.29ms
step:844/1395 train_time:105332ms step_avg:126.30ms
step:845/1395 train_time:105463ms step_avg:126.30ms
step:846/1395 train_time:105596ms step_avg:126.31ms
step:847/1395 train_time:105727ms step_avg:126.32ms
step:848/1395 train_time:105859ms step_avg:126.32ms
step:849/1395 train_time:105989ms step_avg:126.33ms
step:850/1395 train_time:106121ms step_avg:126.33ms
step:851/1395 train_time:106252ms step_avg:126.34ms
step:852/1395 train_time:106383ms step_avg:126.35ms
step:853/1395 train_time:106514ms step_avg:126.35ms
step:854/1395 train_time:106644ms step_avg:126.36ms
step:855/1395 train_time:106775ms step_avg:126.36ms
step:856/1395 train_time:106905ms step_avg:126.36ms
step:857/1395 train_time:107035ms step_avg:126.37ms
step:858/1395 train_time:107166ms step_avg:126.38ms
step:859/1395 train_time:107298ms step_avg:126.38ms
step:860/1395 train_time:107429ms step_avg:126.39ms
step:861/1395 train_time:107559ms step_avg:126.39ms
step:862/1395 train_time:107690ms step_avg:126.40ms
step:863/1395 train_time:107823ms step_avg:126.40ms
step:864/1395 train_time:107953ms step_avg:126.41ms
step:865/1395 train_time:108084ms step_avg:126.41ms
step:866/1395 train_time:108217ms step_avg:126.42ms
step:867/1395 train_time:108348ms step_avg:126.43ms
step:868/1395 train_time:108478ms step_avg:126.43ms
step:869/1395 train_time:108609ms step_avg:126.44ms
step:870/1395 train_time:108740ms step_avg:126.44ms
step:871/1395 train_time:108870ms step_avg:126.45ms
step:872/1395 train_time:109000ms step_avg:126.45ms
step:873/1395 train_time:109131ms step_avg:126.46ms
step:874/1395 train_time:109261ms step_avg:126.46ms
step:875/1395 train_time:109393ms step_avg:126.47ms
step:875/1395 val_loss:3.4758 train_time:109523ms step_avg:126.62ms
step:876/1395 train_time:109544ms step_avg:126.49ms
step:877/1395 train_time:109664ms step_avg:126.49ms
step:878/1395 train_time:109796ms step_avg:126.49ms
step:879/1395 train_time:109927ms step_avg:126.50ms
step:880/1395 train_time:110057ms step_avg:126.50ms
step:881/1395 train_time:110187ms step_avg:126.51ms
step:882/1395 train_time:110318ms step_avg:126.51ms
step:883/1395 train_time:110448ms step_avg:126.52ms
step:884/1395 train_time:110580ms step_avg:126.52ms
step:885/1395 train_time:110712ms step_avg:126.53ms
step:886/1395 train_time:110843ms step_avg:126.53ms
step:887/1395 train_time:110973ms step_avg:126.54ms
step:888/1395 train_time:111104ms step_avg:126.54ms
step:889/1395 train_time:111236ms step_avg:126.55ms
step:890/1395 train_time:111367ms step_avg:126.55ms
step:891/1395 train_time:111497ms step_avg:126.56ms
step:892/1395 train_time:111629ms step_avg:126.56ms
step:893/1395 train_time:111760ms step_avg:126.57ms
step:894/1395 train_time:111891ms step_avg:126.57ms
step:895/1395 train_time:112022ms step_avg:126.58ms
step:896/1395 train_time:112152ms step_avg:126.58ms
step:897/1395 train_time:112283ms step_avg:126.59ms
step:898/1395 train_time:112413ms step_avg:126.59ms
step:899/1395 train_time:112545ms step_avg:126.60ms
step:900/1395 train_time:112676ms step_avg:126.60ms
step:901/1395 train_time:112808ms step_avg:126.61ms
step:902/1395 train_time:112939ms step_avg:126.61ms
step:903/1395 train_time:113070ms step_avg:126.62ms
step:904/1395 train_time:113201ms step_avg:126.62ms
step:905/1395 train_time:113332ms step_avg:126.63ms
step:906/1395 train_time:113462ms step_avg:126.63ms
step:907/1395 train_time:113594ms step_avg:126.64ms
step:908/1395 train_time:113724ms step_avg:126.64ms
step:909/1395 train_time:113855ms step_avg:126.65ms
step:910/1395 train_time:113987ms step_avg:126.65ms
step:911/1395 train_time:114117ms step_avg:126.66ms
step:912/1395 train_time:114248ms step_avg:126.66ms
step:913/1395 train_time:114380ms step_avg:126.67ms
step:914/1395 train_time:114511ms step_avg:126.67ms
step:915/1395 train_time:114642ms step_avg:126.68ms
step:916/1395 train_time:114775ms step_avg:126.68ms
step:917/1395 train_time:114908ms step_avg:126.69ms
step:918/1395 train_time:115038ms step_avg:126.69ms
step:919/1395 train_time:115171ms step_avg:126.70ms
step:920/1395 train_time:115302ms step_avg:126.71ms
step:921/1395 train_time:115432ms step_avg:126.71ms
step:922/1395 train_time:115564ms step_avg:126.71ms
step:923/1395 train_time:115694ms step_avg:126.72ms
step:924/1395 train_time:115824ms step_avg:126.72ms
step:925/1395 train_time:115956ms step_avg:126.73ms
step:926/1395 train_time:116087ms step_avg:126.73ms
step:927/1395 train_time:116217ms step_avg:126.74ms
step:928/1395 train_time:116347ms step_avg:126.74ms
step:929/1395 train_time:116479ms step_avg:126.75ms
step:930/1395 train_time:116610ms step_avg:126.75ms
step:931/1395 train_time:116742ms step_avg:126.76ms
step:932/1395 train_time:116873ms step_avg:126.76ms
step:933/1395 train_time:117006ms step_avg:126.77ms
step:934/1395 train_time:117139ms step_avg:126.77ms
step:935/1395 train_time:117272ms step_avg:126.78ms
step:936/1395 train_time:117404ms step_avg:126.79ms
step:937/1395 train_time:117537ms step_avg:126.79ms
step:938/1395 train_time:117671ms step_avg:126.80ms
step:939/1395 train_time:117803ms step_avg:126.81ms
step:940/1395 train_time:117936ms step_avg:126.81ms
step:941/1395 train_time:118068ms step_avg:126.82ms
step:942/1395 train_time:118200ms step_avg:126.82ms
step:943/1395 train_time:118333ms step_avg:126.83ms
step:944/1395 train_time:118466ms step_avg:126.84ms
step:945/1395 train_time:118599ms step_avg:126.84ms
step:946/1395 train_time:118732ms step_avg:126.85ms
step:947/1395 train_time:118865ms step_avg:126.86ms
step:948/1395 train_time:118998ms step_avg:126.86ms
step:949/1395 train_time:119130ms step_avg:126.87ms
step:950/1395 train_time:119262ms step_avg:126.87ms
step:951/1395 train_time:119397ms step_avg:126.88ms
step:952/1395 train_time:119529ms step_avg:126.89ms
step:953/1395 train_time:119662ms step_avg:126.89ms
step:954/1395 train_time:119795ms step_avg:126.90ms
step:955/1395 train_time:119927ms step_avg:126.91ms
step:956/1395 train_time:120060ms step_avg:126.91ms
step:957/1395 train_time:120192ms step_avg:126.92ms
step:958/1395 train_time:120326ms step_avg:126.93ms
step:959/1395 train_time:120459ms step_avg:126.93ms
step:960/1395 train_time:120592ms step_avg:126.94ms
step:961/1395 train_time:120724ms step_avg:126.94ms
step:962/1395 train_time:120857ms step_avg:126.95ms
step:963/1395 train_time:120989ms step_avg:126.96ms
step:964/1395 train_time:121121ms step_avg:126.96ms
step:965/1395 train_time:121256ms step_avg:126.97ms
step:966/1395 train_time:121387ms step_avg:126.97ms
step:967/1395 train_time:121520ms step_avg:126.98ms
step:968/1395 train_time:121651ms step_avg:126.98ms
step:969/1395 train_time:121784ms step_avg:126.99ms
step:970/1395 train_time:121916ms step_avg:127.00ms
step:971/1395 train_time:122048ms step_avg:127.00ms
step:972/1395 train_time:122181ms step_avg:127.01ms
step:973/1395 train_time:122313ms step_avg:127.01ms
step:974/1395 train_time:122445ms step_avg:127.02ms
step:975/1395 train_time:122577ms step_avg:127.02ms
step:976/1395 train_time:122710ms step_avg:127.03ms
step:977/1395 train_time:122841ms step_avg:127.03ms
step:978/1395 train_time:122974ms step_avg:127.04ms
step:979/1395 train_time:123107ms step_avg:127.04ms
step:980/1395 train_time:123240ms step_avg:127.05ms
step:981/1395 train_time:123371ms step_avg:127.06ms
step:982/1395 train_time:123503ms step_avg:127.06ms
step:983/1395 train_time:123636ms step_avg:127.07ms
step:984/1395 train_time:123768ms step_avg:127.07ms
step:985/1395 train_time:123901ms step_avg:127.08ms
step:986/1395 train_time:124037ms step_avg:127.09ms
step:987/1395 train_time:124169ms step_avg:127.09ms
step:988/1395 train_time:124301ms step_avg:127.10ms
step:989/1395 train_time:124433ms step_avg:127.10ms
step:990/1395 train_time:124566ms step_avg:127.11ms
step:991/1395 train_time:124698ms step_avg:127.11ms
step:992/1395 train_time:124831ms step_avg:127.12ms
step:993/1395 train_time:124965ms step_avg:127.13ms
step:994/1395 train_time:125097ms step_avg:127.13ms
step:995/1395 train_time:125229ms step_avg:127.14ms
step:996/1395 train_time:125361ms step_avg:127.14ms
step:997/1395 train_time:125493ms step_avg:127.15ms
step:998/1395 train_time:125624ms step_avg:127.15ms
step:999/1395 train_time:125758ms step_avg:127.16ms
step:1000/1395 train_time:125890ms step_avg:127.16ms
step:1000/1395 val_loss:3.4127 train_time:126020ms step_avg:127.29ms
step:1001/1395 train_time:126041ms step_avg:127.19ms
step:1002/1395 train_time:126166ms step_avg:127.18ms
step:1003/1395 train_time:126299ms step_avg:127.19ms
step:1004/1395 train_time:126430ms step_avg:127.19ms
step:1005/1395 train_time:126563ms step_avg:127.20ms
step:1006/1395 train_time:126695ms step_avg:127.20ms
step:1007/1395 train_time:126826ms step_avg:127.21ms
step:1008/1395 train_time:126958ms step_avg:127.21ms
step:1009/1395 train_time:127093ms step_avg:127.22ms
step:1010/1395 train_time:127226ms step_avg:127.23ms
step:1011/1395 train_time:127362ms step_avg:127.23ms
step:1012/1395 train_time:127494ms step_avg:127.24ms
step:1013/1395 train_time:127627ms step_avg:127.24ms
step:1014/1395 train_time:127758ms step_avg:127.25ms
step:1015/1395 train_time:127891ms step_avg:127.25ms
step:1016/1395 train_time:128024ms step_avg:127.26ms
step:1017/1395 train_time:128156ms step_avg:127.27ms
step:1018/1395 train_time:128289ms step_avg:127.27ms
step:1019/1395 train_time:128422ms step_avg:127.28ms
step:1020/1395 train_time:128554ms step_avg:127.28ms
step:1021/1395 train_time:128687ms step_avg:127.29ms
step:1022/1395 train_time:128819ms step_avg:127.29ms
step:1023/1395 train_time:128952ms step_avg:127.30ms
step:1024/1395 train_time:129084ms step_avg:127.30ms
step:1025/1395 train_time:129218ms step_avg:127.31ms
step:1026/1395 train_time:129351ms step_avg:127.31ms
step:1027/1395 train_time:129483ms step_avg:127.32ms
step:1028/1395 train_time:129615ms step_avg:127.32ms
step:1029/1395 train_time:129747ms step_avg:127.33ms
step:1030/1395 train_time:129880ms step_avg:127.33ms
step:1031/1395 train_time:130011ms step_avg:127.34ms
step:1032/1395 train_time:130144ms step_avg:127.34ms
step:1033/1395 train_time:130278ms step_avg:127.35ms
step:1034/1395 train_time:130410ms step_avg:127.35ms
step:1035/1395 train_time:130543ms step_avg:127.36ms
step:1036/1395 train_time:130676ms step_avg:127.36ms
step:1037/1395 train_time:130809ms step_avg:127.37ms
step:1038/1395 train_time:130941ms step_avg:127.37ms
step:1039/1395 train_time:131073ms step_avg:127.38ms
step:1040/1395 train_time:131206ms step_avg:127.38ms
step:1041/1395 train_time:131340ms step_avg:127.39ms
step:1042/1395 train_time:131473ms step_avg:127.40ms
step:1043/1395 train_time:131606ms step_avg:127.40ms
step:1044/1395 train_time:131742ms step_avg:127.41ms
step:1045/1395 train_time:131875ms step_avg:127.42ms
step:1046/1395 train_time:132008ms step_avg:127.42ms
step:1047/1395 train_time:132140ms step_avg:127.43ms
step:1048/1395 train_time:132273ms step_avg:127.43ms
step:1049/1395 train_time:132407ms step_avg:127.44ms
step:1050/1395 train_time:132540ms step_avg:127.44ms
step:1051/1395 train_time:132674ms step_avg:127.45ms
step:1052/1395 train_time:132805ms step_avg:127.45ms
step:1053/1395 train_time:132938ms step_avg:127.46ms
step:1054/1395 train_time:133070ms step_avg:127.46ms
step:1055/1395 train_time:133203ms step_avg:127.47ms
step:1056/1395 train_time:133335ms step_avg:127.47ms
step:1057/1395 train_time:133468ms step_avg:127.48ms
step:1058/1395 train_time:133601ms step_avg:127.48ms
step:1059/1395 train_time:133734ms step_avg:127.49ms
step:1060/1395 train_time:133869ms step_avg:127.49ms
step:1061/1395 train_time:134001ms step_avg:127.50ms
step:1062/1395 train_time:134133ms step_avg:127.50ms
step:1063/1395 train_time:134266ms step_avg:127.51ms
step:1064/1395 train_time:134399ms step_avg:127.51ms
step:1065/1395 train_time:134531ms step_avg:127.52ms
step:1066/1395 train_time:134666ms step_avg:127.52ms
step:1067/1395 train_time:134798ms step_avg:127.53ms
step:1068/1395 train_time:134930ms step_avg:127.53ms
step:1069/1395 train_time:135065ms step_avg:127.54ms
step:1070/1395 train_time:135198ms step_avg:127.55ms
step:1071/1395 train_time:135333ms step_avg:127.55ms
step:1072/1395 train_time:135464ms step_avg:127.56ms
step:1073/1395 train_time:135597ms step_avg:127.56ms
step:1074/1395 train_time:135729ms step_avg:127.56ms
step:1075/1395 train_time:135862ms step_avg:127.57ms
step:1076/1395 train_time:135993ms step_avg:127.57ms
step:1077/1395 train_time:136126ms step_avg:127.58ms
step:1078/1395 train_time:136258ms step_avg:127.58ms
step:1079/1395 train_time:136395ms step_avg:127.59ms
step:1080/1395 train_time:136528ms step_avg:127.60ms
step:1081/1395 train_time:136661ms step_avg:127.60ms
step:1082/1395 train_time:136792ms step_avg:127.60ms
step:1083/1395 train_time:136926ms step_avg:127.61ms
step:1084/1395 train_time:137061ms step_avg:127.62ms
step:1085/1395 train_time:137193ms step_avg:127.62ms
step:1086/1395 train_time:137327ms step_avg:127.63ms
step:1087/1395 train_time:137460ms step_avg:127.63ms
step:1088/1395 train_time:137593ms step_avg:127.64ms
step:1089/1395 train_time:137729ms step_avg:127.64ms
step:1090/1395 train_time:137864ms step_avg:127.65ms
step:1091/1395 train_time:137997ms step_avg:127.66ms
step:1092/1395 train_time:138128ms step_avg:127.66ms
step:1093/1395 train_time:138260ms step_avg:127.66ms
step:1094/1395 train_time:138392ms step_avg:127.67ms
step:1095/1395 train_time:138526ms step_avg:127.67ms
step:1096/1395 train_time:138660ms step_avg:127.68ms
step:1097/1395 train_time:138794ms step_avg:127.69ms
step:1098/1395 train_time:138928ms step_avg:127.69ms
step:1099/1395 train_time:139061ms step_avg:127.70ms
step:1100/1395 train_time:139193ms step_avg:127.70ms
step:1101/1395 train_time:139326ms step_avg:127.70ms
step:1102/1395 train_time:139459ms step_avg:127.71ms
step:1103/1395 train_time:139593ms step_avg:127.72ms
step:1104/1395 train_time:139726ms step_avg:127.72ms
step:1105/1395 train_time:139862ms step_avg:127.73ms
step:1106/1395 train_time:139995ms step_avg:127.73ms
step:1107/1395 train_time:140128ms step_avg:127.74ms
step:1108/1395 train_time:140263ms step_avg:127.74ms
step:1109/1395 train_time:140394ms step_avg:127.75ms
step:1110/1395 train_time:140527ms step_avg:127.75ms
step:1111/1395 train_time:140660ms step_avg:127.76ms
step:1112/1395 train_time:140793ms step_avg:127.76ms
step:1113/1395 train_time:140926ms step_avg:127.77ms
step:1114/1395 train_time:141059ms step_avg:127.77ms
step:1115/1395 train_time:141191ms step_avg:127.78ms
step:1116/1395 train_time:141324ms step_avg:127.78ms
step:1117/1395 train_time:141458ms step_avg:127.79ms
step:1118/1395 train_time:141592ms step_avg:127.79ms
step:1119/1395 train_time:141725ms step_avg:127.80ms
step:1120/1395 train_time:141857ms step_avg:127.80ms
step:1121/1395 train_time:141990ms step_avg:127.80ms
step:1122/1395 train_time:142122ms step_avg:127.81ms
step:1123/1395 train_time:142255ms step_avg:127.81ms
step:1124/1395 train_time:142388ms step_avg:127.82ms
step:1125/1395 train_time:142520ms step_avg:127.82ms
step:1125/1395 val_loss:3.3627 train_time:142653ms step_avg:127.94ms
step:1126/1395 train_time:142674ms step_avg:127.84ms
step:1127/1395 train_time:142796ms step_avg:127.84ms
step:1128/1395 train_time:142928ms step_avg:127.84ms
step:1129/1395 train_time:143060ms step_avg:127.85ms
step:1130/1395 train_time:143192ms step_avg:127.85ms
step:1131/1395 train_time:143325ms step_avg:127.85ms
step:1132/1395 train_time:143457ms step_avg:127.86ms
step:1133/1395 train_time:143588ms step_avg:127.86ms
step:1134/1395 train_time:143724ms step_avg:127.87ms
step:1135/1395 train_time:143857ms step_avg:127.87ms
step:1136/1395 train_time:143993ms step_avg:127.88ms
step:1137/1395 train_time:144124ms step_avg:127.88ms
step:1138/1395 train_time:144258ms step_avg:127.89ms
step:1139/1395 train_time:144392ms step_avg:127.89ms
step:1140/1395 train_time:144527ms step_avg:127.90ms
step:1141/1395 train_time:144662ms step_avg:127.91ms
step:1142/1395 train_time:144797ms step_avg:127.91ms
step:1143/1395 train_time:144932ms step_avg:127.92ms
step:1144/1395 train_time:145065ms step_avg:127.92ms
step:1145/1395 train_time:145199ms step_avg:127.93ms
step:1146/1395 train_time:145334ms step_avg:127.93ms
step:1147/1395 train_time:145469ms step_avg:127.94ms
step:1148/1395 train_time:145604ms step_avg:127.95ms
step:1149/1395 train_time:145737ms step_avg:127.95ms
step:1150/1395 train_time:145870ms step_avg:127.96ms
step:1151/1395 train_time:146006ms step_avg:127.96ms
step:1152/1395 train_time:146140ms step_avg:127.97ms
step:1153/1395 train_time:146276ms step_avg:127.98ms
step:1154/1395 train_time:146410ms step_avg:127.98ms
step:1155/1395 train_time:146545ms step_avg:127.99ms
step:1156/1395 train_time:146683ms step_avg:128.00ms
step:1157/1395 train_time:146818ms step_avg:128.00ms
step:1158/1395 train_time:146951ms step_avg:128.01ms
step:1159/1395 train_time:147085ms step_avg:128.01ms
step:1160/1395 train_time:147219ms step_avg:128.02ms
step:1161/1395 train_time:147352ms step_avg:128.02ms
step:1162/1395 train_time:147488ms step_avg:128.03ms
step:1163/1395 train_time:147622ms step_avg:128.03ms
step:1164/1395 train_time:147756ms step_avg:128.04ms
step:1165/1395 train_time:147891ms step_avg:128.04ms
step:1166/1395 train_time:148025ms step_avg:128.05ms
step:1167/1395 train_time:148158ms step_avg:128.05ms
step:1168/1395 train_time:148292ms step_avg:128.06ms
step:1169/1395 train_time:148426ms step_avg:128.06ms
step:1170/1395 train_time:148560ms step_avg:128.07ms
step:1171/1395 train_time:148695ms step_avg:128.07ms
step:1172/1395 train_time:148830ms step_avg:128.08ms
step:1173/1395 train_time:148965ms step_avg:128.09ms
step:1174/1395 train_time:149103ms step_avg:128.10ms
step:1175/1395 train_time:149237ms step_avg:128.10ms
step:1176/1395 train_time:149372ms step_avg:128.11ms
step:1177/1395 train_time:149509ms step_avg:128.11ms
step:1178/1395 train_time:149644ms step_avg:128.12ms
step:1179/1395 train_time:149777ms step_avg:128.12ms
step:1180/1395 train_time:149913ms step_avg:128.13ms
step:1181/1395 train_time:150049ms step_avg:128.14ms
step:1182/1395 train_time:150182ms step_avg:128.14ms
step:1183/1395 train_time:150317ms step_avg:128.15ms
step:1184/1395 train_time:150451ms step_avg:128.15ms
step:1185/1395 train_time:150586ms step_avg:128.16ms
step:1186/1395 train_time:150720ms step_avg:128.16ms
step:1187/1395 train_time:150858ms step_avg:128.17ms
step:1188/1395 train_time:150993ms step_avg:128.18ms
step:1189/1395 train_time:151127ms step_avg:128.18ms
step:1190/1395 train_time:151261ms step_avg:128.19ms
step:1191/1395 train_time:151395ms step_avg:128.19ms
step:1192/1395 train_time:151531ms step_avg:128.20ms
step:1193/1395 train_time:151665ms step_avg:128.20ms
step:1194/1395 train_time:151799ms step_avg:128.21ms
step:1195/1395 train_time:151934ms step_avg:128.21ms
step:1196/1395 train_time:152067ms step_avg:128.22ms
step:1197/1395 train_time:152202ms step_avg:128.22ms
step:1198/1395 train_time:152338ms step_avg:128.23ms
step:1199/1395 train_time:152472ms step_avg:128.24ms
step:1200/1395 train_time:152607ms step_avg:128.24ms
step:1201/1395 train_time:152741ms step_avg:128.25ms
step:1202/1395 train_time:152880ms step_avg:128.26ms
step:1203/1395 train_time:153018ms step_avg:128.26ms
step:1204/1395 train_time:153152ms step_avg:128.27ms
step:1205/1395 train_time:153286ms step_avg:128.27ms
step:1206/1395 train_time:153421ms step_avg:128.28ms
step:1207/1395 train_time:153556ms step_avg:128.28ms
step:1208/1395 train_time:153690ms step_avg:128.29ms
step:1209/1395 train_time:153823ms step_avg:128.29ms
step:1210/1395 train_time:153960ms step_avg:128.30ms
step:1211/1395 train_time:154095ms step_avg:128.31ms
step:1212/1395 train_time:154228ms step_avg:128.31ms
step:1213/1395 train_time:154362ms step_avg:128.31ms
step:1214/1395 train_time:154497ms step_avg:128.32ms
step:1215/1395 train_time:154633ms step_avg:128.33ms
step:1216/1395 train_time:154765ms step_avg:128.33ms
step:1217/1395 train_time:154900ms step_avg:128.33ms
step:1218/1395 train_time:155033ms step_avg:128.34ms
step:1219/1395 train_time:155166ms step_avg:128.34ms
step:1220/1395 train_time:155300ms step_avg:128.35ms
step:1221/1395 train_time:155435ms step_avg:128.35ms
step:1222/1395 train_time:155570ms step_avg:128.36ms
step:1223/1395 train_time:155704ms step_avg:128.36ms
step:1224/1395 train_time:155839ms step_avg:128.37ms
step:1225/1395 train_time:155975ms step_avg:128.37ms
step:1226/1395 train_time:156109ms step_avg:128.38ms
step:1227/1395 train_time:156243ms step_avg:128.38ms
step:1228/1395 train_time:156377ms step_avg:128.39ms
step:1229/1395 train_time:156509ms step_avg:128.39ms
step:1230/1395 train_time:156645ms step_avg:128.40ms
step:1231/1395 train_time:156779ms step_avg:128.40ms
step:1232/1395 train_time:156916ms step_avg:128.41ms
step:1233/1395 train_time:157049ms step_avg:128.41ms
step:1234/1395 train_time:157183ms step_avg:128.42ms
step:1235/1395 train_time:157318ms step_avg:128.42ms
step:1236/1395 train_time:157452ms step_avg:128.43ms
step:1237/1395 train_time:157585ms step_avg:128.43ms
step:1238/1395 train_time:157722ms step_avg:128.44ms
step:1239/1395 train_time:157856ms step_avg:128.44ms
step:1240/1395 train_time:157991ms step_avg:128.45ms
step:1241/1395 train_time:158126ms step_avg:128.45ms
step:1242/1395 train_time:158260ms step_avg:128.46ms
step:1243/1395 train_time:158394ms step_avg:128.46ms
step:1244/1395 train_time:158529ms step_avg:128.47ms
step:1245/1395 train_time:158664ms step_avg:128.47ms
step:1246/1395 train_time:158799ms step_avg:128.48ms
step:1247/1395 train_time:158934ms step_avg:128.48ms
step:1248/1395 train_time:159067ms step_avg:128.49ms
step:1249/1395 train_time:159200ms step_avg:128.49ms
step:1250/1395 train_time:159334ms step_avg:128.50ms
step:1250/1395 val_loss:3.3158 train_time:159467ms step_avg:128.60ms
step:1251/1395 train_time:159488ms step_avg:128.52ms
step:1252/1395 train_time:159612ms step_avg:128.51ms
step:1253/1395 train_time:159746ms step_avg:128.52ms
step:1254/1395 train_time:159879ms step_avg:128.52ms
step:1255/1395 train_time:160019ms step_avg:128.53ms
step:1256/1395 train_time:160152ms step_avg:128.53ms
step:1257/1395 train_time:160286ms step_avg:128.54ms
step:1258/1395 train_time:160420ms step_avg:128.54ms
step:1259/1395 train_time:160557ms step_avg:128.55ms
step:1260/1395 train_time:160690ms step_avg:128.55ms
step:1261/1395 train_time:160823ms step_avg:128.56ms
step:1262/1395 train_time:160960ms step_avg:128.56ms
step:1263/1395 train_time:161094ms step_avg:128.57ms
step:1264/1395 train_time:161229ms step_avg:128.57ms
step:1265/1395 train_time:161363ms step_avg:128.58ms
step:1266/1395 train_time:161498ms step_avg:128.58ms
step:1267/1395 train_time:161631ms step_avg:128.58ms
step:1268/1395 train_time:161765ms step_avg:128.59ms
step:1269/1395 train_time:161901ms step_avg:128.60ms
step:1270/1395 train_time:162036ms step_avg:128.60ms
step:1271/1395 train_time:162170ms step_avg:128.60ms
step:1272/1395 train_time:162304ms step_avg:128.61ms
step:1273/1395 train_time:162438ms step_avg:128.61ms
step:1274/1395 train_time:162571ms step_avg:128.62ms
step:1275/1395 train_time:162707ms step_avg:128.62ms
step:1276/1395 train_time:162841ms step_avg:128.63ms
step:1277/1395 train_time:162975ms step_avg:128.63ms
step:1278/1395 train_time:163110ms step_avg:128.64ms
step:1279/1395 train_time:163244ms step_avg:128.64ms
step:1280/1395 train_time:163381ms step_avg:128.65ms
step:1281/1395 train_time:163515ms step_avg:128.65ms
step:1282/1395 train_time:163649ms step_avg:128.65ms
step:1283/1395 train_time:163783ms step_avg:128.66ms
step:1284/1395 train_time:163919ms step_avg:128.66ms
step:1285/1395 train_time:164052ms step_avg:128.67ms
step:1286/1395 train_time:164188ms step_avg:128.67ms
step:1287/1395 train_time:164321ms step_avg:128.68ms
step:1288/1395 train_time:164456ms step_avg:128.68ms
step:1289/1395 train_time:164593ms step_avg:128.69ms
step:1290/1395 train_time:164729ms step_avg:128.69ms
step:1291/1395 train_time:164864ms step_avg:128.70ms
step:1292/1395 train_time:164999ms step_avg:128.70ms
step:1293/1395 train_time:165135ms step_avg:128.71ms
step:1294/1395 train_time:165269ms step_avg:128.71ms
step:1295/1395 train_time:165403ms step_avg:128.72ms
step:1296/1395 train_time:165538ms step_avg:128.72ms
step:1297/1395 train_time:165674ms step_avg:128.73ms
step:1298/1395 train_time:165808ms step_avg:128.73ms
step:1299/1395 train_time:165941ms step_avg:128.74ms
step:1300/1395 train_time:166075ms step_avg:128.74ms
step:1301/1395 train_time:166210ms step_avg:128.74ms
step:1302/1395 train_time:166343ms step_avg:128.75ms
step:1303/1395 train_time:166479ms step_avg:128.75ms
step:1304/1395 train_time:166615ms step_avg:128.76ms
step:1305/1395 train_time:166749ms step_avg:128.76ms
step:1306/1395 train_time:166883ms step_avg:128.77ms
step:1307/1395 train_time:167017ms step_avg:128.77ms
step:1308/1395 train_time:167152ms step_avg:128.78ms
step:1309/1395 train_time:167287ms step_avg:128.78ms
step:1310/1395 train_time:167422ms step_avg:128.79ms
step:1311/1395 train_time:167556ms step_avg:128.79ms
step:1312/1395 train_time:167690ms step_avg:128.79ms
step:1313/1395 train_time:167824ms step_avg:128.80ms
step:1314/1395 train_time:167958ms step_avg:128.80ms
step:1315/1395 train_time:168093ms step_avg:128.81ms
step:1316/1395 train_time:168227ms step_avg:128.81ms
step:1317/1395 train_time:168362ms step_avg:128.82ms
step:1318/1395 train_time:168497ms step_avg:128.82ms
step:1319/1395 train_time:168634ms step_avg:128.83ms
step:1320/1395 train_time:168768ms step_avg:128.83ms
step:1321/1395 train_time:168903ms step_avg:128.84ms
step:1322/1395 train_time:169041ms step_avg:128.84ms
step:1323/1395 train_time:169175ms step_avg:128.85ms
step:1324/1395 train_time:169308ms step_avg:128.85ms
step:1325/1395 train_time:169445ms step_avg:128.86ms
step:1326/1395 train_time:169580ms step_avg:128.86ms
step:1327/1395 train_time:169714ms step_avg:128.86ms
step:1328/1395 train_time:169848ms step_avg:128.87ms
step:1329/1395 train_time:169987ms step_avg:128.88ms
step:1330/1395 train_time:170124ms step_avg:128.88ms
step:1331/1395 train_time:170262ms step_avg:128.89ms
step:1332/1395 train_time:170399ms step_avg:128.89ms
step:1333/1395 train_time:170533ms step_avg:128.90ms
step:1334/1395 train_time:170668ms step_avg:128.90ms
step:1335/1395 train_time:170801ms step_avg:128.91ms
step:1336/1395 train_time:170938ms step_avg:128.91ms
step:1337/1395 train_time:171071ms step_avg:128.92ms
step:1338/1395 train_time:171205ms step_avg:128.92ms
step:1339/1395 train_time:171339ms step_avg:128.92ms
step:1340/1395 train_time:171475ms step_avg:128.93ms
step:1341/1395 train_time:171608ms step_avg:128.93ms
step:1342/1395 train_time:171742ms step_avg:128.94ms
step:1343/1395 train_time:171877ms step_avg:128.94ms
step:1344/1395 train_time:172011ms step_avg:128.94ms
step:1345/1395 train_time:172145ms step_avg:128.95ms
step:1346/1395 train_time:172281ms step_avg:128.95ms
step:1347/1395 train_time:172418ms step_avg:128.96ms
step:1348/1395 train_time:172551ms step_avg:128.96ms
step:1349/1395 train_time:172687ms step_avg:128.97ms
step:1350/1395 train_time:172821ms step_avg:128.97ms
step:1351/1395 train_time:172956ms step_avg:128.98ms
step:1352/1395 train_time:173094ms step_avg:128.98ms
step:1353/1395 train_time:173231ms step_avg:128.99ms
step:1354/1395 train_time:173368ms step_avg:128.99ms
step:1355/1395 train_time:173502ms step_avg:129.00ms
step:1356/1395 train_time:173636ms step_avg:129.00ms
step:1357/1395 train_time:173771ms step_avg:129.01ms
step:1358/1395 train_time:173908ms step_avg:129.01ms
step:1359/1395 train_time:174042ms step_avg:129.02ms
step:1360/1395 train_time:174180ms step_avg:129.02ms
step:1361/1395 train_time:174314ms step_avg:129.03ms
step:1362/1395 train_time:174453ms step_avg:129.03ms
step:1363/1395 train_time:174590ms step_avg:129.04ms
step:1364/1395 train_time:174726ms step_avg:129.04ms
step:1365/1395 train_time:174859ms step_avg:129.05ms
step:1366/1395 train_time:174995ms step_avg:129.05ms
step:1367/1395 train_time:175131ms step_avg:129.06ms
step:1368/1395 train_time:175266ms step_avg:129.06ms
step:1369/1395 train_time:175405ms step_avg:129.07ms
step:1370/1395 train_time:175544ms step_avg:129.08ms
step:1371/1395 train_time:175680ms step_avg:129.08ms
step:1372/1395 train_time:175819ms step_avg:129.09ms
step:1373/1395 train_time:175954ms step_avg:129.09ms
step:1374/1395 train_time:176091ms step_avg:129.10ms
step:1375/1395 train_time:176225ms step_avg:129.10ms
step:1375/1395 val_loss:3.2816 train_time:176358ms step_avg:129.20ms
step:1376/1395 train_time:176379ms step_avg:129.12ms
step:1377/1395 train_time:176504ms step_avg:129.12ms
step:1378/1395 train_time:176639ms step_avg:129.12ms
step:1379/1395 train_time:176773ms step_avg:129.13ms
step:1380/1395 train_time:176910ms step_avg:129.13ms
step:1381/1395 train_time:177046ms step_avg:129.14ms
step:1382/1395 train_time:177182ms step_avg:129.14ms
step:1383/1395 train_time:177318ms step_avg:129.15ms
step:1384/1395 train_time:177456ms step_avg:129.15ms
step:1385/1395 train_time:177591ms step_avg:129.16ms
step:1386/1395 train_time:177725ms step_avg:129.16ms
step:1387/1395 train_time:177861ms step_avg:129.17ms
step:1388/1395 train_time:177997ms step_avg:129.17ms
step:1389/1395 train_time:178133ms step_avg:129.18ms
step:1390/1395 train_time:178268ms step_avg:129.18ms
step:1391/1395 train_time:178404ms step_avg:129.18ms
step:1392/1395 train_time:178541ms step_avg:129.19ms
step:1393/1395 train_time:178675ms step_avg:129.19ms
step:1394/1395 train_time:178810ms step_avg:129.20ms
step:1395/1395 train_time:178944ms step_avg:129.20ms
step:1395/1395 val_loss:3.2774 train_time:179079ms step_avg:129.30ms
peak memory allocated: 37653 MiB reserved: 39156 MiB
