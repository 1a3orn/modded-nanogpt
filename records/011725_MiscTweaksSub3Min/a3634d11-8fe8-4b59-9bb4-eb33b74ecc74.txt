import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True

# -----------------------------------------------------------------------------
# Custom operators

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

def lm_head_fp8(x: Tensor, w: Tensor) -> Tensor:
    _x = x.flatten(0, -2)
    out: Tensor = torch.ops.nanogpt.mm(_x, w, x_s=2.0, w_s=32.0, grad_s=2.0**29)[0]
    return out.reshape(*x.shape[:-1], -1)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev():
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int):
        super().__init__(in_features, out_features, bias=False)

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x):
        return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len=65536):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        # self.c_q = CastedLinear(dim, dim)
        # self.c_k = CastedLinear(dim, dim)
        # self.c_v = CastedLinear(dim, dim)
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std
        self.qkv_w = nn.Parameter(torch.empty(3, dim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads) # dim // num_heads = head_dim
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # Set attention scale such that the minimum attainable attention entropy
        # (but not necessary the attention entropy itself) is close to 0. By @leloykun
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        # q = self.c_q(x).view(B, T, self.num_heads, -1)
        # k = self.c_k(x).view(B, T, self.num_heads, -1)
        # v = self.c_v(x).view(B, T, self.num_heads, -1)
        # qkv_weight = torch.cat([self.c_q.weight, self.c_k.weight, self.c_v.weight], dim=0).type_as(x)
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3*self.num_heads, -1).chunk(3, dim=-2)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, model_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(model_dim, num_heads, layer_idx) if layer_idx != 7 else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, ve, x0, block_mask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, num_embeddings: int, embedding_dim: int):
        super().__init__()
        self.embed = nn.ModuleList([nn.Embedding(num_embeddings, embedding_dim) for _ in range(3)])

    def forward(self, input_seq) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2], None, None, None, None, None, None, ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual learning
        self.value_embeds = ValueEmbedding(vocab_size, model_dim)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128))
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        assert input_seq.ndim == 1
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        docs = (input_seq == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        def create_doc_swc_block_masks(sliding_window_num_blocks: Tensor):
            kv_idx = block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & document_bm
            full_bm  = causal_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            def build_bm(sw_num_blocks: Tensor) -> BlockMask:
                return BlockMask.from_kv_blocks(
                    torch.clamp_max(kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                    kv_indices,
                    torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                    full_kv_indices,
                    BLOCK_SIZE=BLOCK_SIZE,
                    mask_mod=document_causal,
                )
            return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

        # Long-short SWA block masks by @leloykun & @YouJiacheng
        long_bm, short_bm = create_doc_swc_block_masks(sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = lm_head_fp8(x, self.lm_head.weight) if self.training else self.lm_head(x)
        # @Grad62304977 added tanh softcapping, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use cycle(files) if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    batch_size = 8*64*1024 # batch size in tokens
    num_iterations = 1395 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 64*1024 # FlexAttention sequence length
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
             if console:
                 print(s)
             print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_loader = distributed_data_generator(args.train_files, args.batch_size, rank, world_size)

model = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim >= 2]
embed_params = [model.embed.weight, *model.value_embeds.parameters()]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), fused=True, eps=1e-10)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(it: int):
    t = 1 - it / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_bs = world_size * args.seq_len
        assert args.val_tokens % val_bs == 0
        val_steps = args.val_tokens // val_bs
        val_loader = distributed_data_generator(args.val_files, val_bs, rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION BEGIN -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB"
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250110+cu126 compiled for CUDA 12.6
Thu Jan 16 20:27:45 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   29C    P0             115W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   26C    P0             112W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   25C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   28C    P0             112W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   29C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   27C    P0             111W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   28C    P0             111W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1395 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1395 train_time:349380ms step_avg:nanms
step:2/1395 train_time:350296ms step_avg:nanms
step:3/1395 train_time:350418ms step_avg:nanms
step:4/1395 train_time:350538ms step_avg:nanms
step:5/1395 train_time:350659ms step_avg:nanms
step:6/1395 train_time:350779ms step_avg:nanms
step:7/1395 train_time:350900ms step_avg:nanms
step:8/1395 train_time:351021ms step_avg:nanms
step:9/1395 train_time:351142ms step_avg:nanms
step:10/1395 train_time:351264ms step_avg:nanms
step:11/1395 train_time:124ms step_avg:nanms
step:12/1395 train_time:245ms step_avg:nanms
step:13/1395 train_time:368ms step_avg:122.57ms
step:14/1395 train_time:489ms step_avg:122.33ms
step:15/1395 train_time:612ms step_avg:122.42ms
step:16/1395 train_time:733ms step_avg:122.23ms
step:17/1395 train_time:855ms step_avg:122.17ms
step:18/1395 train_time:977ms step_avg:122.11ms
step:19/1395 train_time:1100ms step_avg:122.22ms
step:20/1395 train_time:1222ms step_avg:122.20ms
step:21/1395 train_time:1344ms step_avg:122.23ms
step:22/1395 train_time:1464ms step_avg:122.00ms
step:23/1395 train_time:1585ms step_avg:121.91ms
step:24/1395 train_time:1707ms step_avg:121.91ms
step:25/1395 train_time:1829ms step_avg:121.92ms
step:26/1395 train_time:1952ms step_avg:122.01ms
step:27/1395 train_time:2074ms step_avg:122.00ms
step:28/1395 train_time:2196ms step_avg:121.98ms
step:29/1395 train_time:2318ms step_avg:122.02ms
step:30/1395 train_time:2441ms step_avg:122.06ms
step:31/1395 train_time:2563ms step_avg:122.04ms
step:32/1395 train_time:2684ms step_avg:122.01ms
step:33/1395 train_time:2806ms step_avg:122.01ms
step:34/1395 train_time:2928ms step_avg:122.02ms
step:35/1395 train_time:3050ms step_avg:121.99ms
step:36/1395 train_time:3171ms step_avg:121.95ms
step:37/1395 train_time:3293ms step_avg:121.96ms
step:38/1395 train_time:3415ms step_avg:121.97ms
step:39/1395 train_time:3537ms step_avg:121.96ms
step:40/1395 train_time:3659ms step_avg:121.95ms
step:41/1395 train_time:3780ms step_avg:121.93ms
step:42/1395 train_time:3902ms step_avg:121.93ms
step:43/1395 train_time:4023ms step_avg:121.92ms
step:44/1395 train_time:4145ms step_avg:121.91ms
step:45/1395 train_time:4266ms step_avg:121.90ms
step:46/1395 train_time:4388ms step_avg:121.90ms
step:47/1395 train_time:4510ms step_avg:121.88ms
step:48/1395 train_time:4632ms step_avg:121.89ms
step:49/1395 train_time:4754ms step_avg:121.90ms
step:50/1395 train_time:4877ms step_avg:121.94ms
step:51/1395 train_time:4999ms step_avg:121.94ms
step:52/1395 train_time:5121ms step_avg:121.92ms
step:53/1395 train_time:5244ms step_avg:121.96ms
step:54/1395 train_time:5366ms step_avg:121.95ms
step:55/1395 train_time:5486ms step_avg:121.92ms
step:56/1395 train_time:5608ms step_avg:121.91ms
step:57/1395 train_time:5730ms step_avg:121.91ms
step:58/1395 train_time:5851ms step_avg:121.91ms
step:59/1395 train_time:5974ms step_avg:121.91ms
step:60/1395 train_time:6096ms step_avg:121.91ms
step:61/1395 train_time:6218ms step_avg:121.91ms
step:62/1395 train_time:6340ms step_avg:121.93ms
step:63/1395 train_time:6463ms step_avg:121.93ms
step:64/1395 train_time:6584ms step_avg:121.92ms
step:65/1395 train_time:6705ms step_avg:121.92ms
step:66/1395 train_time:6826ms step_avg:121.90ms
step:67/1395 train_time:6953ms step_avg:121.99ms
step:68/1395 train_time:7071ms step_avg:121.91ms
step:69/1395 train_time:7193ms step_avg:121.91ms
step:70/1395 train_time:7317ms step_avg:121.95ms
step:71/1395 train_time:7440ms step_avg:121.97ms
step:72/1395 train_time:7562ms step_avg:121.97ms
step:73/1395 train_time:7684ms step_avg:121.96ms
step:74/1395 train_time:7805ms step_avg:121.95ms
step:75/1395 train_time:7926ms step_avg:121.94ms
step:76/1395 train_time:8047ms step_avg:121.92ms
step:77/1395 train_time:8168ms step_avg:121.92ms
step:78/1395 train_time:8292ms step_avg:121.94ms
step:79/1395 train_time:8414ms step_avg:121.94ms
step:80/1395 train_time:8536ms step_avg:121.94ms
step:81/1395 train_time:8657ms step_avg:121.94ms
step:82/1395 train_time:8779ms step_avg:121.94ms
step:83/1395 train_time:8901ms step_avg:121.93ms
step:84/1395 train_time:9022ms step_avg:121.92ms
step:85/1395 train_time:9144ms step_avg:121.92ms
step:86/1395 train_time:9265ms step_avg:121.91ms
step:87/1395 train_time:9387ms step_avg:121.90ms
step:88/1395 train_time:9510ms step_avg:121.93ms
step:89/1395 train_time:9633ms step_avg:121.94ms
step:90/1395 train_time:9756ms step_avg:121.94ms
step:91/1395 train_time:9878ms step_avg:121.95ms
step:92/1395 train_time:10000ms step_avg:121.95ms
step:93/1395 train_time:10121ms step_avg:121.94ms
step:94/1395 train_time:10244ms step_avg:121.96ms
step:95/1395 train_time:10366ms step_avg:121.96ms
step:96/1395 train_time:10487ms step_avg:121.94ms
step:97/1395 train_time:10609ms step_avg:121.94ms
step:98/1395 train_time:10730ms step_avg:121.94ms
step:99/1395 train_time:10852ms step_avg:121.94ms
step:100/1395 train_time:10974ms step_avg:121.93ms
step:101/1395 train_time:11096ms step_avg:121.94ms
step:102/1395 train_time:11219ms step_avg:121.94ms
step:103/1395 train_time:11340ms step_avg:121.94ms
step:104/1395 train_time:11461ms step_avg:121.93ms
step:105/1395 train_time:11584ms step_avg:121.93ms
step:106/1395 train_time:11706ms step_avg:121.94ms
step:107/1395 train_time:11828ms step_avg:121.94ms
step:108/1395 train_time:11950ms step_avg:121.94ms
step:109/1395 train_time:12073ms step_avg:121.95ms
step:110/1395 train_time:12196ms step_avg:121.96ms
step:111/1395 train_time:12318ms step_avg:121.96ms
step:112/1395 train_time:12441ms step_avg:121.97ms
step:113/1395 train_time:12563ms step_avg:121.97ms
step:114/1395 train_time:12686ms step_avg:121.98ms
step:115/1395 train_time:12810ms step_avg:122.00ms
step:116/1395 train_time:12933ms step_avg:122.01ms
step:117/1395 train_time:13056ms step_avg:122.02ms
step:118/1395 train_time:13179ms step_avg:122.03ms
step:119/1395 train_time:13301ms step_avg:122.03ms
step:120/1395 train_time:13422ms step_avg:122.02ms
step:121/1395 train_time:13546ms step_avg:122.03ms
step:122/1395 train_time:13668ms step_avg:122.03ms
step:123/1395 train_time:13791ms step_avg:122.04ms
step:124/1395 train_time:13914ms step_avg:122.05ms
step:125/1395 train_time:14037ms step_avg:122.06ms
step:125/1395 val_loss:4.3901 train_time:14159ms step_avg:123.12ms
step:126/1395 train_time:14183ms step_avg:122.27ms
step:127/1395 train_time:14295ms step_avg:122.18ms
step:128/1395 train_time:14421ms step_avg:122.21ms
step:129/1395 train_time:14544ms step_avg:122.22ms
step:130/1395 train_time:14666ms step_avg:122.22ms
step:131/1395 train_time:14787ms step_avg:122.21ms
step:132/1395 train_time:14909ms step_avg:122.20ms
step:133/1395 train_time:15031ms step_avg:122.20ms
step:134/1395 train_time:15153ms step_avg:122.20ms
step:135/1395 train_time:15277ms step_avg:122.22ms
step:136/1395 train_time:15399ms step_avg:122.21ms
step:137/1395 train_time:15522ms step_avg:122.22ms
step:138/1395 train_time:15645ms step_avg:122.23ms
step:139/1395 train_time:15768ms step_avg:122.24ms
step:140/1395 train_time:15891ms step_avg:122.24ms
step:141/1395 train_time:16014ms step_avg:122.24ms
step:142/1395 train_time:16136ms step_avg:122.24ms
step:143/1395 train_time:16258ms step_avg:122.24ms
step:144/1395 train_time:16380ms step_avg:122.24ms
step:145/1395 train_time:16503ms step_avg:122.25ms
step:146/1395 train_time:16626ms step_avg:122.25ms
step:147/1395 train_time:16749ms step_avg:122.26ms
step:148/1395 train_time:16873ms step_avg:122.27ms
step:149/1395 train_time:16995ms step_avg:122.27ms
step:150/1395 train_time:17117ms step_avg:122.26ms
step:151/1395 train_time:17239ms step_avg:122.26ms
step:152/1395 train_time:17362ms step_avg:122.26ms
step:153/1395 train_time:17485ms step_avg:122.27ms
step:154/1395 train_time:17608ms step_avg:122.28ms
step:155/1395 train_time:17731ms step_avg:122.28ms
step:156/1395 train_time:17855ms step_avg:122.29ms
step:157/1395 train_time:17978ms step_avg:122.30ms
step:158/1395 train_time:18100ms step_avg:122.30ms
step:159/1395 train_time:18223ms step_avg:122.30ms
step:160/1395 train_time:18348ms step_avg:122.32ms
step:161/1395 train_time:18467ms step_avg:122.30ms
step:162/1395 train_time:18590ms step_avg:122.30ms
step:163/1395 train_time:18712ms step_avg:122.30ms
step:164/1395 train_time:18836ms step_avg:122.31ms
step:165/1395 train_time:18959ms step_avg:122.31ms
step:166/1395 train_time:19081ms step_avg:122.31ms
step:167/1395 train_time:19205ms step_avg:122.32ms
step:168/1395 train_time:19329ms step_avg:122.34ms
step:169/1395 train_time:19452ms step_avg:122.34ms
step:170/1395 train_time:19575ms step_avg:122.34ms
step:171/1395 train_time:19696ms step_avg:122.34ms
step:172/1395 train_time:19819ms step_avg:122.34ms
step:173/1395 train_time:19941ms step_avg:122.34ms
step:174/1395 train_time:20064ms step_avg:122.34ms
step:175/1395 train_time:20186ms step_avg:122.34ms
step:176/1395 train_time:20310ms step_avg:122.35ms
step:177/1395 train_time:20433ms step_avg:122.35ms
step:178/1395 train_time:20556ms step_avg:122.36ms
step:179/1395 train_time:20677ms step_avg:122.35ms
step:180/1395 train_time:20800ms step_avg:122.35ms
step:181/1395 train_time:20923ms step_avg:122.36ms
step:182/1395 train_time:21045ms step_avg:122.35ms
step:183/1395 train_time:21168ms step_avg:122.36ms
step:184/1395 train_time:21291ms step_avg:122.36ms
step:185/1395 train_time:21413ms step_avg:122.36ms
step:186/1395 train_time:21536ms step_avg:122.36ms
step:187/1395 train_time:21658ms step_avg:122.36ms
step:188/1395 train_time:21782ms step_avg:122.37ms
step:189/1395 train_time:21905ms step_avg:122.37ms
step:190/1395 train_time:22027ms step_avg:122.37ms
step:191/1395 train_time:22150ms step_avg:122.38ms
step:192/1395 train_time:22273ms step_avg:122.38ms
step:193/1395 train_time:22396ms step_avg:122.38ms
step:194/1395 train_time:22519ms step_avg:122.38ms
step:195/1395 train_time:22641ms step_avg:122.38ms
step:196/1395 train_time:22764ms step_avg:122.39ms
step:197/1395 train_time:22886ms step_avg:122.38ms
step:198/1395 train_time:23009ms step_avg:122.39ms
step:199/1395 train_time:23131ms step_avg:122.39ms
step:200/1395 train_time:23256ms step_avg:122.40ms
step:201/1395 train_time:23377ms step_avg:122.39ms
step:202/1395 train_time:23499ms step_avg:122.39ms
step:203/1395 train_time:23622ms step_avg:122.39ms
step:204/1395 train_time:23745ms step_avg:122.40ms
step:205/1395 train_time:23868ms step_avg:122.40ms
step:206/1395 train_time:23991ms step_avg:122.40ms
step:207/1395 train_time:24113ms step_avg:122.40ms
step:208/1395 train_time:24236ms step_avg:122.40ms
step:209/1395 train_time:24359ms step_avg:122.41ms
step:210/1395 train_time:24483ms step_avg:122.41ms
step:211/1395 train_time:24605ms step_avg:122.41ms
step:212/1395 train_time:24730ms step_avg:122.43ms
step:213/1395 train_time:24853ms step_avg:122.43ms
step:214/1395 train_time:24977ms step_avg:122.43ms
step:215/1395 train_time:25099ms step_avg:122.43ms
step:216/1395 train_time:25222ms step_avg:122.44ms
step:217/1395 train_time:25346ms step_avg:122.45ms
step:218/1395 train_time:25469ms step_avg:122.45ms
step:219/1395 train_time:25592ms step_avg:122.45ms
step:220/1395 train_time:25716ms step_avg:122.46ms
step:221/1395 train_time:25839ms step_avg:122.46ms
step:222/1395 train_time:25962ms step_avg:122.46ms
step:223/1395 train_time:26084ms step_avg:122.46ms
step:224/1395 train_time:26208ms step_avg:122.47ms
step:225/1395 train_time:26332ms step_avg:122.47ms
step:226/1395 train_time:26455ms step_avg:122.48ms
step:227/1395 train_time:26578ms step_avg:122.48ms
step:228/1395 train_time:26701ms step_avg:122.48ms
step:229/1395 train_time:26826ms step_avg:122.49ms
step:230/1395 train_time:26950ms step_avg:122.50ms
step:231/1395 train_time:27073ms step_avg:122.50ms
step:232/1395 train_time:27196ms step_avg:122.51ms
step:233/1395 train_time:27320ms step_avg:122.51ms
step:234/1395 train_time:27444ms step_avg:122.52ms
step:235/1395 train_time:27568ms step_avg:122.52ms
step:236/1395 train_time:27690ms step_avg:122.52ms
step:237/1395 train_time:27814ms step_avg:122.53ms
step:238/1395 train_time:27938ms step_avg:122.54ms
step:239/1395 train_time:28061ms step_avg:122.54ms
step:240/1395 train_time:28185ms step_avg:122.54ms
step:241/1395 train_time:28309ms step_avg:122.55ms
step:242/1395 train_time:28433ms step_avg:122.55ms
step:243/1395 train_time:28556ms step_avg:122.56ms
step:244/1395 train_time:28680ms step_avg:122.56ms
step:245/1395 train_time:28804ms step_avg:122.57ms
step:246/1395 train_time:28928ms step_avg:122.58ms
step:247/1395 train_time:29053ms step_avg:122.59ms
step:248/1395 train_time:29176ms step_avg:122.59ms
step:249/1395 train_time:29300ms step_avg:122.59ms
step:250/1395 train_time:29423ms step_avg:122.60ms
step:250/1395 val_loss:3.9854 train_time:29545ms step_avg:123.10ms
step:251/1395 train_time:29567ms step_avg:122.68ms
step:252/1395 train_time:29686ms step_avg:122.67ms
step:253/1395 train_time:29812ms step_avg:122.68ms
step:254/1395 train_time:29934ms step_avg:122.68ms
step:255/1395 train_time:30058ms step_avg:122.68ms
step:256/1395 train_time:30180ms step_avg:122.68ms
step:257/1395 train_time:30303ms step_avg:122.68ms
step:258/1395 train_time:30426ms step_avg:122.68ms
step:259/1395 train_time:30549ms step_avg:122.69ms
step:260/1395 train_time:30672ms step_avg:122.69ms
step:261/1395 train_time:30797ms step_avg:122.70ms
step:262/1395 train_time:30921ms step_avg:122.70ms
step:263/1395 train_time:31044ms step_avg:122.70ms
step:264/1395 train_time:31168ms step_avg:122.71ms
step:265/1395 train_time:31290ms step_avg:122.71ms
step:266/1395 train_time:31414ms step_avg:122.71ms
step:267/1395 train_time:31538ms step_avg:122.72ms
step:268/1395 train_time:31661ms step_avg:122.72ms
step:269/1395 train_time:31784ms step_avg:122.72ms
step:270/1395 train_time:31908ms step_avg:122.72ms
step:271/1395 train_time:32031ms step_avg:122.72ms
step:272/1395 train_time:32154ms step_avg:122.72ms
step:273/1395 train_time:32277ms step_avg:122.73ms
step:274/1395 train_time:32401ms step_avg:122.73ms
step:275/1395 train_time:32525ms step_avg:122.73ms
step:276/1395 train_time:32648ms step_avg:122.74ms
step:277/1395 train_time:32772ms step_avg:122.74ms
step:278/1395 train_time:32896ms step_avg:122.75ms
step:279/1395 train_time:33020ms step_avg:122.75ms
step:280/1395 train_time:33142ms step_avg:122.75ms
step:281/1395 train_time:33264ms step_avg:122.75ms
step:282/1395 train_time:33388ms step_avg:122.75ms
step:283/1395 train_time:33511ms step_avg:122.75ms
step:284/1395 train_time:33634ms step_avg:122.75ms
step:285/1395 train_time:33757ms step_avg:122.75ms
step:286/1395 train_time:33880ms step_avg:122.75ms
step:287/1395 train_time:34003ms step_avg:122.75ms
step:288/1395 train_time:34125ms step_avg:122.75ms
step:289/1395 train_time:34248ms step_avg:122.75ms
step:290/1395 train_time:34372ms step_avg:122.76ms
step:291/1395 train_time:34496ms step_avg:122.76ms
step:292/1395 train_time:34619ms step_avg:122.76ms
step:293/1395 train_time:34743ms step_avg:122.77ms
step:294/1395 train_time:34866ms step_avg:122.77ms
step:295/1395 train_time:34989ms step_avg:122.77ms
step:296/1395 train_time:35113ms step_avg:122.77ms
step:297/1395 train_time:35237ms step_avg:122.78ms
step:298/1395 train_time:35360ms step_avg:122.78ms
step:299/1395 train_time:35484ms step_avg:122.78ms
step:300/1395 train_time:35607ms step_avg:122.78ms
step:301/1395 train_time:35730ms step_avg:122.78ms
step:302/1395 train_time:35853ms step_avg:122.79ms
step:303/1395 train_time:35976ms step_avg:122.79ms
step:304/1395 train_time:36101ms step_avg:122.79ms
step:305/1395 train_time:36224ms step_avg:122.79ms
step:306/1395 train_time:36348ms step_avg:122.80ms
step:307/1395 train_time:36470ms step_avg:122.80ms
step:308/1395 train_time:36594ms step_avg:122.80ms
step:309/1395 train_time:36718ms step_avg:122.80ms
step:310/1395 train_time:36841ms step_avg:122.80ms
step:311/1395 train_time:36963ms step_avg:122.80ms
step:312/1395 train_time:37086ms step_avg:122.80ms
step:313/1395 train_time:37212ms step_avg:122.81ms
step:314/1395 train_time:37339ms step_avg:122.82ms
step:315/1395 train_time:37464ms step_avg:122.83ms
step:316/1395 train_time:37591ms step_avg:122.85ms
step:317/1395 train_time:37719ms step_avg:122.86ms
step:318/1395 train_time:37844ms step_avg:122.87ms
step:319/1395 train_time:37970ms step_avg:122.88ms
step:320/1395 train_time:38096ms step_avg:122.89ms
step:321/1395 train_time:38222ms step_avg:122.90ms
step:322/1395 train_time:38348ms step_avg:122.91ms
step:323/1395 train_time:38473ms step_avg:122.92ms
step:324/1395 train_time:38601ms step_avg:122.93ms
step:325/1395 train_time:38726ms step_avg:122.94ms
step:326/1395 train_time:38852ms step_avg:122.95ms
step:327/1395 train_time:38978ms step_avg:122.96ms
step:328/1395 train_time:39104ms step_avg:122.97ms
step:329/1395 train_time:39229ms step_avg:122.97ms
step:330/1395 train_time:39355ms step_avg:122.98ms
step:331/1395 train_time:39482ms step_avg:123.00ms
step:332/1395 train_time:39607ms step_avg:123.00ms
step:333/1395 train_time:39733ms step_avg:123.01ms
step:334/1395 train_time:39860ms step_avg:123.02ms
step:335/1395 train_time:39985ms step_avg:123.03ms
step:336/1395 train_time:40112ms step_avg:123.04ms
step:337/1395 train_time:40238ms step_avg:123.05ms
step:338/1395 train_time:40365ms step_avg:123.06ms
step:339/1395 train_time:40492ms step_avg:123.07ms
step:340/1395 train_time:40617ms step_avg:123.08ms
step:341/1395 train_time:40743ms step_avg:123.09ms
step:342/1395 train_time:40869ms step_avg:123.10ms
step:343/1395 train_time:40995ms step_avg:123.11ms
step:344/1395 train_time:41121ms step_avg:123.12ms
step:345/1395 train_time:41246ms step_avg:123.12ms
step:346/1395 train_time:41372ms step_avg:123.13ms
step:347/1395 train_time:41498ms step_avg:123.14ms
step:348/1395 train_time:41624ms step_avg:123.15ms
step:349/1395 train_time:41750ms step_avg:123.16ms
step:350/1395 train_time:41877ms step_avg:123.17ms
step:351/1395 train_time:42003ms step_avg:123.18ms
step:352/1395 train_time:42129ms step_avg:123.18ms
step:353/1395 train_time:42256ms step_avg:123.19ms
step:354/1395 train_time:42382ms step_avg:123.20ms
step:355/1395 train_time:42508ms step_avg:123.21ms
step:356/1395 train_time:42633ms step_avg:123.22ms
step:357/1395 train_time:42758ms step_avg:123.22ms
step:358/1395 train_time:42884ms step_avg:123.23ms
step:359/1395 train_time:43009ms step_avg:123.24ms
step:360/1395 train_time:43135ms step_avg:123.24ms
step:361/1395 train_time:43262ms step_avg:123.25ms
step:362/1395 train_time:43388ms step_avg:123.26ms
step:363/1395 train_time:43514ms step_avg:123.27ms
step:364/1395 train_time:43640ms step_avg:123.28ms
step:365/1395 train_time:43766ms step_avg:123.28ms
step:366/1395 train_time:43891ms step_avg:123.29ms
step:367/1395 train_time:44018ms step_avg:123.30ms
step:368/1395 train_time:44143ms step_avg:123.30ms
step:369/1395 train_time:44270ms step_avg:123.31ms
step:370/1395 train_time:44394ms step_avg:123.32ms
step:371/1395 train_time:44520ms step_avg:123.32ms
step:372/1395 train_time:44645ms step_avg:123.33ms
step:373/1395 train_time:44771ms step_avg:123.34ms
step:374/1395 train_time:44896ms step_avg:123.34ms
step:375/1395 train_time:45022ms step_avg:123.35ms
step:375/1395 val_loss:3.7867 train_time:45147ms step_avg:123.69ms
step:376/1395 train_time:45169ms step_avg:123.41ms
step:377/1395 train_time:45290ms step_avg:123.40ms
step:378/1395 train_time:45418ms step_avg:123.42ms
step:379/1395 train_time:45543ms step_avg:123.42ms
step:380/1395 train_time:45668ms step_avg:123.43ms
step:381/1395 train_time:45794ms step_avg:123.43ms
step:382/1395 train_time:45919ms step_avg:123.44ms
step:383/1395 train_time:46044ms step_avg:123.44ms
step:384/1395 train_time:46169ms step_avg:123.45ms
step:385/1395 train_time:46296ms step_avg:123.46ms
step:386/1395 train_time:46423ms step_avg:123.46ms
step:387/1395 train_time:46549ms step_avg:123.47ms
step:388/1395 train_time:46676ms step_avg:123.48ms
step:389/1395 train_time:46801ms step_avg:123.49ms
step:390/1395 train_time:46926ms step_avg:123.49ms
step:391/1395 train_time:47055ms step_avg:123.50ms
step:392/1395 train_time:47177ms step_avg:123.50ms
step:393/1395 train_time:47304ms step_avg:123.51ms
step:394/1395 train_time:47428ms step_avg:123.51ms
step:395/1395 train_time:47555ms step_avg:123.52ms
step:396/1395 train_time:47681ms step_avg:123.52ms
step:397/1395 train_time:47806ms step_avg:123.53ms
step:398/1395 train_time:47932ms step_avg:123.53ms
step:399/1395 train_time:48058ms step_avg:123.54ms
step:400/1395 train_time:48183ms step_avg:123.55ms
step:401/1395 train_time:48308ms step_avg:123.55ms
step:402/1395 train_time:48435ms step_avg:123.56ms
step:403/1395 train_time:48560ms step_avg:123.56ms
step:404/1395 train_time:48686ms step_avg:123.57ms
step:405/1395 train_time:48810ms step_avg:123.57ms
step:406/1395 train_time:48937ms step_avg:123.58ms
step:407/1395 train_time:49063ms step_avg:123.58ms
step:408/1395 train_time:49189ms step_avg:123.59ms
step:409/1395 train_time:49316ms step_avg:123.60ms
step:410/1395 train_time:49443ms step_avg:123.61ms
step:411/1395 train_time:49568ms step_avg:123.61ms
step:412/1395 train_time:49694ms step_avg:123.62ms
step:413/1395 train_time:49820ms step_avg:123.62ms
step:414/1395 train_time:49945ms step_avg:123.63ms
step:415/1395 train_time:50070ms step_avg:123.63ms
step:416/1395 train_time:50196ms step_avg:123.63ms
step:417/1395 train_time:50322ms step_avg:123.64ms
step:418/1395 train_time:50448ms step_avg:123.65ms
step:419/1395 train_time:50574ms step_avg:123.65ms
step:420/1395 train_time:50701ms step_avg:123.66ms
step:421/1395 train_time:50827ms step_avg:123.67ms
step:422/1395 train_time:50952ms step_avg:123.67ms
step:423/1395 train_time:51078ms step_avg:123.68ms
step:424/1395 train_time:51204ms step_avg:123.68ms
step:425/1395 train_time:51330ms step_avg:123.69ms
step:426/1395 train_time:51458ms step_avg:123.70ms
step:427/1395 train_time:51583ms step_avg:123.70ms
step:428/1395 train_time:51709ms step_avg:123.71ms
step:429/1395 train_time:51837ms step_avg:123.72ms
step:430/1395 train_time:51964ms step_avg:123.72ms
step:431/1395 train_time:52090ms step_avg:123.73ms
step:432/1395 train_time:52215ms step_avg:123.73ms
step:433/1395 train_time:52341ms step_avg:123.74ms
step:434/1395 train_time:52467ms step_avg:123.74ms
step:435/1395 train_time:52594ms step_avg:123.75ms
step:436/1395 train_time:52720ms step_avg:123.75ms
step:437/1395 train_time:52847ms step_avg:123.76ms
step:438/1395 train_time:52975ms step_avg:123.77ms
step:439/1395 train_time:53101ms step_avg:123.78ms
step:440/1395 train_time:53227ms step_avg:123.78ms
step:441/1395 train_time:53353ms step_avg:123.79ms
step:442/1395 train_time:53479ms step_avg:123.79ms
step:443/1395 train_time:53605ms step_avg:123.80ms
step:444/1395 train_time:53731ms step_avg:123.80ms
step:445/1395 train_time:53857ms step_avg:123.81ms
step:446/1395 train_time:53984ms step_avg:123.82ms
step:447/1395 train_time:54112ms step_avg:123.83ms
step:448/1395 train_time:54238ms step_avg:123.83ms
step:449/1395 train_time:54363ms step_avg:123.83ms
step:450/1395 train_time:54489ms step_avg:123.84ms
step:451/1395 train_time:54614ms step_avg:123.84ms
step:452/1395 train_time:54740ms step_avg:123.85ms
step:453/1395 train_time:54866ms step_avg:123.85ms
step:454/1395 train_time:54994ms step_avg:123.86ms
step:455/1395 train_time:55119ms step_avg:123.86ms
step:456/1395 train_time:55246ms step_avg:123.87ms
step:457/1395 train_time:55373ms step_avg:123.88ms
step:458/1395 train_time:55498ms step_avg:123.88ms
step:459/1395 train_time:55625ms step_avg:123.89ms
step:460/1395 train_time:55752ms step_avg:123.89ms
step:461/1395 train_time:55877ms step_avg:123.90ms
step:462/1395 train_time:56005ms step_avg:123.90ms
step:463/1395 train_time:56130ms step_avg:123.91ms
step:464/1395 train_time:56256ms step_avg:123.91ms
step:465/1395 train_time:56382ms step_avg:123.92ms
step:466/1395 train_time:56508ms step_avg:123.92ms
step:467/1395 train_time:56634ms step_avg:123.93ms
step:468/1395 train_time:56761ms step_avg:123.93ms
step:469/1395 train_time:56887ms step_avg:123.94ms
step:470/1395 train_time:57013ms step_avg:123.94ms
step:471/1395 train_time:57139ms step_avg:123.95ms
step:472/1395 train_time:57265ms step_avg:123.95ms
step:473/1395 train_time:57391ms step_avg:123.96ms
step:474/1395 train_time:57517ms step_avg:123.96ms
step:475/1395 train_time:57642ms step_avg:123.96ms
step:476/1395 train_time:57768ms step_avg:123.97ms
step:477/1395 train_time:57895ms step_avg:123.97ms
step:478/1395 train_time:58021ms step_avg:123.98ms
step:479/1395 train_time:58147ms step_avg:123.98ms
step:480/1395 train_time:58274ms step_avg:123.99ms
step:481/1395 train_time:58400ms step_avg:123.99ms
step:482/1395 train_time:58527ms step_avg:124.00ms
step:483/1395 train_time:58654ms step_avg:124.00ms
step:484/1395 train_time:58779ms step_avg:124.01ms
step:485/1395 train_time:58905ms step_avg:124.01ms
step:486/1395 train_time:59031ms step_avg:124.01ms
step:487/1395 train_time:59158ms step_avg:124.02ms
step:488/1395 train_time:59284ms step_avg:124.02ms
step:489/1395 train_time:59410ms step_avg:124.03ms
step:490/1395 train_time:59537ms step_avg:124.04ms
step:491/1395 train_time:59664ms step_avg:124.04ms
step:492/1395 train_time:59789ms step_avg:124.04ms
step:493/1395 train_time:59915ms step_avg:124.05ms
step:494/1395 train_time:60041ms step_avg:124.05ms
step:495/1395 train_time:60167ms step_avg:124.06ms
step:496/1395 train_time:60295ms step_avg:124.06ms
step:497/1395 train_time:60420ms step_avg:124.07ms
step:498/1395 train_time:60547ms step_avg:124.07ms
step:499/1395 train_time:60674ms step_avg:124.08ms
step:500/1395 train_time:60799ms step_avg:124.08ms
step:500/1395 val_loss:3.6685 train_time:60924ms step_avg:124.33ms
step:501/1395 train_time:60952ms step_avg:124.14ms
step:502/1395 train_time:61067ms step_avg:124.12ms
step:503/1395 train_time:61197ms step_avg:124.13ms
step:504/1395 train_time:61324ms step_avg:124.14ms
step:505/1395 train_time:61448ms step_avg:124.14ms
step:506/1395 train_time:61574ms step_avg:124.14ms
step:507/1395 train_time:61700ms step_avg:124.14ms
step:508/1395 train_time:61826ms step_avg:124.15ms
step:509/1395 train_time:61952ms step_avg:124.15ms
step:510/1395 train_time:62078ms step_avg:124.16ms
step:511/1395 train_time:62206ms step_avg:124.16ms
step:512/1395 train_time:62334ms step_avg:124.17ms
step:513/1395 train_time:62459ms step_avg:124.17ms
step:514/1395 train_time:62586ms step_avg:124.18ms
step:515/1395 train_time:62711ms step_avg:124.18ms
step:516/1395 train_time:62837ms step_avg:124.18ms
step:517/1395 train_time:62964ms step_avg:124.19ms
step:518/1395 train_time:63089ms step_avg:124.19ms
step:519/1395 train_time:63217ms step_avg:124.20ms
step:520/1395 train_time:63346ms step_avg:124.21ms
step:521/1395 train_time:63474ms step_avg:124.22ms
step:522/1395 train_time:63602ms step_avg:124.22ms
step:523/1395 train_time:63730ms step_avg:124.23ms
step:524/1395 train_time:63858ms step_avg:124.24ms
step:525/1395 train_time:63986ms step_avg:124.24ms
step:526/1395 train_time:64115ms step_avg:124.25ms
step:527/1395 train_time:64243ms step_avg:124.26ms
step:528/1395 train_time:64371ms step_avg:124.27ms
step:529/1395 train_time:64500ms step_avg:124.28ms
step:530/1395 train_time:64628ms step_avg:124.29ms
step:531/1395 train_time:64756ms step_avg:124.29ms
step:532/1395 train_time:64884ms step_avg:124.30ms
step:533/1395 train_time:65013ms step_avg:124.31ms
step:534/1395 train_time:65141ms step_avg:124.32ms
step:535/1395 train_time:65270ms step_avg:124.32ms
step:536/1395 train_time:65400ms step_avg:124.33ms
step:537/1395 train_time:65528ms step_avg:124.34ms
step:538/1395 train_time:65658ms step_avg:124.35ms
step:539/1395 train_time:65786ms step_avg:124.36ms
step:540/1395 train_time:65915ms step_avg:124.37ms
step:541/1395 train_time:66043ms step_avg:124.38ms
step:542/1395 train_time:66171ms step_avg:124.38ms
step:543/1395 train_time:66300ms step_avg:124.39ms
step:544/1395 train_time:66429ms step_avg:124.40ms
step:545/1395 train_time:66558ms step_avg:124.41ms
step:546/1395 train_time:66686ms step_avg:124.41ms
step:547/1395 train_time:66815ms step_avg:124.42ms
step:548/1395 train_time:66944ms step_avg:124.43ms
step:549/1395 train_time:67072ms step_avg:124.44ms
step:550/1395 train_time:67200ms step_avg:124.45ms
step:551/1395 train_time:67328ms step_avg:124.45ms
step:552/1395 train_time:67456ms step_avg:124.46ms
step:553/1395 train_time:67586ms step_avg:124.47ms
step:554/1395 train_time:67714ms step_avg:124.48ms
step:555/1395 train_time:67843ms step_avg:124.48ms
step:556/1395 train_time:67973ms step_avg:124.49ms
step:557/1395 train_time:68101ms step_avg:124.50ms
step:558/1395 train_time:68229ms step_avg:124.51ms
step:559/1395 train_time:68357ms step_avg:124.51ms
step:560/1395 train_time:68485ms step_avg:124.52ms
step:561/1395 train_time:68613ms step_avg:124.53ms
step:562/1395 train_time:68742ms step_avg:124.53ms
step:563/1395 train_time:68869ms step_avg:124.54ms
step:564/1395 train_time:68998ms step_avg:124.55ms
step:565/1395 train_time:69127ms step_avg:124.55ms
step:566/1395 train_time:69256ms step_avg:124.56ms
step:567/1395 train_time:69384ms step_avg:124.57ms
step:568/1395 train_time:69512ms step_avg:124.57ms
step:569/1395 train_time:69640ms step_avg:124.58ms
step:570/1395 train_time:69768ms step_avg:124.59ms
step:571/1395 train_time:69896ms step_avg:124.59ms
step:572/1395 train_time:70024ms step_avg:124.60ms
step:573/1395 train_time:70151ms step_avg:124.60ms
step:574/1395 train_time:70280ms step_avg:124.61ms
step:575/1395 train_time:70408ms step_avg:124.62ms
step:576/1395 train_time:70537ms step_avg:124.62ms
step:577/1395 train_time:70666ms step_avg:124.63ms
step:578/1395 train_time:70793ms step_avg:124.64ms
step:579/1395 train_time:70922ms step_avg:124.64ms
step:580/1395 train_time:71051ms step_avg:124.65ms
step:581/1395 train_time:71180ms step_avg:124.66ms
step:582/1395 train_time:71308ms step_avg:124.66ms
step:583/1395 train_time:71437ms step_avg:124.67ms
step:584/1395 train_time:71565ms step_avg:124.68ms
step:585/1395 train_time:71694ms step_avg:124.68ms
step:586/1395 train_time:71822ms step_avg:124.69ms
step:587/1395 train_time:71951ms step_avg:124.70ms
step:588/1395 train_time:72079ms step_avg:124.70ms
step:589/1395 train_time:72208ms step_avg:124.71ms
step:590/1395 train_time:72337ms step_avg:124.72ms
step:591/1395 train_time:72465ms step_avg:124.72ms
step:592/1395 train_time:72593ms step_avg:124.73ms
step:593/1395 train_time:72720ms step_avg:124.73ms
step:594/1395 train_time:72848ms step_avg:124.74ms
step:595/1395 train_time:72977ms step_avg:124.75ms
step:596/1395 train_time:73105ms step_avg:124.75ms
step:597/1395 train_time:73233ms step_avg:124.76ms
step:598/1395 train_time:73361ms step_avg:124.76ms
step:599/1395 train_time:73489ms step_avg:124.77ms
step:600/1395 train_time:73617ms step_avg:124.78ms
step:601/1395 train_time:73746ms step_avg:124.78ms
step:602/1395 train_time:73875ms step_avg:124.79ms
step:603/1395 train_time:74003ms step_avg:124.79ms
step:604/1395 train_time:74130ms step_avg:124.80ms
step:605/1395 train_time:74260ms step_avg:124.81ms
step:606/1395 train_time:74388ms step_avg:124.81ms
step:607/1395 train_time:74516ms step_avg:124.82ms
step:608/1395 train_time:74644ms step_avg:124.82ms
step:609/1395 train_time:74772ms step_avg:124.83ms
step:610/1395 train_time:74900ms step_avg:124.83ms
step:611/1395 train_time:75028ms step_avg:124.84ms
step:612/1395 train_time:75157ms step_avg:124.84ms
step:613/1395 train_time:75285ms step_avg:124.85ms
step:614/1395 train_time:75414ms step_avg:124.86ms
step:615/1395 train_time:75543ms step_avg:124.86ms
step:616/1395 train_time:75670ms step_avg:124.87ms
step:617/1395 train_time:75798ms step_avg:124.87ms
step:618/1395 train_time:75926ms step_avg:124.88ms
step:619/1395 train_time:76055ms step_avg:124.88ms
step:620/1395 train_time:76183ms step_avg:124.89ms
step:621/1395 train_time:76311ms step_avg:124.89ms
step:622/1395 train_time:76439ms step_avg:124.90ms
step:623/1395 train_time:76568ms step_avg:124.91ms
step:624/1395 train_time:76697ms step_avg:124.91ms
step:625/1395 train_time:76825ms step_avg:124.92ms
step:625/1395 val_loss:3.5854 train_time:76953ms step_avg:125.13ms
step:626/1395 train_time:76975ms step_avg:124.96ms
step:627/1395 train_time:77098ms step_avg:124.96ms
step:628/1395 train_time:77230ms step_avg:124.97ms
step:629/1395 train_time:77358ms step_avg:124.97ms
step:630/1395 train_time:77486ms step_avg:124.98ms
step:631/1395 train_time:77615ms step_avg:124.98ms
step:632/1395 train_time:77743ms step_avg:124.99ms
step:633/1395 train_time:77870ms step_avg:124.99ms
step:634/1395 train_time:77999ms step_avg:125.00ms
step:635/1395 train_time:78128ms step_avg:125.01ms
step:636/1395 train_time:78257ms step_avg:125.01ms
step:637/1395 train_time:78387ms step_avg:125.02ms
step:638/1395 train_time:78515ms step_avg:125.02ms
step:639/1395 train_time:78643ms step_avg:125.03ms
step:640/1395 train_time:78771ms step_avg:125.03ms
step:641/1395 train_time:78899ms step_avg:125.04ms
step:642/1395 train_time:79028ms step_avg:125.04ms
step:643/1395 train_time:79157ms step_avg:125.05ms
step:644/1395 train_time:79285ms step_avg:125.06ms
step:645/1395 train_time:79415ms step_avg:125.06ms
step:646/1395 train_time:79545ms step_avg:125.07ms
step:647/1395 train_time:79673ms step_avg:125.08ms
step:648/1395 train_time:79803ms step_avg:125.08ms
step:649/1395 train_time:79932ms step_avg:125.09ms
step:650/1395 train_time:80060ms step_avg:125.09ms
step:651/1395 train_time:80189ms step_avg:125.10ms
step:652/1395 train_time:80317ms step_avg:125.10ms
step:653/1395 train_time:80445ms step_avg:125.11ms
step:654/1395 train_time:80574ms step_avg:125.11ms
step:655/1395 train_time:80703ms step_avg:125.12ms
step:656/1395 train_time:80832ms step_avg:125.13ms
step:657/1395 train_time:80961ms step_avg:125.13ms
step:658/1395 train_time:81090ms step_avg:125.14ms
step:659/1395 train_time:81218ms step_avg:125.14ms
step:660/1395 train_time:81347ms step_avg:125.15ms
step:661/1395 train_time:81476ms step_avg:125.16ms
step:662/1395 train_time:81605ms step_avg:125.16ms
step:663/1395 train_time:81733ms step_avg:125.17ms
step:664/1395 train_time:81861ms step_avg:125.17ms
step:665/1395 train_time:81990ms step_avg:125.18ms
step:666/1395 train_time:82119ms step_avg:125.18ms
step:667/1395 train_time:82248ms step_avg:125.19ms
step:668/1395 train_time:82377ms step_avg:125.19ms
step:669/1395 train_time:82506ms step_avg:125.20ms
step:670/1395 train_time:82634ms step_avg:125.20ms
step:671/1395 train_time:82762ms step_avg:125.21ms
step:672/1395 train_time:82891ms step_avg:125.21ms
step:673/1395 train_time:83020ms step_avg:125.22ms
step:674/1395 train_time:83148ms step_avg:125.22ms
step:675/1395 train_time:83278ms step_avg:125.23ms
step:676/1395 train_time:83408ms step_avg:125.24ms
step:677/1395 train_time:83536ms step_avg:125.24ms
step:678/1395 train_time:83664ms step_avg:125.25ms
step:679/1395 train_time:83793ms step_avg:125.25ms
step:680/1395 train_time:83922ms step_avg:125.26ms
step:681/1395 train_time:84050ms step_avg:125.26ms
step:682/1395 train_time:84178ms step_avg:125.27ms
step:683/1395 train_time:84308ms step_avg:125.27ms
step:684/1395 train_time:84436ms step_avg:125.28ms
step:685/1395 train_time:84565ms step_avg:125.28ms
step:686/1395 train_time:84693ms step_avg:125.29ms
step:687/1395 train_time:84821ms step_avg:125.29ms
step:688/1395 train_time:84950ms step_avg:125.30ms
step:689/1395 train_time:85079ms step_avg:125.30ms
step:690/1395 train_time:85207ms step_avg:125.30ms
step:691/1395 train_time:85336ms step_avg:125.31ms
step:692/1395 train_time:85467ms step_avg:125.32ms
step:693/1395 train_time:85595ms step_avg:125.32ms
step:694/1395 train_time:85724ms step_avg:125.33ms
step:695/1395 train_time:85853ms step_avg:125.33ms
step:696/1395 train_time:85981ms step_avg:125.34ms
step:697/1395 train_time:86110ms step_avg:125.34ms
step:698/1395 train_time:86239ms step_avg:125.35ms
step:699/1395 train_time:86368ms step_avg:125.35ms
step:700/1395 train_time:86497ms step_avg:125.36ms
step:701/1395 train_time:86626ms step_avg:125.36ms
step:702/1395 train_time:86755ms step_avg:125.37ms
step:703/1395 train_time:86883ms step_avg:125.37ms
step:704/1395 train_time:87011ms step_avg:125.38ms
step:705/1395 train_time:87141ms step_avg:125.38ms
step:706/1395 train_time:87271ms step_avg:125.39ms
step:707/1395 train_time:87399ms step_avg:125.39ms
step:708/1395 train_time:87528ms step_avg:125.40ms
step:709/1395 train_time:87657ms step_avg:125.40ms
step:710/1395 train_time:87785ms step_avg:125.41ms
step:711/1395 train_time:87915ms step_avg:125.41ms
step:712/1395 train_time:88043ms step_avg:125.42ms
step:713/1395 train_time:88172ms step_avg:125.42ms
step:714/1395 train_time:88300ms step_avg:125.43ms
step:715/1395 train_time:88430ms step_avg:125.43ms
step:716/1395 train_time:88558ms step_avg:125.44ms
step:717/1395 train_time:88687ms step_avg:125.44ms
step:718/1395 train_time:88817ms step_avg:125.45ms
step:719/1395 train_time:88946ms step_avg:125.45ms
step:720/1395 train_time:89075ms step_avg:125.46ms
step:721/1395 train_time:89203ms step_avg:125.46ms
step:722/1395 train_time:89331ms step_avg:125.47ms
step:723/1395 train_time:89460ms step_avg:125.47ms
step:724/1395 train_time:89588ms step_avg:125.47ms
step:725/1395 train_time:89717ms step_avg:125.48ms
step:726/1395 train_time:89847ms step_avg:125.48ms
step:727/1395 train_time:89979ms step_avg:125.49ms
step:728/1395 train_time:90110ms step_avg:125.50ms
step:729/1395 train_time:90239ms step_avg:125.51ms
step:730/1395 train_time:90371ms step_avg:125.51ms
step:731/1395 train_time:90501ms step_avg:125.52ms
step:732/1395 train_time:90631ms step_avg:125.53ms
step:733/1395 train_time:90762ms step_avg:125.54ms
step:734/1395 train_time:90892ms step_avg:125.54ms
step:735/1395 train_time:91022ms step_avg:125.55ms
step:736/1395 train_time:91153ms step_avg:125.55ms
step:737/1395 train_time:91283ms step_avg:125.56ms
step:738/1395 train_time:91413ms step_avg:125.57ms
step:739/1395 train_time:91543ms step_avg:125.57ms
step:740/1395 train_time:91673ms step_avg:125.58ms
step:741/1395 train_time:91805ms step_avg:125.59ms
step:742/1395 train_time:91935ms step_avg:125.59ms
step:743/1395 train_time:92065ms step_avg:125.60ms
step:744/1395 train_time:92195ms step_avg:125.61ms
step:745/1395 train_time:92326ms step_avg:125.61ms
step:746/1395 train_time:92457ms step_avg:125.62ms
step:747/1395 train_time:92587ms step_avg:125.63ms
step:748/1395 train_time:92717ms step_avg:125.63ms
step:749/1395 train_time:92849ms step_avg:125.64ms
step:750/1395 train_time:92980ms step_avg:125.65ms
step:750/1395 val_loss:3.5290 train_time:93111ms step_avg:125.83ms
step:751/1395 train_time:93134ms step_avg:125.69ms
step:752/1395 train_time:93257ms step_avg:125.68ms
step:753/1395 train_time:93389ms step_avg:125.69ms
step:754/1395 train_time:93518ms step_avg:125.70ms
step:755/1395 train_time:93648ms step_avg:125.70ms
step:756/1395 train_time:93778ms step_avg:125.71ms
step:757/1395 train_time:93908ms step_avg:125.71ms
step:758/1395 train_time:94038ms step_avg:125.72ms
step:759/1395 train_time:94169ms step_avg:125.73ms
step:760/1395 train_time:94301ms step_avg:125.73ms
step:761/1395 train_time:94432ms step_avg:125.74ms
step:762/1395 train_time:94562ms step_avg:125.75ms
step:763/1395 train_time:94692ms step_avg:125.75ms
step:764/1395 train_time:94824ms step_avg:125.76ms
step:765/1395 train_time:94954ms step_avg:125.77ms
step:766/1395 train_time:95085ms step_avg:125.77ms
step:767/1395 train_time:95216ms step_avg:125.78ms
step:768/1395 train_time:95347ms step_avg:125.79ms
step:769/1395 train_time:95478ms step_avg:125.79ms
step:770/1395 train_time:95608ms step_avg:125.80ms
step:771/1395 train_time:95739ms step_avg:125.81ms
step:772/1395 train_time:95869ms step_avg:125.81ms
step:773/1395 train_time:96000ms step_avg:125.82ms
step:774/1395 train_time:96130ms step_avg:125.83ms
step:775/1395 train_time:96260ms step_avg:125.83ms
step:776/1395 train_time:96392ms step_avg:125.84ms
step:777/1395 train_time:96522ms step_avg:125.84ms
step:778/1395 train_time:96652ms step_avg:125.85ms
step:779/1395 train_time:96783ms step_avg:125.86ms
step:780/1395 train_time:96914ms step_avg:125.86ms
step:781/1395 train_time:97044ms step_avg:125.87ms
step:782/1395 train_time:97174ms step_avg:125.87ms
step:783/1395 train_time:97305ms step_avg:125.88ms
step:784/1395 train_time:97436ms step_avg:125.89ms
step:785/1395 train_time:97567ms step_avg:125.89ms
step:786/1395 train_time:97697ms step_avg:125.90ms
step:787/1395 train_time:97828ms step_avg:125.90ms
step:788/1395 train_time:97958ms step_avg:125.91ms
step:789/1395 train_time:98088ms step_avg:125.92ms
step:790/1395 train_time:98218ms step_avg:125.92ms
step:791/1395 train_time:98349ms step_avg:125.93ms
step:792/1395 train_time:98479ms step_avg:125.93ms
step:793/1395 train_time:98609ms step_avg:125.94ms
step:794/1395 train_time:98739ms step_avg:125.94ms
step:795/1395 train_time:98871ms step_avg:125.95ms
step:796/1395 train_time:99001ms step_avg:125.96ms
step:797/1395 train_time:99131ms step_avg:125.96ms
step:798/1395 train_time:99262ms step_avg:125.97ms
step:799/1395 train_time:99393ms step_avg:125.97ms
step:800/1395 train_time:99523ms step_avg:125.98ms
step:801/1395 train_time:99653ms step_avg:125.98ms
step:802/1395 train_time:99787ms step_avg:125.99ms
step:803/1395 train_time:99914ms step_avg:125.99ms
step:804/1395 train_time:100043ms step_avg:126.00ms
step:805/1395 train_time:100175ms step_avg:126.01ms
step:806/1395 train_time:100306ms step_avg:126.01ms
step:807/1395 train_time:100436ms step_avg:126.02ms
step:808/1395 train_time:100567ms step_avg:126.02ms
step:809/1395 train_time:100696ms step_avg:126.03ms
step:810/1395 train_time:100827ms step_avg:126.03ms
step:811/1395 train_time:100957ms step_avg:126.04ms
step:812/1395 train_time:101087ms step_avg:126.04ms
step:813/1395 train_time:101217ms step_avg:126.05ms
step:814/1395 train_time:101348ms step_avg:126.05ms
step:815/1395 train_time:101479ms step_avg:126.06ms
step:816/1395 train_time:101610ms step_avg:126.07ms
step:817/1395 train_time:101741ms step_avg:126.07ms
step:818/1395 train_time:101872ms step_avg:126.08ms
step:819/1395 train_time:102003ms step_avg:126.09ms
step:820/1395 train_time:102132ms step_avg:126.09ms
step:821/1395 train_time:102263ms step_avg:126.09ms
step:822/1395 train_time:102394ms step_avg:126.10ms
step:823/1395 train_time:102524ms step_avg:126.11ms
step:824/1395 train_time:102654ms step_avg:126.11ms
step:825/1395 train_time:102785ms step_avg:126.12ms
step:826/1395 train_time:102916ms step_avg:126.12ms
step:827/1395 train_time:103048ms step_avg:126.13ms
step:828/1395 train_time:103178ms step_avg:126.13ms
step:829/1395 train_time:103309ms step_avg:126.14ms
step:830/1395 train_time:103441ms step_avg:126.15ms
step:831/1395 train_time:103572ms step_avg:126.15ms
step:832/1395 train_time:103702ms step_avg:126.16ms
step:833/1395 train_time:103833ms step_avg:126.16ms
step:834/1395 train_time:103964ms step_avg:126.17ms
step:835/1395 train_time:104094ms step_avg:126.18ms
step:836/1395 train_time:104226ms step_avg:126.18ms
step:837/1395 train_time:104357ms step_avg:126.19ms
step:838/1395 train_time:104488ms step_avg:126.19ms
step:839/1395 train_time:104618ms step_avg:126.20ms
step:840/1395 train_time:104749ms step_avg:126.20ms
step:841/1395 train_time:104879ms step_avg:126.21ms
step:842/1395 train_time:105009ms step_avg:126.21ms
step:843/1395 train_time:105140ms step_avg:126.22ms
step:844/1395 train_time:105271ms step_avg:126.22ms
step:845/1395 train_time:105401ms step_avg:126.23ms
step:846/1395 train_time:105533ms step_avg:126.24ms
step:847/1395 train_time:105663ms step_avg:126.24ms
step:848/1395 train_time:105793ms step_avg:126.25ms
step:849/1395 train_time:105924ms step_avg:126.25ms
step:850/1395 train_time:106054ms step_avg:126.26ms
step:851/1395 train_time:106185ms step_avg:126.26ms
step:852/1395 train_time:106317ms step_avg:126.27ms
step:853/1395 train_time:106448ms step_avg:126.27ms
step:854/1395 train_time:106579ms step_avg:126.28ms
step:855/1395 train_time:106710ms step_avg:126.28ms
step:856/1395 train_time:106840ms step_avg:126.29ms
step:857/1395 train_time:106972ms step_avg:126.29ms
step:858/1395 train_time:107103ms step_avg:126.30ms
step:859/1395 train_time:107234ms step_avg:126.31ms
step:860/1395 train_time:107366ms step_avg:126.31ms
step:861/1395 train_time:107496ms step_avg:126.32ms
step:862/1395 train_time:107628ms step_avg:126.32ms
step:863/1395 train_time:107758ms step_avg:126.33ms
step:864/1395 train_time:107889ms step_avg:126.33ms
step:865/1395 train_time:108019ms step_avg:126.34ms
step:866/1395 train_time:108153ms step_avg:126.35ms
step:867/1395 train_time:108285ms step_avg:126.35ms
step:868/1395 train_time:108415ms step_avg:126.36ms
step:869/1395 train_time:108545ms step_avg:126.36ms
step:870/1395 train_time:108676ms step_avg:126.37ms
step:871/1395 train_time:108808ms step_avg:126.37ms
step:872/1395 train_time:108939ms step_avg:126.38ms
step:873/1395 train_time:109069ms step_avg:126.38ms
step:874/1395 train_time:109199ms step_avg:126.39ms
step:875/1395 train_time:109330ms step_avg:126.39ms
step:875/1395 val_loss:3.4783 train_time:109459ms step_avg:126.54ms
step:876/1395 train_time:109482ms step_avg:126.42ms
step:877/1395 train_time:109603ms step_avg:126.42ms
step:878/1395 train_time:109740ms step_avg:126.43ms
step:879/1395 train_time:109866ms step_avg:126.43ms
step:880/1395 train_time:109996ms step_avg:126.43ms
step:881/1395 train_time:110126ms step_avg:126.44ms
step:882/1395 train_time:110257ms step_avg:126.44ms
step:883/1395 train_time:110386ms step_avg:126.44ms
step:884/1395 train_time:110518ms step_avg:126.45ms
step:885/1395 train_time:110650ms step_avg:126.46ms
step:886/1395 train_time:110784ms step_avg:126.47ms
step:887/1395 train_time:110914ms step_avg:126.47ms
step:888/1395 train_time:111046ms step_avg:126.48ms
step:889/1395 train_time:111178ms step_avg:126.48ms
step:890/1395 train_time:111308ms step_avg:126.49ms
step:891/1395 train_time:111438ms step_avg:126.49ms
step:892/1395 train_time:111569ms step_avg:126.50ms
step:893/1395 train_time:111700ms step_avg:126.50ms
step:894/1395 train_time:111830ms step_avg:126.50ms
step:895/1395 train_time:111962ms step_avg:126.51ms
step:896/1395 train_time:112094ms step_avg:126.52ms
step:897/1395 train_time:112225ms step_avg:126.52ms
step:898/1395 train_time:112355ms step_avg:126.53ms
step:899/1395 train_time:112486ms step_avg:126.53ms
step:900/1395 train_time:112617ms step_avg:126.54ms
step:901/1395 train_time:112751ms step_avg:126.54ms
step:902/1395 train_time:112879ms step_avg:126.55ms
step:903/1395 train_time:113010ms step_avg:126.55ms
step:904/1395 train_time:113141ms step_avg:126.56ms
step:905/1395 train_time:113272ms step_avg:126.56ms
step:906/1395 train_time:113402ms step_avg:126.57ms
step:907/1395 train_time:113534ms step_avg:126.57ms
step:908/1395 train_time:113666ms step_avg:126.58ms
step:909/1395 train_time:113796ms step_avg:126.58ms
step:910/1395 train_time:113927ms step_avg:126.59ms
step:911/1395 train_time:114058ms step_avg:126.59ms
step:912/1395 train_time:114188ms step_avg:126.59ms
step:913/1395 train_time:114320ms step_avg:126.60ms
step:914/1395 train_time:114453ms step_avg:126.61ms
step:915/1395 train_time:114583ms step_avg:126.61ms
step:916/1395 train_time:114714ms step_avg:126.62ms
step:917/1395 train_time:114846ms step_avg:126.62ms
step:918/1395 train_time:114976ms step_avg:126.63ms
step:919/1395 train_time:115109ms step_avg:126.63ms
step:920/1395 train_time:115240ms step_avg:126.64ms
step:921/1395 train_time:115371ms step_avg:126.64ms
step:922/1395 train_time:115503ms step_avg:126.65ms
step:923/1395 train_time:115633ms step_avg:126.65ms
step:924/1395 train_time:115764ms step_avg:126.66ms
step:925/1395 train_time:115895ms step_avg:126.66ms
step:926/1395 train_time:116026ms step_avg:126.67ms
step:927/1395 train_time:116157ms step_avg:126.67ms
step:928/1395 train_time:116288ms step_avg:126.68ms
step:929/1395 train_time:116420ms step_avg:126.68ms
step:930/1395 train_time:116550ms step_avg:126.68ms
step:931/1395 train_time:116681ms step_avg:126.69ms
step:932/1395 train_time:116811ms step_avg:126.69ms
step:933/1395 train_time:116945ms step_avg:126.70ms
step:934/1395 train_time:117076ms step_avg:126.71ms
step:935/1395 train_time:117209ms step_avg:126.71ms
step:936/1395 train_time:117342ms step_avg:126.72ms
step:937/1395 train_time:117475ms step_avg:126.73ms
step:938/1395 train_time:117609ms step_avg:126.73ms
step:939/1395 train_time:117740ms step_avg:126.74ms
step:940/1395 train_time:117874ms step_avg:126.75ms
step:941/1395 train_time:118007ms step_avg:126.75ms
step:942/1395 train_time:118140ms step_avg:126.76ms
step:943/1395 train_time:118273ms step_avg:126.77ms
step:944/1395 train_time:118406ms step_avg:126.77ms
step:945/1395 train_time:118539ms step_avg:126.78ms
step:946/1395 train_time:118672ms step_avg:126.79ms
step:947/1395 train_time:118807ms step_avg:126.80ms
step:948/1395 train_time:118940ms step_avg:126.80ms
step:949/1395 train_time:119073ms step_avg:126.81ms
step:950/1395 train_time:119206ms step_avg:126.81ms
step:951/1395 train_time:119340ms step_avg:126.82ms
step:952/1395 train_time:119472ms step_avg:126.83ms
step:953/1395 train_time:119604ms step_avg:126.83ms
step:954/1395 train_time:119736ms step_avg:126.84ms
step:955/1395 train_time:119867ms step_avg:126.84ms
step:956/1395 train_time:120000ms step_avg:126.85ms
step:957/1395 train_time:120133ms step_avg:126.86ms
step:958/1395 train_time:120266ms step_avg:126.86ms
step:959/1395 train_time:120400ms step_avg:126.87ms
step:960/1395 train_time:120532ms step_avg:126.88ms
step:961/1395 train_time:120665ms step_avg:126.88ms
step:962/1395 train_time:120798ms step_avg:126.89ms
step:963/1395 train_time:120931ms step_avg:126.90ms
step:964/1395 train_time:121063ms step_avg:126.90ms
step:965/1395 train_time:121196ms step_avg:126.91ms
step:966/1395 train_time:121328ms step_avg:126.91ms
step:967/1395 train_time:121461ms step_avg:126.92ms
step:968/1395 train_time:121593ms step_avg:126.92ms
step:969/1395 train_time:121727ms step_avg:126.93ms
step:970/1395 train_time:121859ms step_avg:126.94ms
step:971/1395 train_time:121992ms step_avg:126.94ms
step:972/1395 train_time:122124ms step_avg:126.95ms
step:973/1395 train_time:122255ms step_avg:126.95ms
step:974/1395 train_time:122388ms step_avg:126.96ms
step:975/1395 train_time:122520ms step_avg:126.96ms
step:976/1395 train_time:122652ms step_avg:126.97ms
step:977/1395 train_time:122785ms step_avg:126.97ms
step:978/1395 train_time:122917ms step_avg:126.98ms
step:979/1395 train_time:123052ms step_avg:126.99ms
step:980/1395 train_time:123182ms step_avg:126.99ms
step:981/1395 train_time:123314ms step_avg:127.00ms
step:982/1395 train_time:123446ms step_avg:127.00ms
step:983/1395 train_time:123578ms step_avg:127.01ms
step:984/1395 train_time:123710ms step_avg:127.01ms
step:985/1395 train_time:123842ms step_avg:127.02ms
step:986/1395 train_time:123976ms step_avg:127.03ms
step:987/1395 train_time:124108ms step_avg:127.03ms
step:988/1395 train_time:124240ms step_avg:127.04ms
step:989/1395 train_time:124374ms step_avg:127.04ms
step:990/1395 train_time:124506ms step_avg:127.05ms
step:991/1395 train_time:124638ms step_avg:127.05ms
step:992/1395 train_time:124772ms step_avg:127.06ms
step:993/1395 train_time:124908ms step_avg:127.07ms
step:994/1395 train_time:125040ms step_avg:127.07ms
step:995/1395 train_time:125172ms step_avg:127.08ms
step:996/1395 train_time:125304ms step_avg:127.08ms
step:997/1395 train_time:125435ms step_avg:127.09ms
step:998/1395 train_time:125567ms step_avg:127.09ms
step:999/1395 train_time:125699ms step_avg:127.10ms
step:1000/1395 train_time:125833ms step_avg:127.10ms
step:1000/1395 val_loss:3.4150 train_time:125965ms step_avg:127.24ms
step:1001/1395 train_time:125987ms step_avg:127.13ms
step:1002/1395 train_time:126111ms step_avg:127.13ms
step:1003/1395 train_time:126245ms step_avg:127.13ms
step:1004/1395 train_time:126377ms step_avg:127.14ms
step:1005/1395 train_time:126510ms step_avg:127.15ms
step:1006/1395 train_time:126641ms step_avg:127.15ms
step:1007/1395 train_time:126773ms step_avg:127.15ms
step:1008/1395 train_time:126905ms step_avg:127.16ms
step:1009/1395 train_time:127039ms step_avg:127.17ms
step:1010/1395 train_time:127174ms step_avg:127.17ms
step:1011/1395 train_time:127307ms step_avg:127.18ms
step:1012/1395 train_time:127440ms step_avg:127.19ms
step:1013/1395 train_time:127573ms step_avg:127.19ms
step:1014/1395 train_time:127705ms step_avg:127.20ms
step:1015/1395 train_time:127837ms step_avg:127.20ms
step:1016/1395 train_time:127970ms step_avg:127.21ms
step:1017/1395 train_time:128102ms step_avg:127.21ms
step:1018/1395 train_time:128234ms step_avg:127.22ms
step:1019/1395 train_time:128367ms step_avg:127.22ms
step:1020/1395 train_time:128500ms step_avg:127.23ms
step:1021/1395 train_time:128633ms step_avg:127.23ms
step:1022/1395 train_time:128766ms step_avg:127.24ms
step:1023/1395 train_time:128899ms step_avg:127.25ms
step:1024/1395 train_time:129032ms step_avg:127.25ms
step:1025/1395 train_time:129164ms step_avg:127.26ms
step:1026/1395 train_time:129298ms step_avg:127.26ms
step:1027/1395 train_time:129431ms step_avg:127.27ms
step:1028/1395 train_time:129564ms step_avg:127.27ms
step:1029/1395 train_time:129698ms step_avg:127.28ms
step:1030/1395 train_time:129831ms step_avg:127.29ms
step:1031/1395 train_time:129963ms step_avg:127.29ms
step:1032/1395 train_time:130095ms step_avg:127.29ms
step:1033/1395 train_time:130227ms step_avg:127.30ms
step:1034/1395 train_time:130359ms step_avg:127.30ms
step:1035/1395 train_time:130492ms step_avg:127.31ms
step:1036/1395 train_time:130625ms step_avg:127.31ms
step:1037/1395 train_time:130759ms step_avg:127.32ms
step:1038/1395 train_time:130893ms step_avg:127.33ms
step:1039/1395 train_time:131024ms step_avg:127.33ms
step:1040/1395 train_time:131157ms step_avg:127.34ms
step:1041/1395 train_time:131293ms step_avg:127.35ms
step:1042/1395 train_time:131422ms step_avg:127.35ms
step:1043/1395 train_time:131554ms step_avg:127.35ms
step:1044/1395 train_time:131687ms step_avg:127.36ms
step:1045/1395 train_time:131820ms step_avg:127.36ms
step:1046/1395 train_time:131953ms step_avg:127.37ms
step:1047/1395 train_time:132085ms step_avg:127.37ms
step:1048/1395 train_time:132218ms step_avg:127.38ms
step:1049/1395 train_time:132351ms step_avg:127.38ms
step:1050/1395 train_time:132483ms step_avg:127.39ms
step:1051/1395 train_time:132618ms step_avg:127.39ms
step:1052/1395 train_time:132751ms step_avg:127.40ms
step:1053/1395 train_time:132883ms step_avg:127.40ms
step:1054/1395 train_time:133015ms step_avg:127.41ms
step:1055/1395 train_time:133149ms step_avg:127.42ms
step:1056/1395 train_time:133282ms step_avg:127.42ms
step:1057/1395 train_time:133414ms step_avg:127.42ms
step:1058/1395 train_time:133547ms step_avg:127.43ms
step:1059/1395 train_time:133680ms step_avg:127.44ms
step:1060/1395 train_time:133815ms step_avg:127.44ms
step:1061/1395 train_time:133947ms step_avg:127.45ms
step:1062/1395 train_time:134079ms step_avg:127.45ms
step:1063/1395 train_time:134212ms step_avg:127.46ms
step:1064/1395 train_time:134344ms step_avg:127.46ms
step:1065/1395 train_time:134477ms step_avg:127.47ms
step:1066/1395 train_time:134611ms step_avg:127.47ms
step:1067/1395 train_time:134744ms step_avg:127.48ms
step:1068/1395 train_time:134877ms step_avg:127.48ms
step:1069/1395 train_time:135011ms step_avg:127.49ms
step:1070/1395 train_time:135143ms step_avg:127.49ms
step:1071/1395 train_time:135278ms step_avg:127.50ms
step:1072/1395 train_time:135410ms step_avg:127.50ms
step:1073/1395 train_time:135541ms step_avg:127.51ms
step:1074/1395 train_time:135674ms step_avg:127.51ms
step:1075/1395 train_time:135807ms step_avg:127.52ms
step:1076/1395 train_time:135939ms step_avg:127.52ms
step:1077/1395 train_time:136071ms step_avg:127.53ms
step:1078/1395 train_time:136204ms step_avg:127.53ms
step:1079/1395 train_time:136341ms step_avg:127.54ms
step:1080/1395 train_time:136474ms step_avg:127.55ms
step:1081/1395 train_time:136606ms step_avg:127.55ms
step:1082/1395 train_time:136738ms step_avg:127.55ms
step:1083/1395 train_time:136870ms step_avg:127.56ms
step:1084/1395 train_time:137004ms step_avg:127.56ms
step:1085/1395 train_time:137137ms step_avg:127.57ms
step:1086/1395 train_time:137270ms step_avg:127.57ms
step:1087/1395 train_time:137403ms step_avg:127.58ms
step:1088/1395 train_time:137535ms step_avg:127.58ms
step:1089/1395 train_time:137670ms step_avg:127.59ms
step:1090/1395 train_time:137803ms step_avg:127.60ms
step:1091/1395 train_time:137935ms step_avg:127.60ms
step:1092/1395 train_time:138068ms step_avg:127.60ms
step:1093/1395 train_time:138201ms step_avg:127.61ms
step:1094/1395 train_time:138333ms step_avg:127.61ms
step:1095/1395 train_time:138466ms step_avg:127.62ms
step:1096/1395 train_time:138600ms step_avg:127.62ms
step:1097/1395 train_time:138732ms step_avg:127.63ms
step:1098/1395 train_time:138866ms step_avg:127.63ms
step:1099/1395 train_time:138998ms step_avg:127.64ms
step:1100/1395 train_time:139131ms step_avg:127.64ms
step:1101/1395 train_time:139263ms step_avg:127.65ms
step:1102/1395 train_time:139396ms step_avg:127.65ms
step:1103/1395 train_time:139529ms step_avg:127.66ms
step:1104/1395 train_time:139661ms step_avg:127.66ms
step:1105/1395 train_time:139796ms step_avg:127.67ms
step:1106/1395 train_time:139930ms step_avg:127.67ms
step:1107/1395 train_time:140062ms step_avg:127.68ms
step:1108/1395 train_time:140198ms step_avg:127.69ms
step:1109/1395 train_time:140331ms step_avg:127.69ms
step:1110/1395 train_time:140464ms step_avg:127.69ms
step:1111/1395 train_time:140598ms step_avg:127.70ms
step:1112/1395 train_time:140731ms step_avg:127.70ms
step:1113/1395 train_time:140862ms step_avg:127.71ms
step:1114/1395 train_time:140994ms step_avg:127.71ms
step:1115/1395 train_time:141128ms step_avg:127.72ms
step:1116/1395 train_time:141261ms step_avg:127.72ms
step:1117/1395 train_time:141395ms step_avg:127.73ms
step:1118/1395 train_time:141528ms step_avg:127.73ms
step:1119/1395 train_time:141660ms step_avg:127.74ms
step:1120/1395 train_time:141793ms step_avg:127.74ms
step:1121/1395 train_time:141924ms step_avg:127.74ms
step:1122/1395 train_time:142057ms step_avg:127.75ms
step:1123/1395 train_time:142190ms step_avg:127.75ms
step:1124/1395 train_time:142323ms step_avg:127.76ms
step:1125/1395 train_time:142456ms step_avg:127.76ms
step:1125/1395 val_loss:3.3650 train_time:142588ms step_avg:127.88ms
step:1126/1395 train_time:142610ms step_avg:127.79ms
step:1127/1395 train_time:142731ms step_avg:127.78ms
step:1128/1395 train_time:142864ms step_avg:127.79ms
step:1129/1395 train_time:142996ms step_avg:127.79ms
step:1130/1395 train_time:143128ms step_avg:127.79ms
step:1131/1395 train_time:143261ms step_avg:127.80ms
step:1132/1395 train_time:143394ms step_avg:127.80ms
step:1133/1395 train_time:143525ms step_avg:127.80ms
step:1134/1395 train_time:143659ms step_avg:127.81ms
step:1135/1395 train_time:143793ms step_avg:127.82ms
step:1136/1395 train_time:143929ms step_avg:127.82ms
step:1137/1395 train_time:144062ms step_avg:127.83ms
step:1138/1395 train_time:144195ms step_avg:127.83ms
step:1139/1395 train_time:144329ms step_avg:127.84ms
step:1140/1395 train_time:144465ms step_avg:127.85ms
step:1141/1395 train_time:144599ms step_avg:127.85ms
step:1142/1395 train_time:144732ms step_avg:127.86ms
step:1143/1395 train_time:144867ms step_avg:127.86ms
step:1144/1395 train_time:145000ms step_avg:127.87ms
step:1145/1395 train_time:145133ms step_avg:127.87ms
step:1146/1395 train_time:145267ms step_avg:127.88ms
step:1147/1395 train_time:145402ms step_avg:127.88ms
step:1148/1395 train_time:145536ms step_avg:127.89ms
step:1149/1395 train_time:145671ms step_avg:127.89ms
step:1150/1395 train_time:145805ms step_avg:127.90ms
step:1151/1395 train_time:145941ms step_avg:127.91ms
step:1152/1395 train_time:146076ms step_avg:127.91ms
step:1153/1395 train_time:146211ms step_avg:127.92ms
step:1154/1395 train_time:146346ms step_avg:127.92ms
step:1155/1395 train_time:146480ms step_avg:127.93ms
step:1156/1395 train_time:146617ms step_avg:127.94ms
step:1157/1395 train_time:146751ms step_avg:127.94ms
step:1158/1395 train_time:146884ms step_avg:127.95ms
step:1159/1395 train_time:147018ms step_avg:127.95ms
step:1160/1395 train_time:147151ms step_avg:127.96ms
step:1161/1395 train_time:147285ms step_avg:127.96ms
step:1162/1395 train_time:147420ms step_avg:127.97ms
step:1163/1395 train_time:147555ms step_avg:127.97ms
step:1164/1395 train_time:147690ms step_avg:127.98ms
step:1165/1395 train_time:147823ms step_avg:127.99ms
step:1166/1395 train_time:147956ms step_avg:127.99ms
step:1167/1395 train_time:148090ms step_avg:127.99ms
step:1168/1395 train_time:148224ms step_avg:128.00ms
step:1169/1395 train_time:148358ms step_avg:128.00ms
step:1170/1395 train_time:148491ms step_avg:128.01ms
step:1171/1395 train_time:148626ms step_avg:128.02ms
step:1172/1395 train_time:148762ms step_avg:128.02ms
step:1173/1395 train_time:148895ms step_avg:128.03ms
step:1174/1395 train_time:149034ms step_avg:128.04ms
step:1175/1395 train_time:149167ms step_avg:128.04ms
step:1176/1395 train_time:149301ms step_avg:128.05ms
step:1177/1395 train_time:149440ms step_avg:128.05ms
step:1178/1395 train_time:149573ms step_avg:128.06ms
step:1179/1395 train_time:149708ms step_avg:128.07ms
step:1180/1395 train_time:149844ms step_avg:128.07ms
step:1181/1395 train_time:149980ms step_avg:128.08ms
step:1182/1395 train_time:150113ms step_avg:128.08ms
step:1183/1395 train_time:150247ms step_avg:128.09ms
step:1184/1395 train_time:150382ms step_avg:128.09ms
step:1185/1395 train_time:150517ms step_avg:128.10ms
step:1186/1395 train_time:150650ms step_avg:128.10ms
step:1187/1395 train_time:150789ms step_avg:128.11ms
step:1188/1395 train_time:150923ms step_avg:128.12ms
step:1189/1395 train_time:151057ms step_avg:128.12ms
step:1190/1395 train_time:151191ms step_avg:128.13ms
step:1191/1395 train_time:151325ms step_avg:128.13ms
step:1192/1395 train_time:151459ms step_avg:128.14ms
step:1193/1395 train_time:151592ms step_avg:128.14ms
step:1194/1395 train_time:151726ms step_avg:128.15ms
step:1195/1395 train_time:151861ms step_avg:128.15ms
step:1196/1395 train_time:151995ms step_avg:128.16ms
step:1197/1395 train_time:152129ms step_avg:128.16ms
step:1198/1395 train_time:152265ms step_avg:128.17ms
step:1199/1395 train_time:152399ms step_avg:128.17ms
step:1200/1395 train_time:152532ms step_avg:128.18ms
step:1201/1395 train_time:152665ms step_avg:128.18ms
step:1202/1395 train_time:152804ms step_avg:128.19ms
step:1203/1395 train_time:152941ms step_avg:128.20ms
step:1204/1395 train_time:153074ms step_avg:128.20ms
step:1205/1395 train_time:153209ms step_avg:128.21ms
step:1206/1395 train_time:153346ms step_avg:128.22ms
step:1207/1395 train_time:153479ms step_avg:128.22ms
step:1208/1395 train_time:153614ms step_avg:128.22ms
step:1209/1395 train_time:153746ms step_avg:128.23ms
step:1210/1395 train_time:153883ms step_avg:128.24ms
step:1211/1395 train_time:154018ms step_avg:128.24ms
step:1212/1395 train_time:154151ms step_avg:128.25ms
step:1213/1395 train_time:154285ms step_avg:128.25ms
step:1214/1395 train_time:154420ms step_avg:128.26ms
step:1215/1395 train_time:154556ms step_avg:128.26ms
step:1216/1395 train_time:154688ms step_avg:128.27ms
step:1217/1395 train_time:154823ms step_avg:128.27ms
step:1218/1395 train_time:154956ms step_avg:128.28ms
step:1219/1395 train_time:155089ms step_avg:128.28ms
step:1220/1395 train_time:155223ms step_avg:128.28ms
step:1221/1395 train_time:155358ms step_avg:128.29ms
step:1222/1395 train_time:155492ms step_avg:128.29ms
step:1223/1395 train_time:155626ms step_avg:128.30ms
step:1224/1395 train_time:155760ms step_avg:128.30ms
step:1225/1395 train_time:155896ms step_avg:128.31ms
step:1226/1395 train_time:156029ms step_avg:128.31ms
step:1227/1395 train_time:156164ms step_avg:128.32ms
step:1228/1395 train_time:156299ms step_avg:128.32ms
step:1229/1395 train_time:156431ms step_avg:128.33ms
step:1230/1395 train_time:156568ms step_avg:128.33ms
step:1231/1395 train_time:156704ms step_avg:128.34ms
step:1232/1395 train_time:156839ms step_avg:128.35ms
step:1233/1395 train_time:156973ms step_avg:128.35ms
step:1234/1395 train_time:157108ms step_avg:128.36ms
step:1235/1395 train_time:157243ms step_avg:128.36ms
step:1236/1395 train_time:157378ms step_avg:128.37ms
step:1237/1395 train_time:157511ms step_avg:128.37ms
step:1238/1395 train_time:157650ms step_avg:128.38ms
step:1239/1395 train_time:157783ms step_avg:128.38ms
step:1240/1395 train_time:157919ms step_avg:128.39ms
step:1241/1395 train_time:158055ms step_avg:128.40ms
step:1242/1395 train_time:158189ms step_avg:128.40ms
step:1243/1395 train_time:158324ms step_avg:128.41ms
step:1244/1395 train_time:158458ms step_avg:128.41ms
step:1245/1395 train_time:158593ms step_avg:128.42ms
step:1246/1395 train_time:158726ms step_avg:128.42ms
step:1247/1395 train_time:158862ms step_avg:128.43ms
step:1248/1395 train_time:158996ms step_avg:128.43ms
step:1249/1395 train_time:159129ms step_avg:128.43ms
step:1250/1395 train_time:159263ms step_avg:128.44ms
step:1250/1395 val_loss:3.3184 train_time:159396ms step_avg:128.55ms
step:1251/1395 train_time:159418ms step_avg:128.46ms
step:1252/1395 train_time:159546ms step_avg:128.46ms
step:1253/1395 train_time:159679ms step_avg:128.46ms
step:1254/1395 train_time:159813ms step_avg:128.47ms
step:1255/1395 train_time:159951ms step_avg:128.47ms
step:1256/1395 train_time:160083ms step_avg:128.48ms
step:1257/1395 train_time:160217ms step_avg:128.48ms
step:1258/1395 train_time:160351ms step_avg:128.49ms
step:1259/1395 train_time:160488ms step_avg:128.49ms
step:1260/1395 train_time:160621ms step_avg:128.50ms
step:1261/1395 train_time:160755ms step_avg:128.50ms
step:1262/1395 train_time:160890ms step_avg:128.51ms
step:1263/1395 train_time:161024ms step_avg:128.51ms
step:1264/1395 train_time:161158ms step_avg:128.51ms
step:1265/1395 train_time:161292ms step_avg:128.52ms
step:1266/1395 train_time:161427ms step_avg:128.52ms
step:1267/1395 train_time:161561ms step_avg:128.53ms
step:1268/1395 train_time:161696ms step_avg:128.53ms
step:1269/1395 train_time:161832ms step_avg:128.54ms
step:1270/1395 train_time:161967ms step_avg:128.54ms
step:1271/1395 train_time:162102ms step_avg:128.55ms
step:1272/1395 train_time:162234ms step_avg:128.55ms
step:1273/1395 train_time:162372ms step_avg:128.56ms
step:1274/1395 train_time:162501ms step_avg:128.56ms
step:1275/1395 train_time:162636ms step_avg:128.57ms
step:1276/1395 train_time:162771ms step_avg:128.57ms
step:1277/1395 train_time:162905ms step_avg:128.58ms
step:1278/1395 train_time:163039ms step_avg:128.58ms
step:1279/1395 train_time:163173ms step_avg:128.58ms
step:1280/1395 train_time:163310ms step_avg:128.59ms
step:1281/1395 train_time:163444ms step_avg:128.59ms
step:1282/1395 train_time:163577ms step_avg:128.60ms
step:1283/1395 train_time:163711ms step_avg:128.60ms
step:1284/1395 train_time:163847ms step_avg:128.61ms
step:1285/1395 train_time:163981ms step_avg:128.61ms
step:1286/1395 train_time:164117ms step_avg:128.62ms
step:1287/1395 train_time:164251ms step_avg:128.62ms
step:1288/1395 train_time:164384ms step_avg:128.63ms
step:1289/1395 train_time:164521ms step_avg:128.63ms
step:1290/1395 train_time:164657ms step_avg:128.64ms
step:1291/1395 train_time:164795ms step_avg:128.65ms
step:1292/1395 train_time:164929ms step_avg:128.65ms
step:1293/1395 train_time:165065ms step_avg:128.66ms
step:1294/1395 train_time:165200ms step_avg:128.66ms
step:1295/1395 train_time:165334ms step_avg:128.66ms
step:1296/1395 train_time:165468ms step_avg:128.67ms
step:1297/1395 train_time:165603ms step_avg:128.67ms
step:1298/1395 train_time:165736ms step_avg:128.68ms
step:1299/1395 train_time:165871ms step_avg:128.68ms
step:1300/1395 train_time:166007ms step_avg:128.69ms
step:1301/1395 train_time:166142ms step_avg:128.69ms
step:1302/1395 train_time:166277ms step_avg:128.70ms
step:1303/1395 train_time:166411ms step_avg:128.70ms
step:1304/1395 train_time:166547ms step_avg:128.71ms
step:1305/1395 train_time:166684ms step_avg:128.71ms
step:1306/1395 train_time:166817ms step_avg:128.72ms
step:1307/1395 train_time:166953ms step_avg:128.72ms
step:1308/1395 train_time:167087ms step_avg:128.73ms
step:1309/1395 train_time:167222ms step_avg:128.73ms
step:1310/1395 train_time:167357ms step_avg:128.74ms
step:1311/1395 train_time:167491ms step_avg:128.74ms
step:1312/1395 train_time:167624ms step_avg:128.74ms
step:1313/1395 train_time:167758ms step_avg:128.75ms
step:1314/1395 train_time:167892ms step_avg:128.75ms
step:1315/1395 train_time:168027ms step_avg:128.76ms
step:1316/1395 train_time:168161ms step_avg:128.76ms
step:1317/1395 train_time:168296ms step_avg:128.76ms
step:1318/1395 train_time:168431ms step_avg:128.77ms
step:1319/1395 train_time:168568ms step_avg:128.78ms
step:1320/1395 train_time:168701ms step_avg:128.78ms
step:1321/1395 train_time:168835ms step_avg:128.78ms
step:1322/1395 train_time:168972ms step_avg:128.79ms
step:1323/1395 train_time:169106ms step_avg:128.79ms
step:1324/1395 train_time:169240ms step_avg:128.80ms
step:1325/1395 train_time:169374ms step_avg:128.80ms
step:1326/1395 train_time:169509ms step_avg:128.81ms
step:1327/1395 train_time:169643ms step_avg:128.81ms
step:1328/1395 train_time:169778ms step_avg:128.81ms
step:1329/1395 train_time:169917ms step_avg:128.82ms
step:1330/1395 train_time:170053ms step_avg:128.83ms
step:1331/1395 train_time:170189ms step_avg:128.83ms
step:1332/1395 train_time:170324ms step_avg:128.84ms
step:1333/1395 train_time:170459ms step_avg:128.84ms
step:1334/1395 train_time:170593ms step_avg:128.85ms
step:1335/1395 train_time:170726ms step_avg:128.85ms
step:1336/1395 train_time:170863ms step_avg:128.86ms
step:1337/1395 train_time:170998ms step_avg:128.86ms
step:1338/1395 train_time:171132ms step_avg:128.86ms
step:1339/1395 train_time:171266ms step_avg:128.87ms
step:1340/1395 train_time:171403ms step_avg:128.87ms
step:1341/1395 train_time:171536ms step_avg:128.88ms
step:1342/1395 train_time:171671ms step_avg:128.88ms
step:1343/1395 train_time:171804ms step_avg:128.89ms
step:1344/1395 train_time:171938ms step_avg:128.89ms
step:1345/1395 train_time:172072ms step_avg:128.89ms
step:1346/1395 train_time:172207ms step_avg:128.90ms
step:1347/1395 train_time:172344ms step_avg:128.90ms
step:1348/1395 train_time:172478ms step_avg:128.91ms
step:1349/1395 train_time:172614ms step_avg:128.91ms
step:1350/1395 train_time:172747ms step_avg:128.92ms
step:1351/1395 train_time:172882ms step_avg:128.92ms
step:1352/1395 train_time:173022ms step_avg:128.93ms
step:1353/1395 train_time:173160ms step_avg:128.94ms
step:1354/1395 train_time:173297ms step_avg:128.94ms
step:1355/1395 train_time:173430ms step_avg:128.94ms
step:1356/1395 train_time:173565ms step_avg:128.95ms
step:1357/1395 train_time:173701ms step_avg:128.95ms
step:1358/1395 train_time:173839ms step_avg:128.96ms
step:1359/1395 train_time:173974ms step_avg:128.97ms
step:1360/1395 train_time:174110ms step_avg:128.97ms
step:1361/1395 train_time:174246ms step_avg:128.98ms
step:1362/1395 train_time:174383ms step_avg:128.98ms
step:1363/1395 train_time:174520ms step_avg:128.99ms
step:1364/1395 train_time:174657ms step_avg:128.99ms
step:1365/1395 train_time:174791ms step_avg:129.00ms
step:1366/1395 train_time:174926ms step_avg:129.00ms
step:1367/1395 train_time:175062ms step_avg:129.01ms
step:1368/1395 train_time:175198ms step_avg:129.01ms
step:1369/1395 train_time:175337ms step_avg:129.02ms
step:1370/1395 train_time:175476ms step_avg:129.03ms
step:1371/1395 train_time:175611ms step_avg:129.03ms
step:1372/1395 train_time:175747ms step_avg:129.04ms
step:1373/1395 train_time:175883ms step_avg:129.04ms
step:1374/1395 train_time:176020ms step_avg:129.05ms
step:1375/1395 train_time:176154ms step_avg:129.05ms
step:1375/1395 val_loss:3.2836 train_time:176287ms step_avg:129.15ms
step:1376/1395 train_time:176309ms step_avg:129.07ms
step:1377/1395 train_time:176434ms step_avg:129.07ms
step:1378/1395 train_time:176569ms step_avg:129.07ms
step:1379/1395 train_time:176704ms step_avg:129.08ms
step:1380/1395 train_time:176840ms step_avg:129.08ms
step:1381/1395 train_time:176976ms step_avg:129.09ms
step:1382/1395 train_time:177113ms step_avg:129.09ms
step:1383/1395 train_time:177247ms step_avg:129.09ms
step:1384/1395 train_time:177385ms step_avg:129.10ms
step:1385/1395 train_time:177520ms step_avg:129.11ms
step:1386/1395 train_time:177657ms step_avg:129.11ms
step:1387/1395 train_time:177794ms step_avg:129.12ms
step:1388/1395 train_time:177928ms step_avg:129.12ms
step:1389/1395 train_time:178063ms step_avg:129.12ms
step:1390/1395 train_time:178200ms step_avg:129.13ms
step:1391/1395 train_time:178335ms step_avg:129.13ms
step:1392/1395 train_time:178471ms step_avg:129.14ms
step:1393/1395 train_time:178606ms step_avg:129.14ms
step:1394/1395 train_time:178741ms step_avg:129.15ms
step:1395/1395 train_time:178876ms step_avg:129.15ms
step:1395/1395 val_loss:3.2793 train_time:179009ms step_avg:129.25ms
peak memory allocated: 37653 MiB reserved: 39156 MiB
