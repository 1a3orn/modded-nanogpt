import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True

# -----------------------------------------------------------------------------
# Custom operators

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

def lm_head_fp8(x: Tensor, w: Tensor) -> Tensor:
    _x = x.flatten(0, -2)
    out: Tensor = torch.ops.nanogpt.mm(_x, w, x_s=2.0, w_s=32.0, grad_s=2.0**29)[0]
    return out.reshape(*x.shape[:-1], -1)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev():
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int):
        super().__init__(in_features, out_features, bias=False)

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x):
        return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len=65536):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        # self.c_q = CastedLinear(dim, dim)
        # self.c_k = CastedLinear(dim, dim)
        # self.c_v = CastedLinear(dim, dim)
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std
        self.qkv_w = nn.Parameter(torch.empty(3, dim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads) # dim // num_heads = head_dim
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # Set attention scale such that the minimum attainable attention entropy
        # (but not necessary the attention entropy itself) is close to 0. By @leloykun
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        # q = self.c_q(x).view(B, T, self.num_heads, -1)
        # k = self.c_k(x).view(B, T, self.num_heads, -1)
        # v = self.c_v(x).view(B, T, self.num_heads, -1)
        # qkv_weight = torch.cat([self.c_q.weight, self.c_k.weight, self.c_v.weight], dim=0).type_as(x)
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3*self.num_heads, -1).chunk(3, dim=-2)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, model_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(model_dim, num_heads, layer_idx) if layer_idx != 7 else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, ve, x0, block_mask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, num_embeddings: int, embedding_dim: int):
        super().__init__()
        self.embed = nn.ModuleList([nn.Embedding(num_embeddings, embedding_dim) for _ in range(3)])

    def forward(self, input_seq) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2], None, None, None, None, None, None, ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual learning
        self.value_embeds = ValueEmbedding(vocab_size, model_dim)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128))
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        assert input_seq.ndim == 1
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        docs = (input_seq == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        def create_doc_swc_block_masks(sliding_window_num_blocks: Tensor):
            kv_idx = block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & document_bm
            full_bm  = causal_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            def build_bm(sw_num_blocks: Tensor) -> BlockMask:
                return BlockMask.from_kv_blocks(
                    torch.clamp_max(kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                    kv_indices,
                    torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                    full_kv_indices,
                    BLOCK_SIZE=BLOCK_SIZE,
                    mask_mod=document_causal,
                )
            return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

        # Long-short SWA block masks by @leloykun & @YouJiacheng
        long_bm, short_bm = create_doc_swc_block_masks(sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = lm_head_fp8(x, self.lm_head.weight) if self.training else self.lm_head(x)
        # @Grad62304977 added tanh softcapping, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use cycle(files) if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    batch_size = 8*64*1024 # batch size in tokens
    num_iterations = 1395 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 64*1024 # FlexAttention sequence length
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
             if console:
                 print(s)
             print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_loader = distributed_data_generator(args.train_files, args.batch_size, rank, world_size)

model = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim >= 2]
embed_params = [model.embed.weight, *model.value_embeds.parameters()]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), fused=True, eps=1e-10)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(it: int):
    t = 1 - it / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_bs = world_size * args.seq_len
        assert args.val_tokens % val_bs == 0
        val_steps = args.val_tokens // val_bs
        val_loader = distributed_data_generator(args.val_files, val_bs, rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION BEGIN -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB"
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250110+cu126 compiled for CUDA 12.6
Thu Jan 16 21:12:39 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   38C    P0             121W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   28C    P0             115W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0             118W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   38C    P0             119W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   37C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1395 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1395 train_time:23549ms step_avg:nanms
step:2/1395 train_time:24430ms step_avg:nanms
step:3/1395 train_time:24550ms step_avg:nanms
step:4/1395 train_time:24670ms step_avg:nanms
step:5/1395 train_time:24792ms step_avg:nanms
step:6/1395 train_time:24912ms step_avg:nanms
step:7/1395 train_time:25034ms step_avg:nanms
step:8/1395 train_time:25155ms step_avg:nanms
step:9/1395 train_time:25277ms step_avg:nanms
step:10/1395 train_time:25398ms step_avg:nanms
step:11/1395 train_time:123ms step_avg:nanms
step:12/1395 train_time:250ms step_avg:nanms
step:13/1395 train_time:368ms step_avg:122.83ms
step:14/1395 train_time:491ms step_avg:122.75ms
step:15/1395 train_time:614ms step_avg:122.78ms
step:16/1395 train_time:736ms step_avg:122.64ms
step:17/1395 train_time:857ms step_avg:122.49ms
step:18/1395 train_time:978ms step_avg:122.31ms
step:19/1395 train_time:1100ms step_avg:122.19ms
step:20/1395 train_time:1222ms step_avg:122.22ms
step:21/1395 train_time:1345ms step_avg:122.27ms
step:22/1395 train_time:1468ms step_avg:122.33ms
step:23/1395 train_time:1590ms step_avg:122.34ms
step:24/1395 train_time:1713ms step_avg:122.33ms
step:25/1395 train_time:1834ms step_avg:122.29ms
step:26/1395 train_time:1956ms step_avg:122.26ms
step:27/1395 train_time:2078ms step_avg:122.22ms
step:28/1395 train_time:2199ms step_avg:122.17ms
step:29/1395 train_time:2322ms step_avg:122.22ms
step:30/1395 train_time:2445ms step_avg:122.27ms
step:31/1395 train_time:2568ms step_avg:122.30ms
step:32/1395 train_time:2692ms step_avg:122.35ms
step:33/1395 train_time:2815ms step_avg:122.38ms
step:34/1395 train_time:2937ms step_avg:122.35ms
step:35/1395 train_time:3058ms step_avg:122.31ms
step:36/1395 train_time:3179ms step_avg:122.29ms
step:37/1395 train_time:3302ms step_avg:122.29ms
step:38/1395 train_time:3424ms step_avg:122.29ms
step:39/1395 train_time:3550ms step_avg:122.42ms
step:40/1395 train_time:3669ms step_avg:122.32ms
step:41/1395 train_time:3792ms step_avg:122.32ms
step:42/1395 train_time:3914ms step_avg:122.30ms
step:43/1395 train_time:4036ms step_avg:122.30ms
step:44/1395 train_time:4157ms step_avg:122.28ms
step:45/1395 train_time:4279ms step_avg:122.26ms
step:46/1395 train_time:4401ms step_avg:122.24ms
step:47/1395 train_time:4523ms step_avg:122.25ms
step:48/1395 train_time:4645ms step_avg:122.25ms
step:49/1395 train_time:4768ms step_avg:122.26ms
step:50/1395 train_time:4891ms step_avg:122.26ms
step:51/1395 train_time:5014ms step_avg:122.29ms
step:52/1395 train_time:5137ms step_avg:122.31ms
step:53/1395 train_time:5258ms step_avg:122.29ms
step:54/1395 train_time:5380ms step_avg:122.27ms
step:55/1395 train_time:5502ms step_avg:122.27ms
step:56/1395 train_time:5624ms step_avg:122.27ms
step:57/1395 train_time:5746ms step_avg:122.26ms
step:58/1395 train_time:5869ms step_avg:122.26ms
step:59/1395 train_time:5991ms step_avg:122.26ms
step:60/1395 train_time:6114ms step_avg:122.28ms
step:61/1395 train_time:6236ms step_avg:122.27ms
step:62/1395 train_time:6359ms step_avg:122.29ms
step:63/1395 train_time:6482ms step_avg:122.30ms
step:64/1395 train_time:6603ms step_avg:122.27ms
step:65/1395 train_time:6724ms step_avg:122.26ms
step:66/1395 train_time:6847ms step_avg:122.27ms
step:67/1395 train_time:6969ms step_avg:122.27ms
step:68/1395 train_time:7090ms step_avg:122.25ms
step:69/1395 train_time:7213ms step_avg:122.25ms
step:70/1395 train_time:7336ms step_avg:122.26ms
step:71/1395 train_time:7459ms step_avg:122.28ms
step:72/1395 train_time:7581ms step_avg:122.27ms
step:73/1395 train_time:7703ms step_avg:122.27ms
step:74/1395 train_time:7823ms step_avg:122.24ms
step:75/1395 train_time:7945ms step_avg:122.23ms
step:76/1395 train_time:8066ms step_avg:122.22ms
step:77/1395 train_time:8189ms step_avg:122.22ms
step:78/1395 train_time:8312ms step_avg:122.23ms
step:79/1395 train_time:8434ms step_avg:122.23ms
step:80/1395 train_time:8556ms step_avg:122.23ms
step:81/1395 train_time:8679ms step_avg:122.24ms
step:82/1395 train_time:8801ms step_avg:122.24ms
step:83/1395 train_time:8923ms step_avg:122.23ms
step:84/1395 train_time:9049ms step_avg:122.29ms
step:85/1395 train_time:9167ms step_avg:122.23ms
step:86/1395 train_time:9290ms step_avg:122.23ms
step:87/1395 train_time:9411ms step_avg:122.23ms
step:88/1395 train_time:9535ms step_avg:122.24ms
step:89/1395 train_time:9658ms step_avg:122.25ms
step:90/1395 train_time:9780ms step_avg:122.25ms
step:91/1395 train_time:9902ms step_avg:122.25ms
step:92/1395 train_time:10025ms step_avg:122.25ms
step:93/1395 train_time:10148ms step_avg:122.27ms
step:94/1395 train_time:10269ms step_avg:122.25ms
step:95/1395 train_time:10392ms step_avg:122.26ms
step:96/1395 train_time:10514ms step_avg:122.26ms
step:97/1395 train_time:10637ms step_avg:122.27ms
step:98/1395 train_time:10760ms step_avg:122.28ms
step:99/1395 train_time:10880ms step_avg:122.25ms
step:100/1395 train_time:11003ms step_avg:122.25ms
step:101/1395 train_time:11126ms step_avg:122.26ms
step:102/1395 train_time:11248ms step_avg:122.26ms
step:103/1395 train_time:11371ms step_avg:122.27ms
step:104/1395 train_time:11494ms step_avg:122.28ms
step:105/1395 train_time:11616ms step_avg:122.28ms
step:106/1395 train_time:11739ms step_avg:122.28ms
step:107/1395 train_time:11861ms step_avg:122.28ms
step:108/1395 train_time:11984ms step_avg:122.28ms
step:109/1395 train_time:12108ms step_avg:122.30ms
step:110/1395 train_time:12231ms step_avg:122.31ms
step:111/1395 train_time:12354ms step_avg:122.31ms
step:112/1395 train_time:12476ms step_avg:122.32ms
step:113/1395 train_time:12600ms step_avg:122.33ms
step:114/1395 train_time:12724ms step_avg:122.35ms
step:115/1395 train_time:12846ms step_avg:122.34ms
step:116/1395 train_time:12969ms step_avg:122.35ms
step:117/1395 train_time:13092ms step_avg:122.36ms
step:118/1395 train_time:13214ms step_avg:122.35ms
step:119/1395 train_time:13337ms step_avg:122.35ms
step:120/1395 train_time:13459ms step_avg:122.36ms
step:121/1395 train_time:13582ms step_avg:122.36ms
step:122/1395 train_time:13706ms step_avg:122.37ms
step:123/1395 train_time:13828ms step_avg:122.37ms
step:124/1395 train_time:13950ms step_avg:122.37ms
step:125/1395 train_time:14074ms step_avg:122.38ms
step:125/1395 val_loss:4.4066 train_time:14195ms step_avg:123.44ms
step:126/1395 train_time:14217ms step_avg:122.56ms
step:127/1395 train_time:14333ms step_avg:122.50ms
step:128/1395 train_time:14458ms step_avg:122.53ms
step:129/1395 train_time:14582ms step_avg:122.54ms
step:130/1395 train_time:14705ms step_avg:122.54ms
step:131/1395 train_time:14828ms step_avg:122.55ms
step:132/1395 train_time:14951ms step_avg:122.55ms
step:133/1395 train_time:15073ms step_avg:122.55ms
step:134/1395 train_time:15196ms step_avg:122.55ms
step:135/1395 train_time:15319ms step_avg:122.55ms
step:136/1395 train_time:15443ms step_avg:122.56ms
step:137/1395 train_time:15567ms step_avg:122.58ms
step:138/1395 train_time:15690ms step_avg:122.58ms
step:139/1395 train_time:15813ms step_avg:122.58ms
step:140/1395 train_time:15936ms step_avg:122.58ms
step:141/1395 train_time:16059ms step_avg:122.59ms
step:142/1395 train_time:16182ms step_avg:122.59ms
step:143/1395 train_time:16305ms step_avg:122.59ms
step:144/1395 train_time:16429ms step_avg:122.60ms
step:145/1395 train_time:16553ms step_avg:122.62ms
step:146/1395 train_time:16675ms step_avg:122.61ms
step:147/1395 train_time:16798ms step_avg:122.61ms
step:148/1395 train_time:16922ms step_avg:122.62ms
step:149/1395 train_time:17045ms step_avg:122.63ms
step:150/1395 train_time:17168ms step_avg:122.63ms
step:151/1395 train_time:17292ms step_avg:122.63ms
step:152/1395 train_time:17414ms step_avg:122.64ms
step:153/1395 train_time:17538ms step_avg:122.64ms
step:154/1395 train_time:17660ms step_avg:122.64ms
step:155/1395 train_time:17784ms step_avg:122.65ms
step:156/1395 train_time:17908ms step_avg:122.66ms
step:157/1395 train_time:18030ms step_avg:122.66ms
step:158/1395 train_time:18153ms step_avg:122.66ms
step:159/1395 train_time:18277ms step_avg:122.66ms
step:160/1395 train_time:18401ms step_avg:122.67ms
step:161/1395 train_time:18524ms step_avg:122.68ms
step:162/1395 train_time:18648ms step_avg:122.68ms
step:163/1395 train_time:18771ms step_avg:122.68ms
step:164/1395 train_time:18893ms step_avg:122.68ms
step:165/1395 train_time:19017ms step_avg:122.69ms
step:166/1395 train_time:19140ms step_avg:122.69ms
step:167/1395 train_time:19263ms step_avg:122.69ms
step:168/1395 train_time:19386ms step_avg:122.70ms
step:169/1395 train_time:19509ms step_avg:122.70ms
step:170/1395 train_time:19632ms step_avg:122.70ms
step:171/1395 train_time:19755ms step_avg:122.70ms
step:172/1395 train_time:19876ms step_avg:122.69ms
step:173/1395 train_time:19999ms step_avg:122.69ms
step:174/1395 train_time:20124ms step_avg:122.70ms
step:175/1395 train_time:20247ms step_avg:122.71ms
step:176/1395 train_time:20370ms step_avg:122.71ms
step:177/1395 train_time:20494ms step_avg:122.72ms
step:178/1395 train_time:20617ms step_avg:122.72ms
step:179/1395 train_time:20740ms step_avg:122.72ms
step:180/1395 train_time:20862ms step_avg:122.72ms
step:181/1395 train_time:20985ms step_avg:122.72ms
step:182/1395 train_time:21108ms step_avg:122.72ms
step:183/1395 train_time:21230ms step_avg:122.72ms
step:184/1395 train_time:21354ms step_avg:122.72ms
step:185/1395 train_time:21476ms step_avg:122.72ms
step:186/1395 train_time:21599ms step_avg:122.72ms
step:187/1395 train_time:21722ms step_avg:122.73ms
step:188/1395 train_time:21846ms step_avg:122.73ms
step:189/1395 train_time:21969ms step_avg:122.73ms
step:190/1395 train_time:22093ms step_avg:122.74ms
step:191/1395 train_time:22216ms step_avg:122.74ms
step:192/1395 train_time:22339ms step_avg:122.74ms
step:193/1395 train_time:22461ms step_avg:122.74ms
step:194/1395 train_time:22586ms step_avg:122.75ms
step:195/1395 train_time:22709ms step_avg:122.75ms
step:196/1395 train_time:22832ms step_avg:122.76ms
step:197/1395 train_time:22955ms step_avg:122.75ms
step:198/1395 train_time:23078ms step_avg:122.75ms
step:199/1395 train_time:23200ms step_avg:122.75ms
step:200/1395 train_time:23323ms step_avg:122.75ms
step:201/1395 train_time:23447ms step_avg:122.76ms
step:202/1395 train_time:23571ms step_avg:122.76ms
step:203/1395 train_time:23693ms step_avg:122.76ms
step:204/1395 train_time:23815ms step_avg:122.76ms
step:205/1395 train_time:23938ms step_avg:122.76ms
step:206/1395 train_time:24060ms step_avg:122.75ms
step:207/1395 train_time:24183ms step_avg:122.76ms
step:208/1395 train_time:24306ms step_avg:122.76ms
step:209/1395 train_time:24430ms step_avg:122.76ms
step:210/1395 train_time:24554ms step_avg:122.77ms
step:211/1395 train_time:24677ms step_avg:122.77ms
step:212/1395 train_time:24802ms step_avg:122.78ms
step:213/1395 train_time:24925ms step_avg:122.78ms
step:214/1395 train_time:25048ms step_avg:122.78ms
step:215/1395 train_time:25171ms step_avg:122.79ms
step:216/1395 train_time:25294ms step_avg:122.79ms
step:217/1395 train_time:25417ms step_avg:122.79ms
step:218/1395 train_time:25542ms step_avg:122.80ms
step:219/1395 train_time:25665ms step_avg:122.80ms
step:220/1395 train_time:25789ms step_avg:122.80ms
step:221/1395 train_time:25912ms step_avg:122.81ms
step:222/1395 train_time:26035ms step_avg:122.81ms
step:223/1395 train_time:26159ms step_avg:122.81ms
step:224/1395 train_time:26282ms step_avg:122.81ms
step:225/1395 train_time:26405ms step_avg:122.81ms
step:226/1395 train_time:26529ms step_avg:122.82ms
step:227/1395 train_time:26652ms step_avg:122.82ms
step:228/1395 train_time:26776ms step_avg:122.83ms
step:229/1395 train_time:26900ms step_avg:122.83ms
step:230/1395 train_time:27023ms step_avg:122.83ms
step:231/1395 train_time:27147ms step_avg:122.84ms
step:232/1395 train_time:27271ms step_avg:122.84ms
step:233/1395 train_time:27395ms step_avg:122.85ms
step:234/1395 train_time:27519ms step_avg:122.85ms
step:235/1395 train_time:27642ms step_avg:122.85ms
step:236/1395 train_time:27765ms step_avg:122.86ms
step:237/1395 train_time:27889ms step_avg:122.86ms
step:238/1395 train_time:28012ms step_avg:122.86ms
step:239/1395 train_time:28137ms step_avg:122.87ms
step:240/1395 train_time:28260ms step_avg:122.87ms
step:241/1395 train_time:28383ms step_avg:122.87ms
step:242/1395 train_time:28508ms step_avg:122.88ms
step:243/1395 train_time:28631ms step_avg:122.88ms
step:244/1395 train_time:28754ms step_avg:122.88ms
step:245/1395 train_time:28879ms step_avg:122.89ms
step:246/1395 train_time:29003ms step_avg:122.89ms
step:247/1395 train_time:29126ms step_avg:122.90ms
step:248/1395 train_time:29250ms step_avg:122.90ms
step:249/1395 train_time:29373ms step_avg:122.90ms
step:250/1395 train_time:29498ms step_avg:122.91ms
step:250/1395 val_loss:3.9834 train_time:29620ms step_avg:123.42ms
step:251/1395 train_time:29642ms step_avg:123.00ms
step:252/1395 train_time:29761ms step_avg:122.98ms
step:253/1395 train_time:29889ms step_avg:123.00ms
step:254/1395 train_time:30012ms step_avg:123.00ms
step:255/1395 train_time:30135ms step_avg:123.00ms
step:256/1395 train_time:30258ms step_avg:123.00ms
step:257/1395 train_time:30380ms step_avg:123.00ms
step:258/1395 train_time:30503ms step_avg:123.00ms
step:259/1395 train_time:30626ms step_avg:123.00ms
step:260/1395 train_time:30749ms step_avg:123.00ms
step:261/1395 train_time:30874ms step_avg:123.00ms
step:262/1395 train_time:30998ms step_avg:123.01ms
step:263/1395 train_time:31123ms step_avg:123.01ms
step:264/1395 train_time:31247ms step_avg:123.02ms
step:265/1395 train_time:31372ms step_avg:123.03ms
step:266/1395 train_time:31496ms step_avg:123.03ms
step:267/1395 train_time:31620ms step_avg:123.03ms
step:268/1395 train_time:31742ms step_avg:123.03ms
step:269/1395 train_time:31866ms step_avg:123.03ms
step:270/1395 train_time:31990ms step_avg:123.04ms
step:271/1395 train_time:32114ms step_avg:123.04ms
step:272/1395 train_time:32237ms step_avg:123.04ms
step:273/1395 train_time:32360ms step_avg:123.04ms
step:274/1395 train_time:32484ms step_avg:123.04ms
step:275/1395 train_time:32608ms step_avg:123.05ms
step:276/1395 train_time:32730ms step_avg:123.05ms
step:277/1395 train_time:32854ms step_avg:123.05ms
step:278/1395 train_time:32977ms step_avg:123.05ms
step:279/1395 train_time:33101ms step_avg:123.05ms
step:280/1395 train_time:33226ms step_avg:123.06ms
step:281/1395 train_time:33349ms step_avg:123.06ms
step:282/1395 train_time:33473ms step_avg:123.06ms
step:283/1395 train_time:33596ms step_avg:123.06ms
step:284/1395 train_time:33720ms step_avg:123.07ms
step:285/1395 train_time:33844ms step_avg:123.07ms
step:286/1395 train_time:33968ms step_avg:123.07ms
step:287/1395 train_time:34091ms step_avg:123.07ms
step:288/1395 train_time:34216ms step_avg:123.08ms
step:289/1395 train_time:34340ms step_avg:123.08ms
step:290/1395 train_time:34464ms step_avg:123.08ms
step:291/1395 train_time:34589ms step_avg:123.09ms
step:292/1395 train_time:34712ms step_avg:123.09ms
step:293/1395 train_time:34835ms step_avg:123.09ms
step:294/1395 train_time:34958ms step_avg:123.09ms
step:295/1395 train_time:35082ms step_avg:123.09ms
step:296/1395 train_time:35204ms step_avg:123.09ms
step:297/1395 train_time:35331ms step_avg:123.10ms
step:298/1395 train_time:35451ms step_avg:123.09ms
step:299/1395 train_time:35574ms step_avg:123.10ms
step:300/1395 train_time:35698ms step_avg:123.10ms
step:301/1395 train_time:35822ms step_avg:123.10ms
step:302/1395 train_time:35949ms step_avg:123.11ms
step:303/1395 train_time:36072ms step_avg:123.11ms
step:304/1395 train_time:36196ms step_avg:123.11ms
step:305/1395 train_time:36319ms step_avg:123.11ms
step:306/1395 train_time:36442ms step_avg:123.11ms
step:307/1395 train_time:36564ms step_avg:123.11ms
step:308/1395 train_time:36689ms step_avg:123.12ms
step:309/1395 train_time:36812ms step_avg:123.12ms
step:310/1395 train_time:36936ms step_avg:123.12ms
step:311/1395 train_time:37060ms step_avg:123.12ms
step:312/1395 train_time:37185ms step_avg:123.13ms
step:313/1395 train_time:37311ms step_avg:123.14ms
step:314/1395 train_time:37437ms step_avg:123.15ms
step:315/1395 train_time:37563ms step_avg:123.16ms
step:316/1395 train_time:37689ms step_avg:123.17ms
step:317/1395 train_time:37816ms step_avg:123.18ms
step:318/1395 train_time:37941ms step_avg:123.19ms
step:319/1395 train_time:38067ms step_avg:123.19ms
step:320/1395 train_time:38193ms step_avg:123.20ms
step:321/1395 train_time:38318ms step_avg:123.21ms
step:322/1395 train_time:38445ms step_avg:123.22ms
step:323/1395 train_time:38571ms step_avg:123.23ms
step:324/1395 train_time:38697ms step_avg:123.24ms
step:325/1395 train_time:38823ms step_avg:123.25ms
step:326/1395 train_time:38949ms step_avg:123.26ms
step:327/1395 train_time:39076ms step_avg:123.27ms
step:328/1395 train_time:39202ms step_avg:123.28ms
step:329/1395 train_time:39329ms step_avg:123.29ms
step:330/1395 train_time:39455ms step_avg:123.30ms
step:331/1395 train_time:39582ms step_avg:123.31ms
step:332/1395 train_time:39708ms step_avg:123.32ms
step:333/1395 train_time:39833ms step_avg:123.32ms
step:334/1395 train_time:39958ms step_avg:123.33ms
step:335/1395 train_time:40084ms step_avg:123.33ms
step:336/1395 train_time:40210ms step_avg:123.34ms
step:337/1395 train_time:40336ms step_avg:123.35ms
step:338/1395 train_time:40462ms step_avg:123.36ms
step:339/1395 train_time:40589ms step_avg:123.37ms
step:340/1395 train_time:40715ms step_avg:123.38ms
step:341/1395 train_time:40841ms step_avg:123.39ms
step:342/1395 train_time:40967ms step_avg:123.40ms
step:343/1395 train_time:41093ms step_avg:123.40ms
step:344/1395 train_time:41220ms step_avg:123.41ms
step:345/1395 train_time:41347ms step_avg:123.42ms
step:346/1395 train_time:41472ms step_avg:123.43ms
step:347/1395 train_time:41598ms step_avg:123.44ms
step:348/1395 train_time:41724ms step_avg:123.44ms
step:349/1395 train_time:41851ms step_avg:123.45ms
step:350/1395 train_time:41975ms step_avg:123.46ms
step:351/1395 train_time:42102ms step_avg:123.47ms
step:352/1395 train_time:42228ms step_avg:123.47ms
step:353/1395 train_time:42354ms step_avg:123.48ms
step:354/1395 train_time:42480ms step_avg:123.49ms
step:355/1395 train_time:42607ms step_avg:123.50ms
step:356/1395 train_time:42733ms step_avg:123.51ms
step:357/1395 train_time:42859ms step_avg:123.51ms
step:358/1395 train_time:42987ms step_avg:123.52ms
step:359/1395 train_time:43113ms step_avg:123.53ms
step:360/1395 train_time:43238ms step_avg:123.54ms
step:361/1395 train_time:43364ms step_avg:123.54ms
step:362/1395 train_time:43490ms step_avg:123.55ms
step:363/1395 train_time:43615ms step_avg:123.56ms
step:364/1395 train_time:43741ms step_avg:123.56ms
step:365/1395 train_time:43867ms step_avg:123.57ms
step:366/1395 train_time:43993ms step_avg:123.58ms
step:367/1395 train_time:44120ms step_avg:123.59ms
step:368/1395 train_time:44246ms step_avg:123.59ms
step:369/1395 train_time:44373ms step_avg:123.60ms
step:370/1395 train_time:44499ms step_avg:123.61ms
step:371/1395 train_time:44625ms step_avg:123.61ms
step:372/1395 train_time:44752ms step_avg:123.62ms
step:373/1395 train_time:44877ms step_avg:123.63ms
step:374/1395 train_time:45003ms step_avg:123.64ms
step:375/1395 train_time:45131ms step_avg:123.65ms
step:375/1395 val_loss:3.7884 train_time:45255ms step_avg:123.99ms
step:376/1395 train_time:45276ms step_avg:123.70ms
step:377/1395 train_time:45399ms step_avg:123.70ms
step:378/1395 train_time:45528ms step_avg:123.72ms
step:379/1395 train_time:45653ms step_avg:123.72ms
step:380/1395 train_time:45779ms step_avg:123.73ms
step:381/1395 train_time:45905ms step_avg:123.73ms
step:382/1395 train_time:46030ms step_avg:123.74ms
step:383/1395 train_time:46154ms step_avg:123.74ms
step:384/1395 train_time:46280ms step_avg:123.74ms
step:385/1395 train_time:46406ms step_avg:123.75ms
step:386/1395 train_time:46534ms step_avg:123.76ms
step:387/1395 train_time:46661ms step_avg:123.77ms
step:388/1395 train_time:46788ms step_avg:123.78ms
step:389/1395 train_time:46914ms step_avg:123.78ms
step:390/1395 train_time:47039ms step_avg:123.79ms
step:391/1395 train_time:47164ms step_avg:123.79ms
step:392/1395 train_time:47289ms step_avg:123.79ms
step:393/1395 train_time:47414ms step_avg:123.80ms
step:394/1395 train_time:47540ms step_avg:123.80ms
step:395/1395 train_time:47667ms step_avg:123.81ms
step:396/1395 train_time:47793ms step_avg:123.82ms
step:397/1395 train_time:47918ms step_avg:123.82ms
step:398/1395 train_time:48043ms step_avg:123.82ms
step:399/1395 train_time:48169ms step_avg:123.83ms
step:400/1395 train_time:48296ms step_avg:123.84ms
step:401/1395 train_time:48422ms step_avg:123.84ms
step:402/1395 train_time:48548ms step_avg:123.85ms
step:403/1395 train_time:48674ms step_avg:123.85ms
step:404/1395 train_time:48800ms step_avg:123.86ms
step:405/1395 train_time:48926ms step_avg:123.86ms
step:406/1395 train_time:49051ms step_avg:123.87ms
step:407/1395 train_time:49177ms step_avg:123.87ms
step:408/1395 train_time:49304ms step_avg:123.88ms
step:409/1395 train_time:49429ms step_avg:123.88ms
step:410/1395 train_time:49555ms step_avg:123.89ms
step:411/1395 train_time:49681ms step_avg:123.89ms
step:412/1395 train_time:49807ms step_avg:123.90ms
step:413/1395 train_time:49933ms step_avg:123.90ms
step:414/1395 train_time:50060ms step_avg:123.91ms
step:415/1395 train_time:50186ms step_avg:123.92ms
step:416/1395 train_time:50312ms step_avg:123.92ms
step:417/1395 train_time:50439ms step_avg:123.93ms
step:418/1395 train_time:50565ms step_avg:123.93ms
step:419/1395 train_time:50692ms step_avg:123.94ms
step:420/1395 train_time:50819ms step_avg:123.95ms
step:421/1395 train_time:50945ms step_avg:123.95ms
step:422/1395 train_time:51071ms step_avg:123.96ms
step:423/1395 train_time:51198ms step_avg:123.97ms
step:424/1395 train_time:51324ms step_avg:123.97ms
step:425/1395 train_time:51449ms step_avg:123.97ms
step:426/1395 train_time:51577ms step_avg:123.98ms
step:427/1395 train_time:51703ms step_avg:123.99ms
step:428/1395 train_time:51829ms step_avg:123.99ms
step:429/1395 train_time:51954ms step_avg:124.00ms
step:430/1395 train_time:52080ms step_avg:124.00ms
step:431/1395 train_time:52206ms step_avg:124.01ms
step:432/1395 train_time:52332ms step_avg:124.01ms
step:433/1395 train_time:52458ms step_avg:124.01ms
step:434/1395 train_time:52584ms step_avg:124.02ms
step:435/1395 train_time:52709ms step_avg:124.02ms
step:436/1395 train_time:52835ms step_avg:124.03ms
step:437/1395 train_time:52961ms step_avg:124.03ms
step:438/1395 train_time:53089ms step_avg:124.04ms
step:439/1395 train_time:53215ms step_avg:124.04ms
step:440/1395 train_time:53341ms step_avg:124.05ms
step:441/1395 train_time:53467ms step_avg:124.05ms
step:442/1395 train_time:53594ms step_avg:124.06ms
step:443/1395 train_time:53720ms step_avg:124.06ms
step:444/1395 train_time:53845ms step_avg:124.07ms
step:445/1395 train_time:53972ms step_avg:124.07ms
step:446/1395 train_time:54099ms step_avg:124.08ms
step:447/1395 train_time:54226ms step_avg:124.09ms
step:448/1395 train_time:54352ms step_avg:124.09ms
step:449/1395 train_time:54479ms step_avg:124.10ms
step:450/1395 train_time:54605ms step_avg:124.10ms
step:451/1395 train_time:54731ms step_avg:124.11ms
step:452/1395 train_time:54858ms step_avg:124.11ms
step:453/1395 train_time:54984ms step_avg:124.12ms
step:454/1395 train_time:55110ms step_avg:124.12ms
step:455/1395 train_time:55236ms step_avg:124.13ms
step:456/1395 train_time:55363ms step_avg:124.13ms
step:457/1395 train_time:55489ms step_avg:124.14ms
step:458/1395 train_time:55616ms step_avg:124.14ms
step:459/1395 train_time:55742ms step_avg:124.15ms
step:460/1395 train_time:55867ms step_avg:124.15ms
step:461/1395 train_time:55993ms step_avg:124.15ms
step:462/1395 train_time:56119ms step_avg:124.16ms
step:463/1395 train_time:56246ms step_avg:124.16ms
step:464/1395 train_time:56373ms step_avg:124.17ms
step:465/1395 train_time:56500ms step_avg:124.18ms
step:466/1395 train_time:56626ms step_avg:124.18ms
step:467/1395 train_time:56753ms step_avg:124.19ms
step:468/1395 train_time:56879ms step_avg:124.19ms
step:469/1395 train_time:57005ms step_avg:124.19ms
step:470/1395 train_time:57131ms step_avg:124.20ms
step:471/1395 train_time:57257ms step_avg:124.20ms
step:472/1395 train_time:57384ms step_avg:124.21ms
step:473/1395 train_time:57510ms step_avg:124.21ms
step:474/1395 train_time:57638ms step_avg:124.22ms
step:475/1395 train_time:57763ms step_avg:124.22ms
step:476/1395 train_time:57890ms step_avg:124.23ms
step:477/1395 train_time:58019ms step_avg:124.24ms
step:478/1395 train_time:58145ms step_avg:124.24ms
step:479/1395 train_time:58271ms step_avg:124.24ms
step:480/1395 train_time:58396ms step_avg:124.25ms
step:481/1395 train_time:58523ms step_avg:124.25ms
step:482/1395 train_time:58648ms step_avg:124.26ms
step:483/1395 train_time:58775ms step_avg:124.26ms
step:484/1395 train_time:58901ms step_avg:124.26ms
step:485/1395 train_time:59028ms step_avg:124.27ms
step:486/1395 train_time:59154ms step_avg:124.27ms
step:487/1395 train_time:59281ms step_avg:124.28ms
step:488/1395 train_time:59407ms step_avg:124.28ms
step:489/1395 train_time:59534ms step_avg:124.29ms
step:490/1395 train_time:59660ms step_avg:124.29ms
step:491/1395 train_time:59785ms step_avg:124.29ms
step:492/1395 train_time:59911ms step_avg:124.30ms
step:493/1395 train_time:60037ms step_avg:124.30ms
step:494/1395 train_time:60163ms step_avg:124.30ms
step:495/1395 train_time:60291ms step_avg:124.31ms
step:496/1395 train_time:60418ms step_avg:124.32ms
step:497/1395 train_time:60544ms step_avg:124.32ms
step:498/1395 train_time:60669ms step_avg:124.32ms
step:499/1395 train_time:60796ms step_avg:124.33ms
step:500/1395 train_time:60922ms step_avg:124.33ms
step:500/1395 val_loss:3.6704 train_time:61047ms step_avg:124.59ms
step:501/1395 train_time:61069ms step_avg:124.38ms
step:502/1395 train_time:61189ms step_avg:124.37ms
step:503/1395 train_time:61321ms step_avg:124.38ms
step:504/1395 train_time:61447ms step_avg:124.39ms
step:505/1395 train_time:61573ms step_avg:124.39ms
step:506/1395 train_time:61698ms step_avg:124.39ms
step:507/1395 train_time:61824ms step_avg:124.39ms
step:508/1395 train_time:61950ms step_avg:124.40ms
step:509/1395 train_time:62076ms step_avg:124.40ms
step:510/1395 train_time:62203ms step_avg:124.41ms
step:511/1395 train_time:62330ms step_avg:124.41ms
step:512/1395 train_time:62457ms step_avg:124.42ms
step:513/1395 train_time:62584ms step_avg:124.42ms
step:514/1395 train_time:62710ms step_avg:124.42ms
step:515/1395 train_time:62835ms step_avg:124.43ms
step:516/1395 train_time:62962ms step_avg:124.43ms
step:517/1395 train_time:63088ms step_avg:124.43ms
step:518/1395 train_time:63214ms step_avg:124.44ms
step:519/1395 train_time:63343ms step_avg:124.45ms
step:520/1395 train_time:63472ms step_avg:124.45ms
step:521/1395 train_time:63600ms step_avg:124.46ms
step:522/1395 train_time:63729ms step_avg:124.47ms
step:523/1395 train_time:63858ms step_avg:124.48ms
step:524/1395 train_time:63986ms step_avg:124.49ms
step:525/1395 train_time:64114ms step_avg:124.49ms
step:526/1395 train_time:64243ms step_avg:124.50ms
step:527/1395 train_time:64371ms step_avg:124.51ms
step:528/1395 train_time:64501ms step_avg:124.52ms
step:529/1395 train_time:64629ms step_avg:124.53ms
step:530/1395 train_time:64757ms step_avg:124.53ms
step:531/1395 train_time:64886ms step_avg:124.54ms
step:532/1395 train_time:65014ms step_avg:124.55ms
step:533/1395 train_time:65143ms step_avg:124.56ms
step:534/1395 train_time:65272ms step_avg:124.57ms
step:535/1395 train_time:65402ms step_avg:124.58ms
step:536/1395 train_time:65531ms step_avg:124.58ms
step:537/1395 train_time:65660ms step_avg:124.59ms
step:538/1395 train_time:65787ms step_avg:124.60ms
step:539/1395 train_time:65916ms step_avg:124.61ms
step:540/1395 train_time:66045ms step_avg:124.61ms
step:541/1395 train_time:66174ms step_avg:124.62ms
step:542/1395 train_time:66302ms step_avg:124.63ms
step:543/1395 train_time:66432ms step_avg:124.64ms
step:544/1395 train_time:66559ms step_avg:124.64ms
step:545/1395 train_time:66688ms step_avg:124.65ms
step:546/1395 train_time:66817ms step_avg:124.66ms
step:547/1395 train_time:66946ms step_avg:124.67ms
step:548/1395 train_time:67074ms step_avg:124.67ms
step:549/1395 train_time:67203ms step_avg:124.68ms
step:550/1395 train_time:67331ms step_avg:124.69ms
step:551/1395 train_time:67459ms step_avg:124.69ms
step:552/1395 train_time:67588ms step_avg:124.70ms
step:553/1395 train_time:67718ms step_avg:124.71ms
step:554/1395 train_time:67847ms step_avg:124.72ms
step:555/1395 train_time:67975ms step_avg:124.72ms
step:556/1395 train_time:68103ms step_avg:124.73ms
step:557/1395 train_time:68233ms step_avg:124.74ms
step:558/1395 train_time:68362ms step_avg:124.75ms
step:559/1395 train_time:68490ms step_avg:124.75ms
step:560/1395 train_time:68619ms step_avg:124.76ms
step:561/1395 train_time:68747ms step_avg:124.77ms
step:562/1395 train_time:68875ms step_avg:124.77ms
step:563/1395 train_time:69004ms step_avg:124.78ms
step:564/1395 train_time:69131ms step_avg:124.79ms
step:565/1395 train_time:69260ms step_avg:124.79ms
step:566/1395 train_time:69389ms step_avg:124.80ms
step:567/1395 train_time:69519ms step_avg:124.81ms
step:568/1395 train_time:69647ms step_avg:124.81ms
step:569/1395 train_time:69775ms step_avg:124.82ms
step:570/1395 train_time:69903ms step_avg:124.83ms
step:571/1395 train_time:70032ms step_avg:124.83ms
step:572/1395 train_time:70160ms step_avg:124.84ms
step:573/1395 train_time:70288ms step_avg:124.85ms
step:574/1395 train_time:70417ms step_avg:124.85ms
step:575/1395 train_time:70547ms step_avg:124.86ms
step:576/1395 train_time:70674ms step_avg:124.87ms
step:577/1395 train_time:70802ms step_avg:124.87ms
step:578/1395 train_time:70930ms step_avg:124.88ms
step:579/1395 train_time:71059ms step_avg:124.88ms
step:580/1395 train_time:71188ms step_avg:124.89ms
step:581/1395 train_time:71316ms step_avg:124.90ms
step:582/1395 train_time:71445ms step_avg:124.90ms
step:583/1395 train_time:71573ms step_avg:124.91ms
step:584/1395 train_time:71702ms step_avg:124.92ms
step:585/1395 train_time:71830ms step_avg:124.92ms
step:586/1395 train_time:71959ms step_avg:124.93ms
step:587/1395 train_time:72087ms step_avg:124.93ms
step:588/1395 train_time:72215ms step_avg:124.94ms
step:589/1395 train_time:72344ms step_avg:124.95ms
step:590/1395 train_time:72472ms step_avg:124.95ms
step:591/1395 train_time:72601ms step_avg:124.96ms
step:592/1395 train_time:72728ms step_avg:124.96ms
step:593/1395 train_time:72858ms step_avg:124.97ms
step:594/1395 train_time:72986ms step_avg:124.98ms
step:595/1395 train_time:73114ms step_avg:124.98ms
step:596/1395 train_time:73243ms step_avg:124.99ms
step:597/1395 train_time:73372ms step_avg:124.99ms
step:598/1395 train_time:73500ms step_avg:125.00ms
step:599/1395 train_time:73629ms step_avg:125.01ms
step:600/1395 train_time:73758ms step_avg:125.01ms
step:601/1395 train_time:73886ms step_avg:125.02ms
step:602/1395 train_time:74014ms step_avg:125.02ms
step:603/1395 train_time:74142ms step_avg:125.03ms
step:604/1395 train_time:74270ms step_avg:125.03ms
step:605/1395 train_time:74399ms step_avg:125.04ms
step:606/1395 train_time:74527ms step_avg:125.04ms
step:607/1395 train_time:74656ms step_avg:125.05ms
step:608/1395 train_time:74785ms step_avg:125.06ms
step:609/1395 train_time:74913ms step_avg:125.06ms
step:610/1395 train_time:75041ms step_avg:125.07ms
step:611/1395 train_time:75170ms step_avg:125.08ms
step:612/1395 train_time:75299ms step_avg:125.08ms
step:613/1395 train_time:75427ms step_avg:125.09ms
step:614/1395 train_time:75556ms step_avg:125.09ms
step:615/1395 train_time:75685ms step_avg:125.10ms
step:616/1395 train_time:75814ms step_avg:125.11ms
step:617/1395 train_time:75942ms step_avg:125.11ms
step:618/1395 train_time:76071ms step_avg:125.12ms
step:619/1395 train_time:76200ms step_avg:125.12ms
step:620/1395 train_time:76327ms step_avg:125.13ms
step:621/1395 train_time:76455ms step_avg:125.13ms
step:622/1395 train_time:76585ms step_avg:125.14ms
step:623/1395 train_time:76713ms step_avg:125.14ms
step:624/1395 train_time:76842ms step_avg:125.15ms
step:625/1395 train_time:76970ms step_avg:125.16ms
step:625/1395 val_loss:3.5868 train_time:77099ms step_avg:125.36ms
step:626/1395 train_time:77120ms step_avg:125.19ms
step:627/1395 train_time:77243ms step_avg:125.19ms
step:628/1395 train_time:77373ms step_avg:125.20ms
step:629/1395 train_time:77501ms step_avg:125.20ms
step:630/1395 train_time:77630ms step_avg:125.21ms
step:631/1395 train_time:77758ms step_avg:125.21ms
step:632/1395 train_time:77886ms step_avg:125.22ms
step:633/1395 train_time:78015ms step_avg:125.22ms
step:634/1395 train_time:78143ms step_avg:125.23ms
step:635/1395 train_time:78276ms step_avg:125.24ms
step:636/1395 train_time:78404ms step_avg:125.25ms
step:637/1395 train_time:78533ms step_avg:125.25ms
step:638/1395 train_time:78662ms step_avg:125.26ms
step:639/1395 train_time:78790ms step_avg:125.26ms
step:640/1395 train_time:78918ms step_avg:125.27ms
step:641/1395 train_time:79047ms step_avg:125.27ms
step:642/1395 train_time:79175ms step_avg:125.28ms
step:643/1395 train_time:79303ms step_avg:125.28ms
step:644/1395 train_time:79433ms step_avg:125.29ms
step:645/1395 train_time:79562ms step_avg:125.29ms
step:646/1395 train_time:79691ms step_avg:125.30ms
step:647/1395 train_time:79819ms step_avg:125.30ms
step:648/1395 train_time:79949ms step_avg:125.31ms
step:649/1395 train_time:80079ms step_avg:125.32ms
step:650/1395 train_time:80208ms step_avg:125.33ms
step:651/1395 train_time:80338ms step_avg:125.33ms
step:652/1395 train_time:80466ms step_avg:125.34ms
step:653/1395 train_time:80596ms step_avg:125.34ms
step:654/1395 train_time:80725ms step_avg:125.35ms
step:655/1395 train_time:80854ms step_avg:125.35ms
step:656/1395 train_time:80982ms step_avg:125.36ms
step:657/1395 train_time:81111ms step_avg:125.36ms
step:658/1395 train_time:81239ms step_avg:125.37ms
step:659/1395 train_time:81369ms step_avg:125.38ms
step:660/1395 train_time:81501ms step_avg:125.39ms
step:661/1395 train_time:81626ms step_avg:125.39ms
step:662/1395 train_time:81756ms step_avg:125.39ms
step:663/1395 train_time:81884ms step_avg:125.40ms
step:664/1395 train_time:82014ms step_avg:125.40ms
step:665/1395 train_time:82141ms step_avg:125.41ms
step:666/1395 train_time:82270ms step_avg:125.41ms
step:667/1395 train_time:82399ms step_avg:125.42ms
step:668/1395 train_time:82528ms step_avg:125.42ms
step:669/1395 train_time:82657ms step_avg:125.43ms
step:670/1395 train_time:82787ms step_avg:125.43ms
step:671/1395 train_time:82916ms step_avg:125.44ms
step:672/1395 train_time:83044ms step_avg:125.44ms
step:673/1395 train_time:83174ms step_avg:125.45ms
step:674/1395 train_time:83304ms step_avg:125.46ms
step:675/1395 train_time:83431ms step_avg:125.46ms
step:676/1395 train_time:83559ms step_avg:125.46ms
step:677/1395 train_time:83688ms step_avg:125.47ms
step:678/1395 train_time:83817ms step_avg:125.47ms
step:679/1395 train_time:83945ms step_avg:125.48ms
step:680/1395 train_time:84074ms step_avg:125.48ms
step:681/1395 train_time:84203ms step_avg:125.49ms
step:682/1395 train_time:84332ms step_avg:125.49ms
step:683/1395 train_time:84460ms step_avg:125.50ms
step:684/1395 train_time:84589ms step_avg:125.50ms
step:685/1395 train_time:84718ms step_avg:125.51ms
step:686/1395 train_time:84846ms step_avg:125.51ms
step:687/1395 train_time:84974ms step_avg:125.52ms
step:688/1395 train_time:85103ms step_avg:125.52ms
step:689/1395 train_time:85233ms step_avg:125.53ms
step:690/1395 train_time:85362ms step_avg:125.53ms
step:691/1395 train_time:85492ms step_avg:125.54ms
step:692/1395 train_time:85621ms step_avg:125.54ms
step:693/1395 train_time:85750ms step_avg:125.55ms
step:694/1395 train_time:85878ms step_avg:125.55ms
step:695/1395 train_time:86007ms step_avg:125.56ms
step:696/1395 train_time:86137ms step_avg:125.56ms
step:697/1395 train_time:86265ms step_avg:125.57ms
step:698/1395 train_time:86394ms step_avg:125.57ms
step:699/1395 train_time:86522ms step_avg:125.58ms
step:700/1395 train_time:86652ms step_avg:125.58ms
step:701/1395 train_time:86781ms step_avg:125.59ms
step:702/1395 train_time:86910ms step_avg:125.59ms
step:703/1395 train_time:87038ms step_avg:125.60ms
step:704/1395 train_time:87166ms step_avg:125.60ms
step:705/1395 train_time:87295ms step_avg:125.60ms
step:706/1395 train_time:87424ms step_avg:125.61ms
step:707/1395 train_time:87552ms step_avg:125.61ms
step:708/1395 train_time:87681ms step_avg:125.62ms
step:709/1395 train_time:87809ms step_avg:125.62ms
step:710/1395 train_time:87937ms step_avg:125.62ms
step:711/1395 train_time:88065ms step_avg:125.63ms
step:712/1395 train_time:88194ms step_avg:125.63ms
step:713/1395 train_time:88323ms step_avg:125.64ms
step:714/1395 train_time:88452ms step_avg:125.64ms
step:715/1395 train_time:88580ms step_avg:125.65ms
step:716/1395 train_time:88710ms step_avg:125.65ms
step:717/1395 train_time:88838ms step_avg:125.66ms
step:718/1395 train_time:88967ms step_avg:125.66ms
step:719/1395 train_time:89096ms step_avg:125.66ms
step:720/1395 train_time:89225ms step_avg:125.67ms
step:721/1395 train_time:89355ms step_avg:125.68ms
step:722/1395 train_time:89484ms step_avg:125.68ms
step:723/1395 train_time:89614ms step_avg:125.69ms
step:724/1395 train_time:89742ms step_avg:125.69ms
step:725/1395 train_time:89871ms step_avg:125.69ms
step:726/1395 train_time:90002ms step_avg:125.70ms
step:727/1395 train_time:90133ms step_avg:125.71ms
step:728/1395 train_time:90264ms step_avg:125.72ms
step:729/1395 train_time:90396ms step_avg:125.72ms
step:730/1395 train_time:90527ms step_avg:125.73ms
step:731/1395 train_time:90658ms step_avg:125.74ms
step:732/1395 train_time:90788ms step_avg:125.74ms
step:733/1395 train_time:90918ms step_avg:125.75ms
step:734/1395 train_time:91048ms step_avg:125.76ms
step:735/1395 train_time:91179ms step_avg:125.76ms
step:736/1395 train_time:91310ms step_avg:125.77ms
step:737/1395 train_time:91441ms step_avg:125.78ms
step:738/1395 train_time:91572ms step_avg:125.79ms
step:739/1395 train_time:91702ms step_avg:125.79ms
step:740/1395 train_time:91834ms step_avg:125.80ms
step:741/1395 train_time:91965ms step_avg:125.81ms
step:742/1395 train_time:92096ms step_avg:125.81ms
step:743/1395 train_time:92226ms step_avg:125.82ms
step:744/1395 train_time:92356ms step_avg:125.83ms
step:745/1395 train_time:92487ms step_avg:125.83ms
step:746/1395 train_time:92617ms step_avg:125.84ms
step:747/1395 train_time:92748ms step_avg:125.85ms
step:748/1395 train_time:92879ms step_avg:125.85ms
step:749/1395 train_time:93009ms step_avg:125.86ms
step:750/1395 train_time:93141ms step_avg:125.87ms
step:750/1395 val_loss:3.5316 train_time:93271ms step_avg:126.04ms
step:751/1395 train_time:93291ms step_avg:125.90ms
step:752/1395 train_time:93415ms step_avg:125.90ms
step:753/1395 train_time:93548ms step_avg:125.91ms
step:754/1395 train_time:93677ms step_avg:125.91ms
step:755/1395 train_time:93807ms step_avg:125.91ms
step:756/1395 train_time:93936ms step_avg:125.92ms
step:757/1395 train_time:94066ms step_avg:125.93ms
step:758/1395 train_time:94197ms step_avg:125.93ms
step:759/1395 train_time:94328ms step_avg:125.94ms
step:760/1395 train_time:94462ms step_avg:125.95ms
step:761/1395 train_time:94592ms step_avg:125.96ms
step:762/1395 train_time:94723ms step_avg:125.96ms
step:763/1395 train_time:94853ms step_avg:125.97ms
step:764/1395 train_time:94984ms step_avg:125.97ms
step:765/1395 train_time:95113ms step_avg:125.98ms
step:766/1395 train_time:95245ms step_avg:125.99ms
step:767/1395 train_time:95376ms step_avg:125.99ms
step:768/1395 train_time:95507ms step_avg:126.00ms
step:769/1395 train_time:95637ms step_avg:126.00ms
step:770/1395 train_time:95767ms step_avg:126.01ms
step:771/1395 train_time:95898ms step_avg:126.02ms
step:772/1395 train_time:96029ms step_avg:126.02ms
step:773/1395 train_time:96160ms step_avg:126.03ms
step:774/1395 train_time:96291ms step_avg:126.04ms
step:775/1395 train_time:96421ms step_avg:126.04ms
step:776/1395 train_time:96553ms step_avg:126.05ms
step:777/1395 train_time:96684ms step_avg:126.05ms
step:778/1395 train_time:96814ms step_avg:126.06ms
step:779/1395 train_time:96945ms step_avg:126.07ms
step:780/1395 train_time:97076ms step_avg:126.07ms
step:781/1395 train_time:97206ms step_avg:126.08ms
step:782/1395 train_time:97336ms step_avg:126.08ms
step:783/1395 train_time:97467ms step_avg:126.09ms
step:784/1395 train_time:97596ms step_avg:126.09ms
step:785/1395 train_time:97727ms step_avg:126.10ms
step:786/1395 train_time:97858ms step_avg:126.11ms
step:787/1395 train_time:97989ms step_avg:126.11ms
step:788/1395 train_time:98119ms step_avg:126.12ms
step:789/1395 train_time:98249ms step_avg:126.12ms
step:790/1395 train_time:98380ms step_avg:126.13ms
step:791/1395 train_time:98510ms step_avg:126.13ms
step:792/1395 train_time:98640ms step_avg:126.14ms
step:793/1395 train_time:98771ms step_avg:126.14ms
step:794/1395 train_time:98901ms step_avg:126.15ms
step:795/1395 train_time:99034ms step_avg:126.16ms
step:796/1395 train_time:99165ms step_avg:126.16ms
step:797/1395 train_time:99295ms step_avg:126.17ms
step:798/1395 train_time:99425ms step_avg:126.17ms
step:799/1395 train_time:99556ms step_avg:126.18ms
step:800/1395 train_time:99687ms step_avg:126.19ms
step:801/1395 train_time:99817ms step_avg:126.19ms
step:802/1395 train_time:99946ms step_avg:126.20ms
step:803/1395 train_time:100077ms step_avg:126.20ms
step:804/1395 train_time:100207ms step_avg:126.21ms
step:805/1395 train_time:100338ms step_avg:126.21ms
step:806/1395 train_time:100470ms step_avg:126.22ms
step:807/1395 train_time:100599ms step_avg:126.22ms
step:808/1395 train_time:100731ms step_avg:126.23ms
step:809/1395 train_time:100861ms step_avg:126.23ms
step:810/1395 train_time:100992ms step_avg:126.24ms
step:811/1395 train_time:101123ms step_avg:126.25ms
step:812/1395 train_time:101254ms step_avg:126.25ms
step:813/1395 train_time:101385ms step_avg:126.26ms
step:814/1395 train_time:101514ms step_avg:126.26ms
step:815/1395 train_time:101644ms step_avg:126.27ms
step:816/1395 train_time:101776ms step_avg:126.27ms
step:817/1395 train_time:101906ms step_avg:126.28ms
step:818/1395 train_time:102037ms step_avg:126.28ms
step:819/1395 train_time:102168ms step_avg:126.29ms
step:820/1395 train_time:102297ms step_avg:126.29ms
step:821/1395 train_time:102427ms step_avg:126.30ms
step:822/1395 train_time:102557ms step_avg:126.30ms
step:823/1395 train_time:102689ms step_avg:126.31ms
step:824/1395 train_time:102819ms step_avg:126.31ms
step:825/1395 train_time:102950ms step_avg:126.32ms
step:826/1395 train_time:103082ms step_avg:126.33ms
step:827/1395 train_time:103212ms step_avg:126.33ms
step:828/1395 train_time:103342ms step_avg:126.34ms
step:829/1395 train_time:103474ms step_avg:126.34ms
step:830/1395 train_time:103607ms step_avg:126.35ms
step:831/1395 train_time:103737ms step_avg:126.36ms
step:832/1395 train_time:103869ms step_avg:126.36ms
step:833/1395 train_time:103999ms step_avg:126.37ms
step:834/1395 train_time:104131ms step_avg:126.37ms
step:835/1395 train_time:104263ms step_avg:126.38ms
step:836/1395 train_time:104394ms step_avg:126.39ms
step:837/1395 train_time:104527ms step_avg:126.39ms
step:838/1395 train_time:104657ms step_avg:126.40ms
step:839/1395 train_time:104788ms step_avg:126.40ms
step:840/1395 train_time:104919ms step_avg:126.41ms
step:841/1395 train_time:105049ms step_avg:126.41ms
step:842/1395 train_time:105179ms step_avg:126.42ms
step:843/1395 train_time:105310ms step_avg:126.42ms
step:844/1395 train_time:105442ms step_avg:126.43ms
step:845/1395 train_time:105573ms step_avg:126.43ms
step:846/1395 train_time:105704ms step_avg:126.44ms
step:847/1395 train_time:105835ms step_avg:126.45ms
step:848/1395 train_time:105964ms step_avg:126.45ms
step:849/1395 train_time:106096ms step_avg:126.45ms
step:850/1395 train_time:106227ms step_avg:126.46ms
step:851/1395 train_time:106358ms step_avg:126.47ms
step:852/1395 train_time:106490ms step_avg:126.47ms
step:853/1395 train_time:106621ms step_avg:126.48ms
step:854/1395 train_time:106752ms step_avg:126.48ms
step:855/1395 train_time:106884ms step_avg:126.49ms
step:856/1395 train_time:107013ms step_avg:126.49ms
step:857/1395 train_time:107143ms step_avg:126.50ms
step:858/1395 train_time:107275ms step_avg:126.50ms
step:859/1395 train_time:107407ms step_avg:126.51ms
step:860/1395 train_time:107539ms step_avg:126.52ms
step:861/1395 train_time:107669ms step_avg:126.52ms
step:862/1395 train_time:107801ms step_avg:126.53ms
step:863/1395 train_time:107931ms step_avg:126.53ms
step:864/1395 train_time:108062ms step_avg:126.54ms
step:865/1395 train_time:108193ms step_avg:126.54ms
step:866/1395 train_time:108325ms step_avg:126.55ms
step:867/1395 train_time:108456ms step_avg:126.55ms
step:868/1395 train_time:108588ms step_avg:126.56ms
step:869/1395 train_time:108718ms step_avg:126.56ms
step:870/1395 train_time:108849ms step_avg:126.57ms
step:871/1395 train_time:108979ms step_avg:126.57ms
step:872/1395 train_time:109110ms step_avg:126.58ms
step:873/1395 train_time:109240ms step_avg:126.58ms
step:874/1395 train_time:109370ms step_avg:126.59ms
step:875/1395 train_time:109503ms step_avg:126.59ms
step:875/1395 val_loss:3.4815 train_time:109633ms step_avg:126.74ms
step:876/1395 train_time:109654ms step_avg:126.62ms
step:877/1395 train_time:109777ms step_avg:126.62ms
step:878/1395 train_time:109909ms step_avg:126.62ms
step:879/1395 train_time:110040ms step_avg:126.63ms
step:880/1395 train_time:110170ms step_avg:126.63ms
step:881/1395 train_time:110301ms step_avg:126.64ms
step:882/1395 train_time:110430ms step_avg:126.64ms
step:883/1395 train_time:110560ms step_avg:126.64ms
step:884/1395 train_time:110692ms step_avg:126.65ms
step:885/1395 train_time:110825ms step_avg:126.66ms
step:886/1395 train_time:110956ms step_avg:126.66ms
step:887/1395 train_time:111086ms step_avg:126.67ms
step:888/1395 train_time:111218ms step_avg:126.67ms
step:889/1395 train_time:111350ms step_avg:126.68ms
step:890/1395 train_time:111481ms step_avg:126.68ms
step:891/1395 train_time:111611ms step_avg:126.69ms
step:892/1395 train_time:111742ms step_avg:126.69ms
step:893/1395 train_time:111874ms step_avg:126.70ms
step:894/1395 train_time:112006ms step_avg:126.70ms
step:895/1395 train_time:112137ms step_avg:126.71ms
step:896/1395 train_time:112267ms step_avg:126.71ms
step:897/1395 train_time:112397ms step_avg:126.72ms
step:898/1395 train_time:112528ms step_avg:126.72ms
step:899/1395 train_time:112660ms step_avg:126.73ms
step:900/1395 train_time:112791ms step_avg:126.73ms
step:901/1395 train_time:112921ms step_avg:126.74ms
step:902/1395 train_time:113052ms step_avg:126.74ms
step:903/1395 train_time:113183ms step_avg:126.74ms
step:904/1395 train_time:113314ms step_avg:126.75ms
step:905/1395 train_time:113444ms step_avg:126.75ms
step:906/1395 train_time:113575ms step_avg:126.76ms
step:907/1395 train_time:113708ms step_avg:126.76ms
step:908/1395 train_time:113839ms step_avg:126.77ms
step:909/1395 train_time:113970ms step_avg:126.77ms
step:910/1395 train_time:114103ms step_avg:126.78ms
step:911/1395 train_time:114234ms step_avg:126.79ms
step:912/1395 train_time:114365ms step_avg:126.79ms
step:913/1395 train_time:114497ms step_avg:126.80ms
step:914/1395 train_time:114628ms step_avg:126.80ms
step:915/1395 train_time:114760ms step_avg:126.81ms
step:916/1395 train_time:114890ms step_avg:126.81ms
step:917/1395 train_time:115022ms step_avg:126.82ms
step:918/1395 train_time:115152ms step_avg:126.82ms
step:919/1395 train_time:115285ms step_avg:126.83ms
step:920/1395 train_time:115417ms step_avg:126.83ms
step:921/1395 train_time:115548ms step_avg:126.84ms
step:922/1395 train_time:115679ms step_avg:126.84ms
step:923/1395 train_time:115810ms step_avg:126.85ms
step:924/1395 train_time:115941ms step_avg:126.85ms
step:925/1395 train_time:116072ms step_avg:126.85ms
step:926/1395 train_time:116205ms step_avg:126.86ms
step:927/1395 train_time:116336ms step_avg:126.87ms
step:928/1395 train_time:116466ms step_avg:126.87ms
step:929/1395 train_time:116597ms step_avg:126.87ms
step:930/1395 train_time:116730ms step_avg:126.88ms
step:931/1395 train_time:116859ms step_avg:126.88ms
step:932/1395 train_time:116989ms step_avg:126.89ms
step:933/1395 train_time:117123ms step_avg:126.89ms
step:934/1395 train_time:117256ms step_avg:126.90ms
step:935/1395 train_time:117389ms step_avg:126.91ms
step:936/1395 train_time:117522ms step_avg:126.91ms
step:937/1395 train_time:117656ms step_avg:126.92ms
step:938/1395 train_time:117789ms step_avg:126.93ms
step:939/1395 train_time:117921ms step_avg:126.93ms
step:940/1395 train_time:118055ms step_avg:126.94ms
step:941/1395 train_time:118187ms step_avg:126.95ms
step:942/1395 train_time:118320ms step_avg:126.95ms
step:943/1395 train_time:118452ms step_avg:126.96ms
step:944/1395 train_time:118585ms step_avg:126.96ms
step:945/1395 train_time:118718ms step_avg:126.97ms
step:946/1395 train_time:118850ms step_avg:126.98ms
step:947/1395 train_time:118983ms step_avg:126.98ms
step:948/1395 train_time:119115ms step_avg:126.99ms
step:949/1395 train_time:119249ms step_avg:127.00ms
step:950/1395 train_time:119382ms step_avg:127.00ms
step:951/1395 train_time:119515ms step_avg:127.01ms
step:952/1395 train_time:119646ms step_avg:127.01ms
step:953/1395 train_time:119779ms step_avg:127.02ms
step:954/1395 train_time:119911ms step_avg:127.02ms
step:955/1395 train_time:120043ms step_avg:127.03ms
step:956/1395 train_time:120178ms step_avg:127.04ms
step:957/1395 train_time:120309ms step_avg:127.04ms
step:958/1395 train_time:120443ms step_avg:127.05ms
step:959/1395 train_time:120575ms step_avg:127.06ms
step:960/1395 train_time:120709ms step_avg:127.06ms
step:961/1395 train_time:120842ms step_avg:127.07ms
step:962/1395 train_time:120974ms step_avg:127.07ms
step:963/1395 train_time:121108ms step_avg:127.08ms
step:964/1395 train_time:121241ms step_avg:127.09ms
step:965/1395 train_time:121374ms step_avg:127.09ms
step:966/1395 train_time:121506ms step_avg:127.10ms
step:967/1395 train_time:121638ms step_avg:127.10ms
step:968/1395 train_time:121769ms step_avg:127.11ms
step:969/1395 train_time:121901ms step_avg:127.11ms
step:970/1395 train_time:122034ms step_avg:127.12ms
step:971/1395 train_time:122167ms step_avg:127.12ms
step:972/1395 train_time:122300ms step_avg:127.13ms
step:973/1395 train_time:122432ms step_avg:127.14ms
step:974/1395 train_time:122564ms step_avg:127.14ms
step:975/1395 train_time:122696ms step_avg:127.15ms
step:976/1395 train_time:122828ms step_avg:127.15ms
step:977/1395 train_time:122961ms step_avg:127.16ms
step:978/1395 train_time:123093ms step_avg:127.16ms
step:979/1395 train_time:123226ms step_avg:127.17ms
step:980/1395 train_time:123359ms step_avg:127.17ms
step:981/1395 train_time:123491ms step_avg:127.18ms
step:982/1395 train_time:123622ms step_avg:127.18ms
step:983/1395 train_time:123754ms step_avg:127.19ms
step:984/1395 train_time:123886ms step_avg:127.19ms
step:985/1395 train_time:124019ms step_avg:127.20ms
step:986/1395 train_time:124154ms step_avg:127.21ms
step:987/1395 train_time:124286ms step_avg:127.21ms
step:988/1395 train_time:124417ms step_avg:127.22ms
step:989/1395 train_time:124550ms step_avg:127.22ms
step:990/1395 train_time:124683ms step_avg:127.23ms
step:991/1395 train_time:124816ms step_avg:127.23ms
step:992/1395 train_time:124951ms step_avg:127.24ms
step:993/1395 train_time:125087ms step_avg:127.25ms
step:994/1395 train_time:125218ms step_avg:127.25ms
step:995/1395 train_time:125350ms step_avg:127.26ms
step:996/1395 train_time:125482ms step_avg:127.26ms
step:997/1395 train_time:125615ms step_avg:127.27ms
step:998/1395 train_time:125747ms step_avg:127.27ms
step:999/1395 train_time:125880ms step_avg:127.28ms
step:1000/1395 train_time:126013ms step_avg:127.29ms
step:1000/1395 val_loss:3.4180 train_time:126145ms step_avg:127.42ms
step:1001/1395 train_time:126166ms step_avg:127.31ms
step:1002/1395 train_time:126288ms step_avg:127.31ms
step:1003/1395 train_time:126421ms step_avg:127.31ms
step:1004/1395 train_time:126554ms step_avg:127.32ms
step:1005/1395 train_time:126688ms step_avg:127.32ms
step:1006/1395 train_time:126819ms step_avg:127.33ms
step:1007/1395 train_time:126952ms step_avg:127.33ms
step:1008/1395 train_time:127084ms step_avg:127.34ms
step:1009/1395 train_time:127217ms step_avg:127.34ms
step:1010/1395 train_time:127350ms step_avg:127.35ms
step:1011/1395 train_time:127484ms step_avg:127.36ms
step:1012/1395 train_time:127615ms step_avg:127.36ms
step:1013/1395 train_time:127748ms step_avg:127.37ms
step:1014/1395 train_time:127881ms step_avg:127.37ms
step:1015/1395 train_time:128013ms step_avg:127.38ms
step:1016/1395 train_time:128147ms step_avg:127.38ms
step:1017/1395 train_time:128279ms step_avg:127.39ms
step:1018/1395 train_time:128412ms step_avg:127.39ms
step:1019/1395 train_time:128545ms step_avg:127.40ms
step:1020/1395 train_time:128676ms step_avg:127.40ms
step:1021/1395 train_time:128809ms step_avg:127.41ms
step:1022/1395 train_time:128940ms step_avg:127.41ms
step:1023/1395 train_time:129073ms step_avg:127.42ms
step:1024/1395 train_time:129206ms step_avg:127.42ms
step:1025/1395 train_time:129339ms step_avg:127.43ms
step:1026/1395 train_time:129472ms step_avg:127.43ms
step:1027/1395 train_time:129606ms step_avg:127.44ms
step:1028/1395 train_time:129741ms step_avg:127.45ms
step:1029/1395 train_time:129875ms step_avg:127.45ms
step:1030/1395 train_time:130008ms step_avg:127.46ms
step:1031/1395 train_time:130140ms step_avg:127.46ms
step:1032/1395 train_time:130275ms step_avg:127.47ms
step:1033/1395 train_time:130404ms step_avg:127.47ms
step:1034/1395 train_time:130537ms step_avg:127.48ms
step:1035/1395 train_time:130671ms step_avg:127.48ms
step:1036/1395 train_time:130804ms step_avg:127.49ms
step:1037/1395 train_time:130938ms step_avg:127.50ms
step:1038/1395 train_time:131070ms step_avg:127.50ms
step:1039/1395 train_time:131202ms step_avg:127.50ms
step:1040/1395 train_time:131334ms step_avg:127.51ms
step:1041/1395 train_time:131466ms step_avg:127.51ms
step:1042/1395 train_time:131599ms step_avg:127.52ms
step:1043/1395 train_time:131733ms step_avg:127.52ms
step:1044/1395 train_time:131868ms step_avg:127.53ms
step:1045/1395 train_time:132001ms step_avg:127.54ms
step:1046/1395 train_time:132134ms step_avg:127.54ms
step:1047/1395 train_time:132266ms step_avg:127.55ms
step:1048/1395 train_time:132399ms step_avg:127.55ms
step:1049/1395 train_time:132533ms step_avg:127.56ms
step:1050/1395 train_time:132666ms step_avg:127.56ms
step:1051/1395 train_time:132801ms step_avg:127.57ms
step:1052/1395 train_time:132933ms step_avg:127.57ms
step:1053/1395 train_time:133066ms step_avg:127.58ms
step:1054/1395 train_time:133198ms step_avg:127.58ms
step:1055/1395 train_time:133331ms step_avg:127.59ms
step:1056/1395 train_time:133464ms step_avg:127.59ms
step:1057/1395 train_time:133597ms step_avg:127.60ms
step:1058/1395 train_time:133731ms step_avg:127.61ms
step:1059/1395 train_time:133864ms step_avg:127.61ms
step:1060/1395 train_time:133998ms step_avg:127.62ms
step:1061/1395 train_time:134130ms step_avg:127.62ms
step:1062/1395 train_time:134263ms step_avg:127.63ms
step:1063/1395 train_time:134395ms step_avg:127.63ms
step:1064/1395 train_time:134528ms step_avg:127.64ms
step:1065/1395 train_time:134660ms step_avg:127.64ms
step:1066/1395 train_time:134794ms step_avg:127.65ms
step:1067/1395 train_time:134927ms step_avg:127.65ms
step:1068/1395 train_time:135059ms step_avg:127.66ms
step:1069/1395 train_time:135195ms step_avg:127.66ms
step:1070/1395 train_time:135327ms step_avg:127.67ms
step:1071/1395 train_time:135461ms step_avg:127.67ms
step:1072/1395 train_time:135593ms step_avg:127.68ms
step:1073/1395 train_time:135725ms step_avg:127.68ms
step:1074/1395 train_time:135857ms step_avg:127.69ms
step:1075/1395 train_time:135990ms step_avg:127.69ms
step:1076/1395 train_time:136122ms step_avg:127.69ms
step:1077/1395 train_time:136255ms step_avg:127.70ms
step:1078/1395 train_time:136388ms step_avg:127.70ms
step:1079/1395 train_time:136525ms step_avg:127.71ms
step:1080/1395 train_time:136658ms step_avg:127.72ms
step:1081/1395 train_time:136790ms step_avg:127.72ms
step:1082/1395 train_time:136922ms step_avg:127.73ms
step:1083/1395 train_time:137054ms step_avg:127.73ms
step:1084/1395 train_time:137189ms step_avg:127.74ms
step:1085/1395 train_time:137322ms step_avg:127.74ms
step:1086/1395 train_time:137455ms step_avg:127.75ms
step:1087/1395 train_time:137591ms step_avg:127.75ms
step:1088/1395 train_time:137724ms step_avg:127.76ms
step:1089/1395 train_time:137859ms step_avg:127.77ms
step:1090/1395 train_time:137993ms step_avg:127.77ms
step:1091/1395 train_time:138126ms step_avg:127.78ms
step:1092/1395 train_time:138258ms step_avg:127.78ms
step:1093/1395 train_time:138391ms step_avg:127.79ms
step:1094/1395 train_time:138524ms step_avg:127.79ms
step:1095/1395 train_time:138657ms step_avg:127.79ms
step:1096/1395 train_time:138793ms step_avg:127.80ms
step:1097/1395 train_time:138926ms step_avg:127.81ms
step:1098/1395 train_time:139060ms step_avg:127.81ms
step:1099/1395 train_time:139193ms step_avg:127.82ms
step:1100/1395 train_time:139325ms step_avg:127.82ms
step:1101/1395 train_time:139457ms step_avg:127.83ms
step:1102/1395 train_time:139590ms step_avg:127.83ms
step:1103/1395 train_time:139724ms step_avg:127.84ms
step:1104/1395 train_time:139856ms step_avg:127.84ms
step:1105/1395 train_time:139993ms step_avg:127.85ms
step:1106/1395 train_time:140126ms step_avg:127.85ms
step:1107/1395 train_time:140258ms step_avg:127.86ms
step:1108/1395 train_time:140394ms step_avg:127.86ms
step:1109/1395 train_time:140526ms step_avg:127.87ms
step:1110/1395 train_time:140658ms step_avg:127.87ms
step:1111/1395 train_time:140792ms step_avg:127.88ms
step:1112/1395 train_time:140925ms step_avg:127.88ms
step:1113/1395 train_time:141056ms step_avg:127.88ms
step:1114/1395 train_time:141189ms step_avg:127.89ms
step:1115/1395 train_time:141322ms step_avg:127.89ms
step:1116/1395 train_time:141455ms step_avg:127.90ms
step:1117/1395 train_time:141589ms step_avg:127.90ms
step:1118/1395 train_time:141724ms step_avg:127.91ms
step:1119/1395 train_time:141857ms step_avg:127.91ms
step:1120/1395 train_time:141989ms step_avg:127.92ms
step:1121/1395 train_time:142122ms step_avg:127.92ms
step:1122/1395 train_time:142255ms step_avg:127.93ms
step:1123/1395 train_time:142387ms step_avg:127.93ms
step:1124/1395 train_time:142520ms step_avg:127.94ms
step:1125/1395 train_time:142652ms step_avg:127.94ms
step:1125/1395 val_loss:3.3675 train_time:142786ms step_avg:128.06ms
step:1126/1395 train_time:142809ms step_avg:127.97ms
step:1127/1395 train_time:142931ms step_avg:127.96ms
step:1128/1395 train_time:143065ms step_avg:127.97ms
step:1129/1395 train_time:143197ms step_avg:127.97ms
step:1130/1395 train_time:143329ms step_avg:127.97ms
step:1131/1395 train_time:143463ms step_avg:127.98ms
step:1132/1395 train_time:143595ms step_avg:127.98ms
step:1133/1395 train_time:143727ms step_avg:127.99ms
step:1134/1395 train_time:143861ms step_avg:127.99ms
step:1135/1395 train_time:143994ms step_avg:127.99ms
step:1136/1395 train_time:144129ms step_avg:128.00ms
step:1137/1395 train_time:144261ms step_avg:128.00ms
step:1138/1395 train_time:144395ms step_avg:128.01ms
step:1139/1395 train_time:144529ms step_avg:128.01ms
step:1140/1395 train_time:144663ms step_avg:128.02ms
step:1141/1395 train_time:144798ms step_avg:128.03ms
step:1142/1395 train_time:144932ms step_avg:128.03ms
step:1143/1395 train_time:145068ms step_avg:128.04ms
step:1144/1395 train_time:145202ms step_avg:128.04ms
step:1145/1395 train_time:145337ms step_avg:128.05ms
step:1146/1395 train_time:145471ms step_avg:128.06ms
step:1147/1395 train_time:145606ms step_avg:128.06ms
step:1148/1395 train_time:145739ms step_avg:128.07ms
step:1149/1395 train_time:145873ms step_avg:128.07ms
step:1150/1395 train_time:146007ms step_avg:128.08ms
step:1151/1395 train_time:146142ms step_avg:128.08ms
step:1152/1395 train_time:146276ms step_avg:128.09ms
step:1153/1395 train_time:146412ms step_avg:128.09ms
step:1154/1395 train_time:146546ms step_avg:128.10ms
step:1155/1395 train_time:146679ms step_avg:128.10ms
step:1156/1395 train_time:146817ms step_avg:128.11ms
step:1157/1395 train_time:146951ms step_avg:128.12ms
step:1158/1395 train_time:147085ms step_avg:128.12ms
step:1159/1395 train_time:147219ms step_avg:128.13ms
step:1160/1395 train_time:147353ms step_avg:128.13ms
step:1161/1395 train_time:147486ms step_avg:128.14ms
step:1162/1395 train_time:147622ms step_avg:128.14ms
step:1163/1395 train_time:147755ms step_avg:128.15ms
step:1164/1395 train_time:147889ms step_avg:128.15ms
step:1165/1395 train_time:148023ms step_avg:128.16ms
step:1166/1395 train_time:148158ms step_avg:128.16ms
step:1167/1395 train_time:148291ms step_avg:128.17ms
step:1168/1395 train_time:148425ms step_avg:128.17ms
step:1169/1395 train_time:148559ms step_avg:128.18ms
step:1170/1395 train_time:148693ms step_avg:128.18ms
step:1171/1395 train_time:148828ms step_avg:128.19ms
step:1172/1395 train_time:148962ms step_avg:128.19ms
step:1173/1395 train_time:149097ms step_avg:128.20ms
step:1174/1395 train_time:149234ms step_avg:128.21ms
step:1175/1395 train_time:149369ms step_avg:128.21ms
step:1176/1395 train_time:149504ms step_avg:128.22ms
step:1177/1395 train_time:149641ms step_avg:128.23ms
step:1178/1395 train_time:149775ms step_avg:128.23ms
step:1179/1395 train_time:149910ms step_avg:128.24ms
step:1180/1395 train_time:150046ms step_avg:128.24ms
step:1181/1395 train_time:150182ms step_avg:128.25ms
step:1182/1395 train_time:150315ms step_avg:128.25ms
step:1183/1395 train_time:150450ms step_avg:128.26ms
step:1184/1395 train_time:150584ms step_avg:128.27ms
step:1185/1395 train_time:150724ms step_avg:128.28ms
step:1186/1395 train_time:150853ms step_avg:128.28ms
step:1187/1395 train_time:150992ms step_avg:128.29ms
step:1188/1395 train_time:151125ms step_avg:128.29ms
step:1189/1395 train_time:151259ms step_avg:128.29ms
step:1190/1395 train_time:151392ms step_avg:128.30ms
step:1191/1395 train_time:151527ms step_avg:128.30ms
step:1192/1395 train_time:151661ms step_avg:128.31ms
step:1193/1395 train_time:151794ms step_avg:128.31ms
step:1194/1395 train_time:151929ms step_avg:128.32ms
step:1195/1395 train_time:152063ms step_avg:128.32ms
step:1196/1395 train_time:152198ms step_avg:128.33ms
step:1197/1395 train_time:152332ms step_avg:128.33ms
step:1198/1395 train_time:152468ms step_avg:128.34ms
step:1199/1395 train_time:152602ms step_avg:128.34ms
step:1200/1395 train_time:152737ms step_avg:128.35ms
step:1201/1395 train_time:152870ms step_avg:128.35ms
step:1202/1395 train_time:153008ms step_avg:128.36ms
step:1203/1395 train_time:153147ms step_avg:128.37ms
step:1204/1395 train_time:153282ms step_avg:128.38ms
step:1205/1395 train_time:153418ms step_avg:128.38ms
step:1206/1395 train_time:153554ms step_avg:128.39ms
step:1207/1395 train_time:153687ms step_avg:128.39ms
step:1208/1395 train_time:153821ms step_avg:128.40ms
step:1209/1395 train_time:153955ms step_avg:128.40ms
step:1210/1395 train_time:154091ms step_avg:128.41ms
step:1211/1395 train_time:154228ms step_avg:128.42ms
step:1212/1395 train_time:154359ms step_avg:128.42ms
step:1213/1395 train_time:154493ms step_avg:128.42ms
step:1214/1395 train_time:154629ms step_avg:128.43ms
step:1215/1395 train_time:154765ms step_avg:128.44ms
step:1216/1395 train_time:154897ms step_avg:128.44ms
step:1217/1395 train_time:155032ms step_avg:128.44ms
step:1218/1395 train_time:155164ms step_avg:128.45ms
step:1219/1395 train_time:155297ms step_avg:128.45ms
step:1220/1395 train_time:155431ms step_avg:128.46ms
step:1221/1395 train_time:155564ms step_avg:128.46ms
step:1222/1395 train_time:155700ms step_avg:128.46ms
step:1223/1395 train_time:155833ms step_avg:128.47ms
step:1224/1395 train_time:155967ms step_avg:128.47ms
step:1225/1395 train_time:156102ms step_avg:128.48ms
step:1226/1395 train_time:156237ms step_avg:128.48ms
step:1227/1395 train_time:156372ms step_avg:128.49ms
step:1228/1395 train_time:156509ms step_avg:128.50ms
step:1229/1395 train_time:156641ms step_avg:128.50ms
step:1230/1395 train_time:156777ms step_avg:128.51ms
step:1231/1395 train_time:156913ms step_avg:128.51ms
step:1232/1395 train_time:157050ms step_avg:128.52ms
step:1233/1395 train_time:157184ms step_avg:128.52ms
step:1234/1395 train_time:157318ms step_avg:128.53ms
step:1235/1395 train_time:157454ms step_avg:128.53ms
step:1236/1395 train_time:157588ms step_avg:128.54ms
step:1237/1395 train_time:157722ms step_avg:128.54ms
step:1238/1395 train_time:157860ms step_avg:128.55ms
step:1239/1395 train_time:157994ms step_avg:128.55ms
step:1240/1395 train_time:158128ms step_avg:128.56ms
step:1241/1395 train_time:158264ms step_avg:128.57ms
step:1242/1395 train_time:158397ms step_avg:128.57ms
step:1243/1395 train_time:158532ms step_avg:128.57ms
step:1244/1395 train_time:158666ms step_avg:128.58ms
step:1245/1395 train_time:158800ms step_avg:128.58ms
step:1246/1395 train_time:158934ms step_avg:128.59ms
step:1247/1395 train_time:159068ms step_avg:128.59ms
step:1248/1395 train_time:159201ms step_avg:128.60ms
step:1249/1395 train_time:159335ms step_avg:128.60ms
step:1250/1395 train_time:159469ms step_avg:128.60ms
step:1250/1395 val_loss:3.3197 train_time:159602ms step_avg:128.71ms
step:1251/1395 train_time:159622ms step_avg:128.62ms
step:1252/1395 train_time:159748ms step_avg:128.62ms
step:1253/1395 train_time:159882ms step_avg:128.63ms
step:1254/1395 train_time:160015ms step_avg:128.63ms
step:1255/1395 train_time:160154ms step_avg:128.64ms
step:1256/1395 train_time:160288ms step_avg:128.64ms
step:1257/1395 train_time:160422ms step_avg:128.65ms
step:1258/1395 train_time:160560ms step_avg:128.65ms
step:1259/1395 train_time:160691ms step_avg:128.66ms
step:1260/1395 train_time:160826ms step_avg:128.66ms
step:1261/1395 train_time:160960ms step_avg:128.67ms
step:1262/1395 train_time:161096ms step_avg:128.67ms
step:1263/1395 train_time:161231ms step_avg:128.68ms
step:1264/1395 train_time:161365ms step_avg:128.68ms
step:1265/1395 train_time:161499ms step_avg:128.68ms
step:1266/1395 train_time:161634ms step_avg:128.69ms
step:1267/1395 train_time:161767ms step_avg:128.69ms
step:1268/1395 train_time:161902ms step_avg:128.70ms
step:1269/1395 train_time:162037ms step_avg:128.70ms
step:1270/1395 train_time:162171ms step_avg:128.71ms
step:1271/1395 train_time:162306ms step_avg:128.71ms
step:1272/1395 train_time:162440ms step_avg:128.72ms
step:1273/1395 train_time:162575ms step_avg:128.72ms
step:1274/1395 train_time:162708ms step_avg:128.72ms
step:1275/1395 train_time:162843ms step_avg:128.73ms
step:1276/1395 train_time:162977ms step_avg:128.73ms
step:1277/1395 train_time:163110ms step_avg:128.74ms
step:1278/1395 train_time:163243ms step_avg:128.74ms
step:1279/1395 train_time:163376ms step_avg:128.74ms
step:1280/1395 train_time:163512ms step_avg:128.75ms
step:1281/1395 train_time:163647ms step_avg:128.75ms
step:1282/1395 train_time:163781ms step_avg:128.76ms
step:1283/1395 train_time:163918ms step_avg:128.76ms
step:1284/1395 train_time:164053ms step_avg:128.77ms
step:1285/1395 train_time:164186ms step_avg:128.77ms
step:1286/1395 train_time:164320ms step_avg:128.78ms
step:1287/1395 train_time:164455ms step_avg:128.78ms
step:1288/1395 train_time:164588ms step_avg:128.79ms
step:1289/1395 train_time:164725ms step_avg:128.79ms
step:1290/1395 train_time:164861ms step_avg:128.80ms
step:1291/1395 train_time:164998ms step_avg:128.80ms
step:1292/1395 train_time:165132ms step_avg:128.81ms
step:1293/1395 train_time:165268ms step_avg:128.81ms
step:1294/1395 train_time:165401ms step_avg:128.82ms
step:1295/1395 train_time:165536ms step_avg:128.82ms
step:1296/1395 train_time:165672ms step_avg:128.83ms
step:1297/1395 train_time:165807ms step_avg:128.83ms
step:1298/1395 train_time:165940ms step_avg:128.84ms
step:1299/1395 train_time:166075ms step_avg:128.84ms
step:1300/1395 train_time:166209ms step_avg:128.84ms
step:1301/1395 train_time:166343ms step_avg:128.85ms
step:1302/1395 train_time:166477ms step_avg:128.85ms
step:1303/1395 train_time:166612ms step_avg:128.86ms
step:1304/1395 train_time:166748ms step_avg:128.86ms
step:1305/1395 train_time:166884ms step_avg:128.87ms
step:1306/1395 train_time:167018ms step_avg:128.87ms
step:1307/1395 train_time:167152ms step_avg:128.88ms
step:1308/1395 train_time:167288ms step_avg:128.88ms
step:1309/1395 train_time:167423ms step_avg:128.89ms
step:1310/1395 train_time:167556ms step_avg:128.89ms
step:1311/1395 train_time:167690ms step_avg:128.89ms
step:1312/1395 train_time:167824ms step_avg:128.90ms
step:1313/1395 train_time:167958ms step_avg:128.90ms
step:1314/1395 train_time:168091ms step_avg:128.90ms
step:1315/1395 train_time:168226ms step_avg:128.91ms
step:1316/1395 train_time:168360ms step_avg:128.91ms
step:1317/1395 train_time:168493ms step_avg:128.92ms
step:1318/1395 train_time:168630ms step_avg:128.92ms
step:1319/1395 train_time:168766ms step_avg:128.93ms
step:1320/1395 train_time:168900ms step_avg:128.93ms
step:1321/1395 train_time:169034ms step_avg:128.94ms
step:1322/1395 train_time:169171ms step_avg:128.94ms
step:1323/1395 train_time:169306ms step_avg:128.95ms
step:1324/1395 train_time:169439ms step_avg:128.95ms
step:1325/1395 train_time:169575ms step_avg:128.95ms
step:1326/1395 train_time:169710ms step_avg:128.96ms
step:1327/1395 train_time:169844ms step_avg:128.96ms
step:1328/1395 train_time:169977ms step_avg:128.97ms
step:1329/1395 train_time:170116ms step_avg:128.97ms
step:1330/1395 train_time:170251ms step_avg:128.98ms
step:1331/1395 train_time:170388ms step_avg:128.98ms
step:1332/1395 train_time:170526ms step_avg:128.99ms
step:1333/1395 train_time:170662ms step_avg:129.00ms
step:1334/1395 train_time:170796ms step_avg:129.00ms
step:1335/1395 train_time:170930ms step_avg:129.00ms
step:1336/1395 train_time:171066ms step_avg:129.01ms
step:1337/1395 train_time:171200ms step_avg:129.01ms
step:1338/1395 train_time:171335ms step_avg:129.02ms
step:1339/1395 train_time:171470ms step_avg:129.02ms
step:1340/1395 train_time:171607ms step_avg:129.03ms
step:1341/1395 train_time:171740ms step_avg:129.03ms
step:1342/1395 train_time:171875ms step_avg:129.04ms
step:1343/1395 train_time:172009ms step_avg:129.04ms
step:1344/1395 train_time:172142ms step_avg:129.04ms
step:1345/1395 train_time:172276ms step_avg:129.05ms
step:1346/1395 train_time:172410ms step_avg:129.05ms
step:1347/1395 train_time:172547ms step_avg:129.06ms
step:1348/1395 train_time:172682ms step_avg:129.06ms
step:1349/1395 train_time:172819ms step_avg:129.07ms
step:1350/1395 train_time:172953ms step_avg:129.07ms
step:1351/1395 train_time:173088ms step_avg:129.07ms
step:1352/1395 train_time:173226ms step_avg:129.08ms
step:1353/1395 train_time:173364ms step_avg:129.09ms
step:1354/1395 train_time:173499ms step_avg:129.09ms
step:1355/1395 train_time:173633ms step_avg:129.10ms
step:1356/1395 train_time:173768ms step_avg:129.10ms
step:1357/1395 train_time:173904ms step_avg:129.10ms
step:1358/1395 train_time:174040ms step_avg:129.11ms
step:1359/1395 train_time:174175ms step_avg:129.11ms
step:1360/1395 train_time:174311ms step_avg:129.12ms
step:1361/1395 train_time:174447ms step_avg:129.12ms
step:1362/1395 train_time:174585ms step_avg:129.13ms
step:1363/1395 train_time:174723ms step_avg:129.14ms
step:1364/1395 train_time:174859ms step_avg:129.14ms
step:1365/1395 train_time:174992ms step_avg:129.15ms
step:1366/1395 train_time:175128ms step_avg:129.15ms
step:1367/1395 train_time:175264ms step_avg:129.16ms
step:1368/1395 train_time:175399ms step_avg:129.16ms
step:1369/1395 train_time:175537ms step_avg:129.17ms
step:1370/1395 train_time:175678ms step_avg:129.17ms
step:1371/1395 train_time:175811ms step_avg:129.18ms
step:1372/1395 train_time:175950ms step_avg:129.19ms
step:1373/1395 train_time:176085ms step_avg:129.19ms
step:1374/1395 train_time:176223ms step_avg:129.20ms
step:1375/1395 train_time:176357ms step_avg:129.20ms
step:1375/1395 val_loss:3.2842 train_time:176491ms step_avg:129.30ms
step:1376/1395 train_time:176511ms step_avg:129.22ms
step:1377/1395 train_time:176634ms step_avg:129.21ms
step:1378/1395 train_time:176769ms step_avg:129.22ms
step:1379/1395 train_time:176903ms step_avg:129.22ms
step:1380/1395 train_time:177039ms step_avg:129.23ms
step:1381/1395 train_time:177177ms step_avg:129.23ms
step:1382/1395 train_time:177312ms step_avg:129.24ms
step:1383/1395 train_time:177448ms step_avg:129.24ms
step:1384/1395 train_time:177586ms step_avg:129.25ms
step:1385/1395 train_time:177721ms step_avg:129.25ms
step:1386/1395 train_time:177855ms step_avg:129.26ms
step:1387/1395 train_time:177992ms step_avg:129.26ms
step:1388/1395 train_time:178127ms step_avg:129.27ms
step:1389/1395 train_time:178264ms step_avg:129.27ms
step:1390/1395 train_time:178400ms step_avg:129.28ms
step:1391/1395 train_time:178534ms step_avg:129.28ms
step:1392/1395 train_time:178671ms step_avg:129.28ms
step:1393/1395 train_time:178804ms step_avg:129.29ms
step:1394/1395 train_time:178940ms step_avg:129.29ms
step:1395/1395 train_time:179074ms step_avg:129.30ms
step:1395/1395 val_loss:3.2801 train_time:179210ms step_avg:129.39ms
peak memory allocated: 37653 MiB reserved: 39236 MiB
