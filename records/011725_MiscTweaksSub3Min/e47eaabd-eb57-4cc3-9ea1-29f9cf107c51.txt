import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True

# -----------------------------------------------------------------------------
# Custom operators

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

def lm_head_fp8(x: Tensor, w: Tensor) -> Tensor:
    _x = x.flatten(0, -2)
    out: Tensor = torch.ops.nanogpt.mm(_x, w, x_s=2.0, w_s=32.0, grad_s=2.0**29)[0]
    return out.reshape(*x.shape[:-1], -1)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev():
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int):
        super().__init__(in_features, out_features, bias=False)

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x):
        return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len=65536):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        # self.c_q = CastedLinear(dim, dim)
        # self.c_k = CastedLinear(dim, dim)
        # self.c_v = CastedLinear(dim, dim)
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std
        self.qkv_w = nn.Parameter(torch.empty(3, dim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads) # dim // num_heads = head_dim
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # Set attention scale such that the minimum attainable attention entropy
        # (but not necessary the attention entropy itself) is close to 0. By @leloykun
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        # q = self.c_q(x).view(B, T, self.num_heads, -1)
        # k = self.c_k(x).view(B, T, self.num_heads, -1)
        # v = self.c_v(x).view(B, T, self.num_heads, -1)
        # qkv_weight = torch.cat([self.c_q.weight, self.c_k.weight, self.c_v.weight], dim=0).type_as(x)
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3*self.num_heads, -1).chunk(3, dim=-2)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, model_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(model_dim, num_heads, layer_idx) if layer_idx != 7 else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, ve, x0, block_mask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, num_embeddings: int, embedding_dim: int):
        super().__init__()
        self.embed = nn.ModuleList([nn.Embedding(num_embeddings, embedding_dim) for _ in range(3)])

    def forward(self, input_seq) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2], None, None, None, None, None, None, ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual learning
        self.value_embeds = ValueEmbedding(vocab_size, model_dim)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128))
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        assert input_seq.ndim == 1
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        docs = (input_seq == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        def create_doc_swc_block_masks(sliding_window_num_blocks: Tensor):
            kv_idx = block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & document_bm
            full_bm  = causal_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            def build_bm(sw_num_blocks: Tensor) -> BlockMask:
                return BlockMask.from_kv_blocks(
                    torch.clamp_max(kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                    kv_indices,
                    torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                    full_kv_indices,
                    BLOCK_SIZE=BLOCK_SIZE,
                    mask_mod=document_causal,
                )
            return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

        # Long-short SWA block masks by @leloykun & @YouJiacheng
        long_bm, short_bm = create_doc_swc_block_masks(sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = lm_head_fp8(x, self.lm_head.weight) if self.training else self.lm_head(x)
        # @Grad62304977 added tanh softcapping, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use cycle(files) if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    batch_size = 8*64*1024 # batch size in tokens
    num_iterations = 1395 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 64*1024 # FlexAttention sequence length
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
             if console:
                 print(s)
             print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_loader = distributed_data_generator(args.train_files, args.batch_size, rank, world_size)

model = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim >= 2]
embed_params = [model.embed.weight, *model.value_embeds.parameters()]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), fused=True, eps=1e-10)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(it: int):
    t = 1 - it / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_bs = world_size * args.seq_len
        assert args.val_tokens % val_bs == 0
        val_steps = args.val_tokens // val_bs
        val_loader = distributed_data_generator(args.val_files, val_bs, rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION BEGIN -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB"
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250110+cu126 compiled for CUDA 12.6
Thu Jan 16 21:08:12 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   38C    P0             121W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   28C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0             118W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   38C    P0             120W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   37C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1395 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1395 train_time:25692ms step_avg:nanms
step:2/1395 train_time:26202ms step_avg:nanms
step:3/1395 train_time:26322ms step_avg:nanms
step:4/1395 train_time:26443ms step_avg:nanms
step:5/1395 train_time:26563ms step_avg:nanms
step:6/1395 train_time:26684ms step_avg:nanms
step:7/1395 train_time:26806ms step_avg:nanms
step:8/1395 train_time:26928ms step_avg:nanms
step:9/1395 train_time:27050ms step_avg:nanms
step:10/1395 train_time:27173ms step_avg:nanms
step:11/1395 train_time:124ms step_avg:nanms
step:12/1395 train_time:247ms step_avg:nanms
step:13/1395 train_time:369ms step_avg:123.04ms
step:14/1395 train_time:490ms step_avg:122.51ms
step:15/1395 train_time:613ms step_avg:122.53ms
step:16/1395 train_time:734ms step_avg:122.28ms
step:17/1395 train_time:856ms step_avg:122.32ms
step:18/1395 train_time:980ms step_avg:122.46ms
step:19/1395 train_time:1103ms step_avg:122.59ms
step:20/1395 train_time:1227ms step_avg:122.74ms
step:21/1395 train_time:1351ms step_avg:122.78ms
step:22/1395 train_time:1472ms step_avg:122.69ms
step:23/1395 train_time:1594ms step_avg:122.63ms
step:24/1395 train_time:1715ms step_avg:122.51ms
step:25/1395 train_time:1837ms step_avg:122.45ms
step:26/1395 train_time:1958ms step_avg:122.40ms
step:27/1395 train_time:2081ms step_avg:122.42ms
step:28/1395 train_time:2203ms step_avg:122.41ms
step:29/1395 train_time:2326ms step_avg:122.40ms
step:30/1395 train_time:2447ms step_avg:122.36ms
step:31/1395 train_time:2570ms step_avg:122.39ms
step:32/1395 train_time:2692ms step_avg:122.38ms
step:33/1395 train_time:2816ms step_avg:122.44ms
step:34/1395 train_time:2938ms step_avg:122.40ms
step:35/1395 train_time:3060ms step_avg:122.42ms
step:36/1395 train_time:3183ms step_avg:122.42ms
step:37/1395 train_time:3307ms step_avg:122.47ms
step:38/1395 train_time:3429ms step_avg:122.48ms
step:39/1395 train_time:3552ms step_avg:122.48ms
step:40/1395 train_time:3673ms step_avg:122.44ms
step:41/1395 train_time:3796ms step_avg:122.45ms
step:42/1395 train_time:3917ms step_avg:122.40ms
step:43/1395 train_time:4040ms step_avg:122.41ms
step:44/1395 train_time:4163ms step_avg:122.44ms
step:45/1395 train_time:4285ms step_avg:122.42ms
step:46/1395 train_time:4407ms step_avg:122.41ms
step:47/1395 train_time:4530ms step_avg:122.43ms
step:48/1395 train_time:4651ms step_avg:122.39ms
step:49/1395 train_time:4773ms step_avg:122.39ms
step:50/1395 train_time:4895ms step_avg:122.38ms
step:51/1395 train_time:5016ms step_avg:122.35ms
step:52/1395 train_time:5137ms step_avg:122.32ms
step:53/1395 train_time:5260ms step_avg:122.33ms
step:54/1395 train_time:5382ms step_avg:122.32ms
step:55/1395 train_time:5505ms step_avg:122.33ms
step:56/1395 train_time:5627ms step_avg:122.32ms
step:57/1395 train_time:5750ms step_avg:122.33ms
step:58/1395 train_time:5873ms step_avg:122.35ms
step:59/1395 train_time:5998ms step_avg:122.40ms
step:60/1395 train_time:6120ms step_avg:122.40ms
step:61/1395 train_time:6243ms step_avg:122.42ms
step:62/1395 train_time:6366ms step_avg:122.42ms
step:63/1395 train_time:6487ms step_avg:122.40ms
step:64/1395 train_time:6610ms step_avg:122.41ms
step:65/1395 train_time:6733ms step_avg:122.42ms
step:66/1395 train_time:6855ms step_avg:122.41ms
step:67/1395 train_time:6978ms step_avg:122.42ms
step:68/1395 train_time:7099ms step_avg:122.40ms
step:69/1395 train_time:7222ms step_avg:122.40ms
step:70/1395 train_time:7343ms step_avg:122.39ms
step:71/1395 train_time:7465ms step_avg:122.38ms
step:72/1395 train_time:7587ms step_avg:122.38ms
step:73/1395 train_time:7709ms step_avg:122.37ms
step:74/1395 train_time:7831ms step_avg:122.37ms
step:75/1395 train_time:7955ms step_avg:122.38ms
step:76/1395 train_time:8076ms step_avg:122.37ms
step:77/1395 train_time:8199ms step_avg:122.37ms
step:78/1395 train_time:8322ms step_avg:122.39ms
step:79/1395 train_time:8445ms step_avg:122.39ms
step:80/1395 train_time:8567ms step_avg:122.38ms
step:81/1395 train_time:8688ms step_avg:122.36ms
step:82/1395 train_time:8810ms step_avg:122.36ms
step:83/1395 train_time:8932ms step_avg:122.36ms
step:84/1395 train_time:9054ms step_avg:122.36ms
step:85/1395 train_time:9176ms step_avg:122.35ms
step:86/1395 train_time:9298ms step_avg:122.35ms
step:87/1395 train_time:9421ms step_avg:122.36ms
step:88/1395 train_time:9544ms step_avg:122.35ms
step:89/1395 train_time:9667ms step_avg:122.36ms
step:90/1395 train_time:9791ms step_avg:122.39ms
step:91/1395 train_time:9913ms step_avg:122.38ms
step:92/1395 train_time:10034ms step_avg:122.37ms
step:93/1395 train_time:10157ms step_avg:122.37ms
step:94/1395 train_time:10279ms step_avg:122.37ms
step:95/1395 train_time:10402ms step_avg:122.37ms
step:96/1395 train_time:10524ms step_avg:122.37ms
step:97/1395 train_time:10646ms step_avg:122.37ms
step:98/1395 train_time:10769ms step_avg:122.37ms
step:99/1395 train_time:10891ms step_avg:122.38ms
step:100/1395 train_time:11013ms step_avg:122.37ms
step:101/1395 train_time:11136ms step_avg:122.37ms
step:102/1395 train_time:11257ms step_avg:122.36ms
step:103/1395 train_time:11381ms step_avg:122.38ms
step:104/1395 train_time:11503ms step_avg:122.37ms
step:105/1395 train_time:11626ms step_avg:122.38ms
step:106/1395 train_time:11749ms step_avg:122.39ms
step:107/1395 train_time:11872ms step_avg:122.39ms
step:108/1395 train_time:11994ms step_avg:122.39ms
step:109/1395 train_time:12118ms step_avg:122.40ms
step:110/1395 train_time:12241ms step_avg:122.41ms
step:111/1395 train_time:12365ms step_avg:122.43ms
step:112/1395 train_time:12488ms step_avg:122.43ms
step:113/1395 train_time:12610ms step_avg:122.43ms
step:114/1395 train_time:12733ms step_avg:122.43ms
step:115/1395 train_time:12856ms step_avg:122.44ms
step:116/1395 train_time:12979ms step_avg:122.44ms
step:117/1395 train_time:13102ms step_avg:122.45ms
step:118/1395 train_time:13225ms step_avg:122.45ms
step:119/1395 train_time:13348ms step_avg:122.45ms
step:120/1395 train_time:13470ms step_avg:122.45ms
step:121/1395 train_time:13592ms step_avg:122.45ms
step:122/1395 train_time:13716ms step_avg:122.47ms
step:123/1395 train_time:13840ms step_avg:122.48ms
step:124/1395 train_time:13962ms step_avg:122.48ms
step:125/1395 train_time:14086ms step_avg:122.49ms
step:125/1395 val_loss:4.4116 train_time:14209ms step_avg:123.56ms
step:126/1395 train_time:14231ms step_avg:122.68ms
step:127/1395 train_time:14348ms step_avg:122.64ms
step:128/1395 train_time:14474ms step_avg:122.66ms
step:129/1395 train_time:14597ms step_avg:122.67ms
step:130/1395 train_time:14720ms step_avg:122.67ms
step:131/1395 train_time:14842ms step_avg:122.66ms
step:132/1395 train_time:14964ms step_avg:122.66ms
step:133/1395 train_time:15087ms step_avg:122.65ms
step:134/1395 train_time:15209ms step_avg:122.65ms
step:135/1395 train_time:15332ms step_avg:122.66ms
step:136/1395 train_time:15456ms step_avg:122.67ms
step:137/1395 train_time:15580ms step_avg:122.68ms
step:138/1395 train_time:15703ms step_avg:122.68ms
step:139/1395 train_time:15825ms step_avg:122.68ms
step:140/1395 train_time:15947ms step_avg:122.67ms
step:141/1395 train_time:16070ms step_avg:122.67ms
step:142/1395 train_time:16193ms step_avg:122.68ms
step:143/1395 train_time:16316ms step_avg:122.68ms
step:144/1395 train_time:16440ms step_avg:122.68ms
step:145/1395 train_time:16563ms step_avg:122.69ms
step:146/1395 train_time:16687ms step_avg:122.70ms
step:147/1395 train_time:16812ms step_avg:122.72ms
step:148/1395 train_time:16936ms step_avg:122.72ms
step:149/1395 train_time:17058ms step_avg:122.72ms
step:150/1395 train_time:17180ms step_avg:122.72ms
step:151/1395 train_time:17303ms step_avg:122.72ms
step:152/1395 train_time:17425ms step_avg:122.71ms
step:153/1395 train_time:17548ms step_avg:122.71ms
step:154/1395 train_time:17672ms step_avg:122.72ms
step:155/1395 train_time:17797ms step_avg:122.74ms
step:156/1395 train_time:17920ms step_avg:122.74ms
step:157/1395 train_time:18043ms step_avg:122.74ms
step:158/1395 train_time:18166ms step_avg:122.74ms
step:159/1395 train_time:18288ms step_avg:122.74ms
step:160/1395 train_time:18411ms step_avg:122.74ms
step:161/1395 train_time:18534ms step_avg:122.74ms
step:162/1395 train_time:18657ms step_avg:122.74ms
step:163/1395 train_time:18780ms step_avg:122.74ms
step:164/1395 train_time:18903ms step_avg:122.75ms
step:165/1395 train_time:19026ms step_avg:122.75ms
step:166/1395 train_time:19149ms step_avg:122.75ms
step:167/1395 train_time:19273ms step_avg:122.76ms
step:168/1395 train_time:19396ms step_avg:122.76ms
step:169/1395 train_time:19519ms step_avg:122.76ms
step:170/1395 train_time:19643ms step_avg:122.77ms
step:171/1395 train_time:19766ms step_avg:122.77ms
step:172/1395 train_time:19890ms step_avg:122.78ms
step:173/1395 train_time:20012ms step_avg:122.77ms
step:174/1395 train_time:20136ms step_avg:122.78ms
step:175/1395 train_time:20259ms step_avg:122.78ms
step:176/1395 train_time:20382ms step_avg:122.78ms
step:177/1395 train_time:20504ms step_avg:122.78ms
step:178/1395 train_time:20627ms step_avg:122.78ms
step:179/1395 train_time:20750ms step_avg:122.78ms
step:180/1395 train_time:20874ms step_avg:122.79ms
step:181/1395 train_time:20997ms step_avg:122.79ms
step:182/1395 train_time:21121ms step_avg:122.80ms
step:183/1395 train_time:21244ms step_avg:122.80ms
step:184/1395 train_time:21367ms step_avg:122.80ms
step:185/1395 train_time:21489ms step_avg:122.79ms
step:186/1395 train_time:21612ms step_avg:122.80ms
step:187/1395 train_time:21735ms step_avg:122.80ms
step:188/1395 train_time:21859ms step_avg:122.80ms
step:189/1395 train_time:21982ms step_avg:122.80ms
step:190/1395 train_time:22106ms step_avg:122.81ms
step:191/1395 train_time:22229ms step_avg:122.81ms
step:192/1395 train_time:22352ms step_avg:122.81ms
step:193/1395 train_time:22475ms step_avg:122.81ms
step:194/1395 train_time:22598ms step_avg:122.81ms
step:195/1395 train_time:22721ms step_avg:122.81ms
step:196/1395 train_time:22844ms step_avg:122.82ms
step:197/1395 train_time:22967ms step_avg:122.82ms
step:198/1395 train_time:23089ms step_avg:122.82ms
step:199/1395 train_time:23213ms step_avg:122.82ms
step:200/1395 train_time:23336ms step_avg:122.82ms
step:201/1395 train_time:23459ms step_avg:122.82ms
step:202/1395 train_time:23584ms step_avg:122.83ms
step:203/1395 train_time:23705ms step_avg:122.83ms
step:204/1395 train_time:23828ms step_avg:122.83ms
step:205/1395 train_time:23950ms step_avg:122.82ms
step:206/1395 train_time:24073ms step_avg:122.82ms
step:207/1395 train_time:24197ms step_avg:122.83ms
step:208/1395 train_time:24322ms step_avg:122.84ms
step:209/1395 train_time:24444ms step_avg:122.83ms
step:210/1395 train_time:24567ms step_avg:122.84ms
step:211/1395 train_time:24692ms step_avg:122.84ms
step:212/1395 train_time:24816ms step_avg:122.85ms
step:213/1395 train_time:24940ms step_avg:122.86ms
step:214/1395 train_time:25063ms step_avg:122.86ms
step:215/1395 train_time:25187ms step_avg:122.86ms
step:216/1395 train_time:25310ms step_avg:122.86ms
step:217/1395 train_time:25433ms step_avg:122.87ms
step:218/1395 train_time:25557ms step_avg:122.87ms
step:219/1395 train_time:25682ms step_avg:122.88ms
step:220/1395 train_time:25806ms step_avg:122.89ms
step:221/1395 train_time:25929ms step_avg:122.89ms
step:222/1395 train_time:26052ms step_avg:122.89ms
step:223/1395 train_time:26176ms step_avg:122.89ms
step:224/1395 train_time:26299ms step_avg:122.89ms
step:225/1395 train_time:26422ms step_avg:122.89ms
step:226/1395 train_time:26546ms step_avg:122.90ms
step:227/1395 train_time:26669ms step_avg:122.90ms
step:228/1395 train_time:26793ms step_avg:122.91ms
step:229/1395 train_time:26916ms step_avg:122.90ms
step:230/1395 train_time:27040ms step_avg:122.91ms
step:231/1395 train_time:27162ms step_avg:122.91ms
step:232/1395 train_time:27286ms step_avg:122.91ms
step:233/1395 train_time:27410ms step_avg:122.91ms
step:234/1395 train_time:27534ms step_avg:122.92ms
step:235/1395 train_time:27658ms step_avg:122.92ms
step:236/1395 train_time:27782ms step_avg:122.93ms
step:237/1395 train_time:27906ms step_avg:122.94ms
step:238/1395 train_time:28031ms step_avg:122.94ms
step:239/1395 train_time:28154ms step_avg:122.94ms
step:240/1395 train_time:28278ms step_avg:122.95ms
step:241/1395 train_time:28401ms step_avg:122.95ms
step:242/1395 train_time:28524ms step_avg:122.95ms
step:243/1395 train_time:28648ms step_avg:122.95ms
step:244/1395 train_time:28772ms step_avg:122.96ms
step:245/1395 train_time:28896ms step_avg:122.96ms
step:246/1395 train_time:29021ms step_avg:122.97ms
step:247/1395 train_time:29144ms step_avg:122.97ms
step:248/1395 train_time:29267ms step_avg:122.97ms
step:249/1395 train_time:29390ms step_avg:122.97ms
step:250/1395 train_time:29513ms step_avg:122.97ms
step:250/1395 val_loss:3.9867 train_time:29635ms step_avg:123.48ms
step:251/1395 train_time:29656ms step_avg:123.06ms
step:252/1395 train_time:29772ms step_avg:123.03ms
step:253/1395 train_time:29898ms step_avg:123.04ms
step:254/1395 train_time:30021ms step_avg:123.04ms
step:255/1395 train_time:30145ms step_avg:123.04ms
step:256/1395 train_time:30268ms step_avg:123.04ms
step:257/1395 train_time:30392ms step_avg:123.04ms
step:258/1395 train_time:30515ms step_avg:123.04ms
step:259/1395 train_time:30638ms step_avg:123.04ms
step:260/1395 train_time:30763ms step_avg:123.05ms
step:261/1395 train_time:30887ms step_avg:123.06ms
step:262/1395 train_time:31012ms step_avg:123.06ms
step:263/1395 train_time:31135ms step_avg:123.06ms
step:264/1395 train_time:31258ms step_avg:123.06ms
step:265/1395 train_time:31381ms step_avg:123.06ms
step:266/1395 train_time:31504ms step_avg:123.06ms
step:267/1395 train_time:31628ms step_avg:123.07ms
step:268/1395 train_time:31752ms step_avg:123.07ms
step:269/1395 train_time:31876ms step_avg:123.07ms
step:270/1395 train_time:31999ms step_avg:123.07ms
step:271/1395 train_time:32123ms step_avg:123.08ms
step:272/1395 train_time:32246ms step_avg:123.08ms
step:273/1395 train_time:32370ms step_avg:123.08ms
step:274/1395 train_time:32494ms step_avg:123.08ms
step:275/1395 train_time:32617ms step_avg:123.08ms
step:276/1395 train_time:32741ms step_avg:123.09ms
step:277/1395 train_time:32864ms step_avg:123.09ms
step:278/1395 train_time:32989ms step_avg:123.09ms
step:279/1395 train_time:33113ms step_avg:123.10ms
step:280/1395 train_time:33236ms step_avg:123.10ms
step:281/1395 train_time:33360ms step_avg:123.10ms
step:282/1395 train_time:33483ms step_avg:123.10ms
step:283/1395 train_time:33607ms step_avg:123.10ms
step:284/1395 train_time:33731ms step_avg:123.11ms
step:285/1395 train_time:33855ms step_avg:123.11ms
step:286/1395 train_time:33977ms step_avg:123.11ms
step:287/1395 train_time:34101ms step_avg:123.11ms
step:288/1395 train_time:34225ms step_avg:123.11ms
step:289/1395 train_time:34351ms step_avg:123.12ms
step:290/1395 train_time:34473ms step_avg:123.12ms
step:291/1395 train_time:34596ms step_avg:123.12ms
step:292/1395 train_time:34720ms step_avg:123.12ms
step:293/1395 train_time:34844ms step_avg:123.12ms
step:294/1395 train_time:34967ms step_avg:123.12ms
step:295/1395 train_time:35091ms step_avg:123.13ms
step:296/1395 train_time:35214ms step_avg:123.13ms
step:297/1395 train_time:35339ms step_avg:123.13ms
step:298/1395 train_time:35462ms step_avg:123.13ms
step:299/1395 train_time:35584ms step_avg:123.13ms
step:300/1395 train_time:35708ms step_avg:123.13ms
step:301/1395 train_time:35830ms step_avg:123.13ms
step:302/1395 train_time:35954ms step_avg:123.13ms
step:303/1395 train_time:36077ms step_avg:123.13ms
step:304/1395 train_time:36200ms step_avg:123.13ms
step:305/1395 train_time:36325ms step_avg:123.14ms
step:306/1395 train_time:36449ms step_avg:123.14ms
step:307/1395 train_time:36574ms step_avg:123.14ms
step:308/1395 train_time:36696ms step_avg:123.14ms
step:309/1395 train_time:36820ms step_avg:123.15ms
step:310/1395 train_time:36943ms step_avg:123.14ms
step:311/1395 train_time:37067ms step_avg:123.14ms
step:312/1395 train_time:37191ms step_avg:123.15ms
step:313/1395 train_time:37317ms step_avg:123.16ms
step:314/1395 train_time:37443ms step_avg:123.17ms
step:315/1395 train_time:37569ms step_avg:123.18ms
step:316/1395 train_time:37695ms step_avg:123.19ms
step:317/1395 train_time:37821ms step_avg:123.19ms
step:318/1395 train_time:37947ms step_avg:123.21ms
step:319/1395 train_time:38073ms step_avg:123.21ms
step:320/1395 train_time:38200ms step_avg:123.22ms
step:321/1395 train_time:38326ms step_avg:123.23ms
step:322/1395 train_time:38453ms step_avg:123.25ms
step:323/1395 train_time:38580ms step_avg:123.26ms
step:324/1395 train_time:38706ms step_avg:123.27ms
step:325/1395 train_time:38832ms step_avg:123.28ms
step:326/1395 train_time:38958ms step_avg:123.28ms
step:327/1395 train_time:39084ms step_avg:123.29ms
step:328/1395 train_time:39209ms step_avg:123.30ms
step:329/1395 train_time:39336ms step_avg:123.31ms
step:330/1395 train_time:39463ms step_avg:123.32ms
step:331/1395 train_time:39590ms step_avg:123.33ms
step:332/1395 train_time:39717ms step_avg:123.34ms
step:333/1395 train_time:39844ms step_avg:123.36ms
step:334/1395 train_time:39970ms step_avg:123.36ms
step:335/1395 train_time:40095ms step_avg:123.37ms
step:336/1395 train_time:40221ms step_avg:123.38ms
step:337/1395 train_time:40347ms step_avg:123.38ms
step:338/1395 train_time:40474ms step_avg:123.40ms
step:339/1395 train_time:40600ms step_avg:123.41ms
step:340/1395 train_time:40727ms step_avg:123.42ms
step:341/1395 train_time:40856ms step_avg:123.43ms
step:342/1395 train_time:40983ms step_avg:123.44ms
step:343/1395 train_time:41109ms step_avg:123.45ms
step:344/1395 train_time:41236ms step_avg:123.46ms
step:345/1395 train_time:41362ms step_avg:123.47ms
step:346/1395 train_time:41488ms step_avg:123.48ms
step:347/1395 train_time:41613ms step_avg:123.48ms
step:348/1395 train_time:41740ms step_avg:123.49ms
step:349/1395 train_time:41867ms step_avg:123.50ms
step:350/1395 train_time:41995ms step_avg:123.51ms
step:351/1395 train_time:42121ms step_avg:123.52ms
step:352/1395 train_time:42247ms step_avg:123.53ms
step:353/1395 train_time:42374ms step_avg:123.54ms
step:354/1395 train_time:42500ms step_avg:123.55ms
step:355/1395 train_time:42626ms step_avg:123.55ms
step:356/1395 train_time:42752ms step_avg:123.56ms
step:357/1395 train_time:42879ms step_avg:123.57ms
step:358/1395 train_time:43006ms step_avg:123.58ms
step:359/1395 train_time:43133ms step_avg:123.59ms
step:360/1395 train_time:43259ms step_avg:123.60ms
step:361/1395 train_time:43387ms step_avg:123.61ms
step:362/1395 train_time:43513ms step_avg:123.62ms
step:363/1395 train_time:43639ms step_avg:123.62ms
step:364/1395 train_time:43765ms step_avg:123.63ms
step:365/1395 train_time:43891ms step_avg:123.64ms
step:366/1395 train_time:44018ms step_avg:123.65ms
step:367/1395 train_time:44144ms step_avg:123.65ms
step:368/1395 train_time:44270ms step_avg:123.66ms
step:369/1395 train_time:44396ms step_avg:123.67ms
step:370/1395 train_time:44522ms step_avg:123.67ms
step:371/1395 train_time:44649ms step_avg:123.68ms
step:372/1395 train_time:44777ms step_avg:123.69ms
step:373/1395 train_time:44903ms step_avg:123.70ms
step:374/1395 train_time:45030ms step_avg:123.71ms
step:375/1395 train_time:45155ms step_avg:123.71ms
step:375/1395 val_loss:3.7844 train_time:45279ms step_avg:124.05ms
step:376/1395 train_time:45302ms step_avg:123.78ms
step:377/1395 train_time:45418ms step_avg:123.75ms
step:378/1395 train_time:45546ms step_avg:123.77ms
step:379/1395 train_time:45672ms step_avg:123.77ms
step:380/1395 train_time:45797ms step_avg:123.77ms
step:381/1395 train_time:45923ms step_avg:123.78ms
step:382/1395 train_time:46048ms step_avg:123.79ms
step:383/1395 train_time:46174ms step_avg:123.79ms
step:384/1395 train_time:46299ms step_avg:123.79ms
step:385/1395 train_time:46427ms step_avg:123.80ms
step:386/1395 train_time:46553ms step_avg:123.81ms
step:387/1395 train_time:46679ms step_avg:123.82ms
step:388/1395 train_time:46806ms step_avg:123.82ms
step:389/1395 train_time:46931ms step_avg:123.83ms
step:390/1395 train_time:47057ms step_avg:123.83ms
step:391/1395 train_time:47183ms step_avg:123.84ms
step:392/1395 train_time:47308ms step_avg:123.84ms
step:393/1395 train_time:47434ms step_avg:123.85ms
step:394/1395 train_time:47560ms step_avg:123.85ms
step:395/1395 train_time:47687ms step_avg:123.86ms
step:396/1395 train_time:47812ms step_avg:123.87ms
step:397/1395 train_time:47938ms step_avg:123.87ms
step:398/1395 train_time:48064ms step_avg:123.88ms
step:399/1395 train_time:48190ms step_avg:123.88ms
step:400/1395 train_time:48316ms step_avg:123.89ms
step:401/1395 train_time:48442ms step_avg:123.89ms
step:402/1395 train_time:48569ms step_avg:123.90ms
step:403/1395 train_time:48696ms step_avg:123.91ms
step:404/1395 train_time:48823ms step_avg:123.92ms
step:405/1395 train_time:48948ms step_avg:123.92ms
step:406/1395 train_time:49074ms step_avg:123.92ms
step:407/1395 train_time:49201ms step_avg:123.93ms
step:408/1395 train_time:49328ms step_avg:123.94ms
step:409/1395 train_time:49453ms step_avg:123.94ms
step:410/1395 train_time:49580ms step_avg:123.95ms
step:411/1395 train_time:49705ms step_avg:123.95ms
step:412/1395 train_time:49833ms step_avg:123.96ms
step:413/1395 train_time:49960ms step_avg:123.97ms
step:414/1395 train_time:50085ms step_avg:123.97ms
step:415/1395 train_time:50211ms step_avg:123.98ms
step:416/1395 train_time:50338ms step_avg:123.99ms
step:417/1395 train_time:50465ms step_avg:123.99ms
step:418/1395 train_time:50591ms step_avg:124.00ms
step:419/1395 train_time:50717ms step_avg:124.00ms
step:420/1395 train_time:50845ms step_avg:124.01ms
step:421/1395 train_time:50974ms step_avg:124.02ms
step:422/1395 train_time:51100ms step_avg:124.03ms
step:423/1395 train_time:51226ms step_avg:124.03ms
step:424/1395 train_time:51352ms step_avg:124.04ms
step:425/1395 train_time:51480ms step_avg:124.05ms
step:426/1395 train_time:51606ms step_avg:124.05ms
step:427/1395 train_time:51732ms step_avg:124.06ms
step:428/1395 train_time:51858ms step_avg:124.06ms
step:429/1395 train_time:51984ms step_avg:124.07ms
step:430/1395 train_time:52110ms step_avg:124.07ms
step:431/1395 train_time:52236ms step_avg:124.08ms
step:432/1395 train_time:52362ms step_avg:124.08ms
step:433/1395 train_time:52488ms step_avg:124.09ms
step:434/1395 train_time:52615ms step_avg:124.09ms
step:435/1395 train_time:52741ms step_avg:124.10ms
step:436/1395 train_time:52869ms step_avg:124.10ms
step:437/1395 train_time:52995ms step_avg:124.11ms
step:438/1395 train_time:53122ms step_avg:124.12ms
step:439/1395 train_time:53249ms step_avg:124.12ms
step:440/1395 train_time:53375ms step_avg:124.13ms
step:441/1395 train_time:53501ms step_avg:124.13ms
step:442/1395 train_time:53627ms step_avg:124.14ms
step:443/1395 train_time:53754ms step_avg:124.14ms
step:444/1395 train_time:53880ms step_avg:124.15ms
step:445/1395 train_time:54007ms step_avg:124.15ms
step:446/1395 train_time:54133ms step_avg:124.16ms
step:447/1395 train_time:54259ms step_avg:124.16ms
step:448/1395 train_time:54386ms step_avg:124.17ms
step:449/1395 train_time:54512ms step_avg:124.17ms
step:450/1395 train_time:54638ms step_avg:124.18ms
step:451/1395 train_time:54765ms step_avg:124.18ms
step:452/1395 train_time:54892ms step_avg:124.19ms
step:453/1395 train_time:55019ms step_avg:124.20ms
step:454/1395 train_time:55146ms step_avg:124.20ms
step:455/1395 train_time:55273ms step_avg:124.21ms
step:456/1395 train_time:55398ms step_avg:124.21ms
step:457/1395 train_time:55524ms step_avg:124.22ms
step:458/1395 train_time:55650ms step_avg:124.22ms
step:459/1395 train_time:55777ms step_avg:124.22ms
step:460/1395 train_time:55903ms step_avg:124.23ms
step:461/1395 train_time:56030ms step_avg:124.23ms
step:462/1395 train_time:56157ms step_avg:124.24ms
step:463/1395 train_time:56282ms step_avg:124.24ms
step:464/1395 train_time:56410ms step_avg:124.25ms
step:465/1395 train_time:56536ms step_avg:124.25ms
step:466/1395 train_time:56663ms step_avg:124.26ms
step:467/1395 train_time:56790ms step_avg:124.27ms
step:468/1395 train_time:56916ms step_avg:124.27ms
step:469/1395 train_time:57042ms step_avg:124.28ms
step:470/1395 train_time:57169ms step_avg:124.28ms
step:471/1395 train_time:57295ms step_avg:124.29ms
step:472/1395 train_time:57422ms step_avg:124.29ms
step:473/1395 train_time:57548ms step_avg:124.29ms
step:474/1395 train_time:57674ms step_avg:124.30ms
step:475/1395 train_time:57800ms step_avg:124.30ms
step:476/1395 train_time:57926ms step_avg:124.31ms
step:477/1395 train_time:58052ms step_avg:124.31ms
step:478/1395 train_time:58177ms step_avg:124.31ms
step:479/1395 train_time:58304ms step_avg:124.32ms
step:480/1395 train_time:58431ms step_avg:124.32ms
step:481/1395 train_time:58557ms step_avg:124.32ms
step:482/1395 train_time:58683ms step_avg:124.33ms
step:483/1395 train_time:58810ms step_avg:124.33ms
step:484/1395 train_time:58937ms step_avg:124.34ms
step:485/1395 train_time:59063ms step_avg:124.34ms
step:486/1395 train_time:59190ms step_avg:124.35ms
step:487/1395 train_time:59317ms step_avg:124.35ms
step:488/1395 train_time:59443ms step_avg:124.36ms
step:489/1395 train_time:59569ms step_avg:124.36ms
step:490/1395 train_time:59695ms step_avg:124.36ms
step:491/1395 train_time:59821ms step_avg:124.37ms
step:492/1395 train_time:59947ms step_avg:124.37ms
step:493/1395 train_time:60074ms step_avg:124.38ms
step:494/1395 train_time:60200ms step_avg:124.38ms
step:495/1395 train_time:60326ms step_avg:124.38ms
step:496/1395 train_time:60452ms step_avg:124.39ms
step:497/1395 train_time:60579ms step_avg:124.39ms
step:498/1395 train_time:60706ms step_avg:124.40ms
step:499/1395 train_time:60834ms step_avg:124.41ms
step:500/1395 train_time:60960ms step_avg:124.41ms
step:500/1395 val_loss:3.6650 train_time:61084ms step_avg:124.66ms
step:501/1395 train_time:61105ms step_avg:124.45ms
step:502/1395 train_time:61226ms step_avg:124.44ms
step:503/1395 train_time:61355ms step_avg:124.45ms
step:504/1395 train_time:61482ms step_avg:124.46ms
step:505/1395 train_time:61608ms step_avg:124.46ms
step:506/1395 train_time:61735ms step_avg:124.47ms
step:507/1395 train_time:61861ms step_avg:124.47ms
step:508/1395 train_time:61987ms step_avg:124.47ms
step:509/1395 train_time:62113ms step_avg:124.48ms
step:510/1395 train_time:62240ms step_avg:124.48ms
step:511/1395 train_time:62367ms step_avg:124.49ms
step:512/1395 train_time:62494ms step_avg:124.49ms
step:513/1395 train_time:62621ms step_avg:124.49ms
step:514/1395 train_time:62747ms step_avg:124.50ms
step:515/1395 train_time:62873ms step_avg:124.50ms
step:516/1395 train_time:62998ms step_avg:124.50ms
step:517/1395 train_time:63125ms step_avg:124.51ms
step:518/1395 train_time:63251ms step_avg:124.51ms
step:519/1395 train_time:63379ms step_avg:124.52ms
step:520/1395 train_time:63509ms step_avg:124.53ms
step:521/1395 train_time:63638ms step_avg:124.54ms
step:522/1395 train_time:63767ms step_avg:124.55ms
step:523/1395 train_time:63896ms step_avg:124.55ms
step:524/1395 train_time:64024ms step_avg:124.56ms
step:525/1395 train_time:64153ms step_avg:124.57ms
step:526/1395 train_time:64281ms step_avg:124.58ms
step:527/1395 train_time:64410ms step_avg:124.58ms
step:528/1395 train_time:64538ms step_avg:124.59ms
step:529/1395 train_time:64667ms step_avg:124.60ms
step:530/1395 train_time:64796ms step_avg:124.61ms
step:531/1395 train_time:64925ms step_avg:124.62ms
step:532/1395 train_time:65053ms step_avg:124.62ms
step:533/1395 train_time:65181ms step_avg:124.63ms
step:534/1395 train_time:65310ms step_avg:124.64ms
step:535/1395 train_time:65439ms step_avg:124.65ms
step:536/1395 train_time:65568ms step_avg:124.65ms
step:537/1395 train_time:65697ms step_avg:124.66ms
step:538/1395 train_time:65825ms step_avg:124.67ms
step:539/1395 train_time:65954ms step_avg:124.68ms
step:540/1395 train_time:66083ms step_avg:124.68ms
step:541/1395 train_time:66211ms step_avg:124.69ms
step:542/1395 train_time:66339ms step_avg:124.70ms
step:543/1395 train_time:66467ms step_avg:124.70ms
step:544/1395 train_time:66596ms step_avg:124.71ms
step:545/1395 train_time:66724ms step_avg:124.72ms
step:546/1395 train_time:66854ms step_avg:124.73ms
step:547/1395 train_time:66981ms step_avg:124.73ms
step:548/1395 train_time:67110ms step_avg:124.74ms
step:549/1395 train_time:67238ms step_avg:124.75ms
step:550/1395 train_time:67368ms step_avg:124.76ms
step:551/1395 train_time:67496ms step_avg:124.76ms
step:552/1395 train_time:67624ms step_avg:124.77ms
step:553/1395 train_time:67752ms step_avg:124.77ms
step:554/1395 train_time:67881ms step_avg:124.78ms
step:555/1395 train_time:68011ms step_avg:124.79ms
step:556/1395 train_time:68139ms step_avg:124.80ms
step:557/1395 train_time:68267ms step_avg:124.80ms
step:558/1395 train_time:68395ms step_avg:124.81ms
step:559/1395 train_time:68523ms step_avg:124.82ms
step:560/1395 train_time:68652ms step_avg:124.82ms
step:561/1395 train_time:68780ms step_avg:124.83ms
step:562/1395 train_time:68909ms step_avg:124.84ms
step:563/1395 train_time:69037ms step_avg:124.84ms
step:564/1395 train_time:69165ms step_avg:124.85ms
step:565/1395 train_time:69293ms step_avg:124.85ms
step:566/1395 train_time:69421ms step_avg:124.86ms
step:567/1395 train_time:69550ms step_avg:124.87ms
step:568/1395 train_time:69678ms step_avg:124.87ms
step:569/1395 train_time:69806ms step_avg:124.88ms
step:570/1395 train_time:69935ms step_avg:124.88ms
step:571/1395 train_time:70063ms step_avg:124.89ms
step:572/1395 train_time:70191ms step_avg:124.90ms
step:573/1395 train_time:70320ms step_avg:124.90ms
step:574/1395 train_time:70449ms step_avg:124.91ms
step:575/1395 train_time:70578ms step_avg:124.92ms
step:576/1395 train_time:70705ms step_avg:124.92ms
step:577/1395 train_time:70834ms step_avg:124.93ms
step:578/1395 train_time:70962ms step_avg:124.93ms
step:579/1395 train_time:71090ms step_avg:124.94ms
step:580/1395 train_time:71218ms step_avg:124.94ms
step:581/1395 train_time:71346ms step_avg:124.95ms
step:582/1395 train_time:71475ms step_avg:124.96ms
step:583/1395 train_time:71604ms step_avg:124.96ms
step:584/1395 train_time:71733ms step_avg:124.97ms
step:585/1395 train_time:71861ms step_avg:124.98ms
step:586/1395 train_time:71990ms step_avg:124.98ms
step:587/1395 train_time:72118ms step_avg:124.99ms
step:588/1395 train_time:72247ms step_avg:124.99ms
step:589/1395 train_time:72375ms step_avg:125.00ms
step:590/1395 train_time:72504ms step_avg:125.01ms
step:591/1395 train_time:72633ms step_avg:125.01ms
step:592/1395 train_time:72762ms step_avg:125.02ms
step:593/1395 train_time:72889ms step_avg:125.02ms
step:594/1395 train_time:73017ms step_avg:125.03ms
step:595/1395 train_time:73147ms step_avg:125.04ms
step:596/1395 train_time:73275ms step_avg:125.04ms
step:597/1395 train_time:73405ms step_avg:125.05ms
step:598/1395 train_time:73534ms step_avg:125.06ms
step:599/1395 train_time:73662ms step_avg:125.06ms
step:600/1395 train_time:73791ms step_avg:125.07ms
step:601/1395 train_time:73920ms step_avg:125.08ms
step:602/1395 train_time:74049ms step_avg:125.08ms
step:603/1395 train_time:74177ms step_avg:125.09ms
step:604/1395 train_time:74305ms step_avg:125.09ms
step:605/1395 train_time:74433ms step_avg:125.10ms
step:606/1395 train_time:74561ms step_avg:125.10ms
step:607/1395 train_time:74690ms step_avg:125.11ms
step:608/1395 train_time:74818ms step_avg:125.11ms
step:609/1395 train_time:74948ms step_avg:125.12ms
step:610/1395 train_time:75077ms step_avg:125.13ms
step:611/1395 train_time:75205ms step_avg:125.13ms
step:612/1395 train_time:75333ms step_avg:125.14ms
step:613/1395 train_time:75462ms step_avg:125.14ms
step:614/1395 train_time:75590ms step_avg:125.15ms
step:615/1395 train_time:75718ms step_avg:125.15ms
step:616/1395 train_time:75846ms step_avg:125.16ms
step:617/1395 train_time:75975ms step_avg:125.16ms
step:618/1395 train_time:76104ms step_avg:125.17ms
step:619/1395 train_time:76232ms step_avg:125.18ms
step:620/1395 train_time:76360ms step_avg:125.18ms
step:621/1395 train_time:76489ms step_avg:125.19ms
step:622/1395 train_time:76618ms step_avg:125.19ms
step:623/1395 train_time:76746ms step_avg:125.20ms
step:624/1395 train_time:76876ms step_avg:125.21ms
step:625/1395 train_time:77005ms step_avg:125.21ms
step:625/1395 val_loss:3.5804 train_time:77133ms step_avg:125.42ms
step:626/1395 train_time:77155ms step_avg:125.25ms
step:627/1395 train_time:77272ms step_avg:125.24ms
step:628/1395 train_time:77402ms step_avg:125.25ms
step:629/1395 train_time:77531ms step_avg:125.25ms
step:630/1395 train_time:77659ms step_avg:125.26ms
step:631/1395 train_time:77787ms step_avg:125.26ms
step:632/1395 train_time:77915ms step_avg:125.27ms
step:633/1395 train_time:78043ms step_avg:125.27ms
step:634/1395 train_time:78173ms step_avg:125.28ms
step:635/1395 train_time:78304ms step_avg:125.29ms
step:636/1395 train_time:78432ms step_avg:125.29ms
step:637/1395 train_time:78561ms step_avg:125.30ms
step:638/1395 train_time:78690ms step_avg:125.30ms
step:639/1395 train_time:78818ms step_avg:125.31ms
step:640/1395 train_time:78947ms step_avg:125.31ms
step:641/1395 train_time:79075ms step_avg:125.32ms
step:642/1395 train_time:79205ms step_avg:125.32ms
step:643/1395 train_time:79333ms step_avg:125.33ms
step:644/1395 train_time:79463ms step_avg:125.34ms
step:645/1395 train_time:79591ms step_avg:125.34ms
step:646/1395 train_time:79719ms step_avg:125.35ms
step:647/1395 train_time:79848ms step_avg:125.35ms
step:648/1395 train_time:79978ms step_avg:125.36ms
step:649/1395 train_time:80109ms step_avg:125.37ms
step:650/1395 train_time:80237ms step_avg:125.37ms
step:651/1395 train_time:80366ms step_avg:125.38ms
step:652/1395 train_time:80494ms step_avg:125.38ms
step:653/1395 train_time:80624ms step_avg:125.39ms
step:654/1395 train_time:80752ms step_avg:125.39ms
step:655/1395 train_time:80880ms step_avg:125.40ms
step:656/1395 train_time:81009ms step_avg:125.40ms
step:657/1395 train_time:81138ms step_avg:125.41ms
step:658/1395 train_time:81267ms step_avg:125.41ms
step:659/1395 train_time:81395ms step_avg:125.42ms
step:660/1395 train_time:81525ms step_avg:125.42ms
step:661/1395 train_time:81654ms step_avg:125.43ms
step:662/1395 train_time:81783ms step_avg:125.43ms
step:663/1395 train_time:81912ms step_avg:125.44ms
step:664/1395 train_time:82041ms step_avg:125.44ms
step:665/1395 train_time:82170ms step_avg:125.45ms
step:666/1395 train_time:82299ms step_avg:125.46ms
step:667/1395 train_time:82428ms step_avg:125.46ms
step:668/1395 train_time:82557ms step_avg:125.47ms
step:669/1395 train_time:82685ms step_avg:125.47ms
step:670/1395 train_time:82814ms step_avg:125.48ms
step:671/1395 train_time:82943ms step_avg:125.48ms
step:672/1395 train_time:83072ms step_avg:125.49ms
step:673/1395 train_time:83201ms step_avg:125.49ms
step:674/1395 train_time:83330ms step_avg:125.50ms
step:675/1395 train_time:83458ms step_avg:125.50ms
step:676/1395 train_time:83588ms step_avg:125.51ms
step:677/1395 train_time:83716ms step_avg:125.51ms
step:678/1395 train_time:83845ms step_avg:125.52ms
step:679/1395 train_time:83974ms step_avg:125.52ms
step:680/1395 train_time:84103ms step_avg:125.53ms
step:681/1395 train_time:84231ms step_avg:125.53ms
step:682/1395 train_time:84359ms step_avg:125.53ms
step:683/1395 train_time:84488ms step_avg:125.54ms
step:684/1395 train_time:84616ms step_avg:125.54ms
step:685/1395 train_time:84745ms step_avg:125.55ms
step:686/1395 train_time:84873ms step_avg:125.55ms
step:687/1395 train_time:85003ms step_avg:125.56ms
step:688/1395 train_time:85132ms step_avg:125.56ms
step:689/1395 train_time:85262ms step_avg:125.57ms
step:690/1395 train_time:85391ms step_avg:125.58ms
step:691/1395 train_time:85520ms step_avg:125.58ms
step:692/1395 train_time:85649ms step_avg:125.58ms
step:693/1395 train_time:85778ms step_avg:125.59ms
step:694/1395 train_time:85906ms step_avg:125.59ms
step:695/1395 train_time:86035ms step_avg:125.60ms
step:696/1395 train_time:86164ms step_avg:125.60ms
step:697/1395 train_time:86292ms step_avg:125.61ms
step:698/1395 train_time:86420ms step_avg:125.61ms
step:699/1395 train_time:86551ms step_avg:125.62ms
step:700/1395 train_time:86680ms step_avg:125.62ms
step:701/1395 train_time:86809ms step_avg:125.63ms
step:702/1395 train_time:86938ms step_avg:125.63ms
step:703/1395 train_time:87066ms step_avg:125.64ms
step:704/1395 train_time:87195ms step_avg:125.64ms
step:705/1395 train_time:87324ms step_avg:125.65ms
step:706/1395 train_time:87453ms step_avg:125.65ms
step:707/1395 train_time:87583ms step_avg:125.66ms
step:708/1395 train_time:87712ms step_avg:125.66ms
step:709/1395 train_time:87840ms step_avg:125.67ms
step:710/1395 train_time:87970ms step_avg:125.67ms
step:711/1395 train_time:88099ms step_avg:125.68ms
step:712/1395 train_time:88228ms step_avg:125.68ms
step:713/1395 train_time:88356ms step_avg:125.68ms
step:714/1395 train_time:88485ms step_avg:125.69ms
step:715/1395 train_time:88613ms step_avg:125.69ms
step:716/1395 train_time:88742ms step_avg:125.70ms
step:717/1395 train_time:88872ms step_avg:125.70ms
step:718/1395 train_time:89001ms step_avg:125.71ms
step:719/1395 train_time:89129ms step_avg:125.71ms
step:720/1395 train_time:89258ms step_avg:125.72ms
step:721/1395 train_time:89387ms step_avg:125.72ms
step:722/1395 train_time:89516ms step_avg:125.73ms
step:723/1395 train_time:89646ms step_avg:125.73ms
step:724/1395 train_time:89775ms step_avg:125.74ms
step:725/1395 train_time:89904ms step_avg:125.74ms
step:726/1395 train_time:90035ms step_avg:125.75ms
step:727/1395 train_time:90166ms step_avg:125.75ms
step:728/1395 train_time:90297ms step_avg:125.76ms
step:729/1395 train_time:90427ms step_avg:125.77ms
step:730/1395 train_time:90559ms step_avg:125.78ms
step:731/1395 train_time:90690ms step_avg:125.78ms
step:732/1395 train_time:90821ms step_avg:125.79ms
step:733/1395 train_time:90952ms step_avg:125.80ms
step:734/1395 train_time:91082ms step_avg:125.80ms
step:735/1395 train_time:91213ms step_avg:125.81ms
step:736/1395 train_time:91345ms step_avg:125.82ms
step:737/1395 train_time:91475ms step_avg:125.83ms
step:738/1395 train_time:91606ms step_avg:125.83ms
step:739/1395 train_time:91736ms step_avg:125.84ms
step:740/1395 train_time:91867ms step_avg:125.84ms
step:741/1395 train_time:92000ms step_avg:125.85ms
step:742/1395 train_time:92130ms step_avg:125.86ms
step:743/1395 train_time:92260ms step_avg:125.87ms
step:744/1395 train_time:92390ms step_avg:125.87ms
step:745/1395 train_time:92520ms step_avg:125.88ms
step:746/1395 train_time:92652ms step_avg:125.89ms
step:747/1395 train_time:92782ms step_avg:125.89ms
step:748/1395 train_time:92912ms step_avg:125.90ms
step:749/1395 train_time:93043ms step_avg:125.90ms
step:750/1395 train_time:93174ms step_avg:125.91ms
step:750/1395 val_loss:3.5274 train_time:93303ms step_avg:126.09ms
step:751/1395 train_time:93324ms step_avg:125.94ms
step:752/1395 train_time:93446ms step_avg:125.94ms
step:753/1395 train_time:93578ms step_avg:125.95ms
step:754/1395 train_time:93708ms step_avg:125.95ms
step:755/1395 train_time:93838ms step_avg:125.96ms
step:756/1395 train_time:93968ms step_avg:125.96ms
step:757/1395 train_time:94099ms step_avg:125.97ms
step:758/1395 train_time:94229ms step_avg:125.97ms
step:759/1395 train_time:94361ms step_avg:125.98ms
step:760/1395 train_time:94493ms step_avg:125.99ms
step:761/1395 train_time:94624ms step_avg:126.00ms
step:762/1395 train_time:94754ms step_avg:126.00ms
step:763/1395 train_time:94885ms step_avg:126.01ms
step:764/1395 train_time:95016ms step_avg:126.02ms
step:765/1395 train_time:95146ms step_avg:126.02ms
step:766/1395 train_time:95278ms step_avg:126.03ms
step:767/1395 train_time:95410ms step_avg:126.04ms
step:768/1395 train_time:95541ms step_avg:126.04ms
step:769/1395 train_time:95672ms step_avg:126.05ms
step:770/1395 train_time:95803ms step_avg:126.06ms
step:771/1395 train_time:95933ms step_avg:126.06ms
step:772/1395 train_time:96064ms step_avg:126.07ms
step:773/1395 train_time:96195ms step_avg:126.07ms
step:774/1395 train_time:96325ms step_avg:126.08ms
step:775/1395 train_time:96456ms step_avg:126.09ms
step:776/1395 train_time:96587ms step_avg:126.09ms
step:777/1395 train_time:96718ms step_avg:126.10ms
step:778/1395 train_time:96848ms step_avg:126.10ms
step:779/1395 train_time:96979ms step_avg:126.11ms
step:780/1395 train_time:97110ms step_avg:126.12ms
step:781/1395 train_time:97240ms step_avg:126.12ms
step:782/1395 train_time:97370ms step_avg:126.13ms
step:783/1395 train_time:97502ms step_avg:126.14ms
step:784/1395 train_time:97633ms step_avg:126.14ms
step:785/1395 train_time:97764ms step_avg:126.15ms
step:786/1395 train_time:97894ms step_avg:126.15ms
step:787/1395 train_time:98025ms step_avg:126.16ms
step:788/1395 train_time:98156ms step_avg:126.16ms
step:789/1395 train_time:98287ms step_avg:126.17ms
step:790/1395 train_time:98416ms step_avg:126.17ms
step:791/1395 train_time:98547ms step_avg:126.18ms
step:792/1395 train_time:98678ms step_avg:126.19ms
step:793/1395 train_time:98808ms step_avg:126.19ms
step:794/1395 train_time:98938ms step_avg:126.20ms
step:795/1395 train_time:99070ms step_avg:126.20ms
step:796/1395 train_time:99201ms step_avg:126.21ms
step:797/1395 train_time:99332ms step_avg:126.22ms
step:798/1395 train_time:99464ms step_avg:126.22ms
step:799/1395 train_time:99595ms step_avg:126.23ms
step:800/1395 train_time:99726ms step_avg:126.24ms
step:801/1395 train_time:99856ms step_avg:126.24ms
step:802/1395 train_time:99987ms step_avg:126.25ms
step:803/1395 train_time:100118ms step_avg:126.25ms
step:804/1395 train_time:100248ms step_avg:126.26ms
step:805/1395 train_time:100379ms step_avg:126.26ms
step:806/1395 train_time:100509ms step_avg:126.27ms
step:807/1395 train_time:100640ms step_avg:126.27ms
step:808/1395 train_time:100770ms step_avg:126.28ms
step:809/1395 train_time:100901ms step_avg:126.28ms
step:810/1395 train_time:101032ms step_avg:126.29ms
step:811/1395 train_time:101163ms step_avg:126.30ms
step:812/1395 train_time:101294ms step_avg:126.30ms
step:813/1395 train_time:101424ms step_avg:126.31ms
step:814/1395 train_time:101554ms step_avg:126.31ms
step:815/1395 train_time:101684ms step_avg:126.32ms
step:816/1395 train_time:101815ms step_avg:126.32ms
step:817/1395 train_time:101945ms step_avg:126.33ms
step:818/1395 train_time:102076ms step_avg:126.33ms
step:819/1395 train_time:102207ms step_avg:126.34ms
step:820/1395 train_time:102338ms step_avg:126.34ms
step:821/1395 train_time:102469ms step_avg:126.35ms
step:822/1395 train_time:102599ms step_avg:126.35ms
step:823/1395 train_time:102731ms step_avg:126.36ms
step:824/1395 train_time:102863ms step_avg:126.37ms
step:825/1395 train_time:102992ms step_avg:126.37ms
step:826/1395 train_time:103124ms step_avg:126.38ms
step:827/1395 train_time:103256ms step_avg:126.38ms
step:828/1395 train_time:103387ms step_avg:126.39ms
step:829/1395 train_time:103519ms step_avg:126.40ms
step:830/1395 train_time:103650ms step_avg:126.40ms
step:831/1395 train_time:103783ms step_avg:126.41ms
step:832/1395 train_time:103914ms step_avg:126.42ms
step:833/1395 train_time:104044ms step_avg:126.42ms
step:834/1395 train_time:104176ms step_avg:126.43ms
step:835/1395 train_time:104307ms step_avg:126.43ms
step:836/1395 train_time:104437ms step_avg:126.44ms
step:837/1395 train_time:104568ms step_avg:126.44ms
step:838/1395 train_time:104699ms step_avg:126.45ms
step:839/1395 train_time:104830ms step_avg:126.45ms
step:840/1395 train_time:104962ms step_avg:126.46ms
step:841/1395 train_time:105093ms step_avg:126.47ms
step:842/1395 train_time:105225ms step_avg:126.47ms
step:843/1395 train_time:105357ms step_avg:126.48ms
step:844/1395 train_time:105487ms step_avg:126.48ms
step:845/1395 train_time:105617ms step_avg:126.49ms
step:846/1395 train_time:105749ms step_avg:126.49ms
step:847/1395 train_time:105881ms step_avg:126.50ms
step:848/1395 train_time:106013ms step_avg:126.51ms
step:849/1395 train_time:106144ms step_avg:126.51ms
step:850/1395 train_time:106275ms step_avg:126.52ms
step:851/1395 train_time:106408ms step_avg:126.53ms
step:852/1395 train_time:106539ms step_avg:126.53ms
step:853/1395 train_time:106670ms step_avg:126.54ms
step:854/1395 train_time:106800ms step_avg:126.54ms
step:855/1395 train_time:106932ms step_avg:126.55ms
step:856/1395 train_time:107063ms step_avg:126.55ms
step:857/1395 train_time:107194ms step_avg:126.56ms
step:858/1395 train_time:107325ms step_avg:126.56ms
step:859/1395 train_time:107457ms step_avg:126.57ms
step:860/1395 train_time:107588ms step_avg:126.57ms
step:861/1395 train_time:107718ms step_avg:126.58ms
step:862/1395 train_time:107849ms step_avg:126.58ms
step:863/1395 train_time:107980ms step_avg:126.59ms
step:864/1395 train_time:108113ms step_avg:126.60ms
step:865/1395 train_time:108243ms step_avg:126.60ms
step:866/1395 train_time:108375ms step_avg:126.61ms
step:867/1395 train_time:108506ms step_avg:126.61ms
step:868/1395 train_time:108638ms step_avg:126.62ms
step:869/1395 train_time:108768ms step_avg:126.62ms
step:870/1395 train_time:108899ms step_avg:126.63ms
step:871/1395 train_time:109029ms step_avg:126.63ms
step:872/1395 train_time:109160ms step_avg:126.64ms
step:873/1395 train_time:109291ms step_avg:126.64ms
step:874/1395 train_time:109423ms step_avg:126.65ms
step:875/1395 train_time:109555ms step_avg:126.65ms
step:875/1395 val_loss:3.4778 train_time:109684ms step_avg:126.80ms
step:876/1395 train_time:109705ms step_avg:126.68ms
step:877/1395 train_time:109827ms step_avg:126.68ms
step:878/1395 train_time:109959ms step_avg:126.68ms
step:879/1395 train_time:110090ms step_avg:126.69ms
step:880/1395 train_time:110220ms step_avg:126.69ms
step:881/1395 train_time:110351ms step_avg:126.69ms
step:882/1395 train_time:110482ms step_avg:126.70ms
step:883/1395 train_time:110612ms step_avg:126.70ms
step:884/1395 train_time:110743ms step_avg:126.71ms
step:885/1395 train_time:110875ms step_avg:126.71ms
step:886/1395 train_time:111008ms step_avg:126.72ms
step:887/1395 train_time:111139ms step_avg:126.73ms
step:888/1395 train_time:111270ms step_avg:126.73ms
step:889/1395 train_time:111403ms step_avg:126.74ms
step:890/1395 train_time:111534ms step_avg:126.74ms
step:891/1395 train_time:111663ms step_avg:126.75ms
step:892/1395 train_time:111795ms step_avg:126.75ms
step:893/1395 train_time:111927ms step_avg:126.76ms
step:894/1395 train_time:112058ms step_avg:126.76ms
step:895/1395 train_time:112190ms step_avg:126.77ms
step:896/1395 train_time:112321ms step_avg:126.77ms
step:897/1395 train_time:112452ms step_avg:126.78ms
step:898/1395 train_time:112583ms step_avg:126.78ms
step:899/1395 train_time:112715ms step_avg:126.79ms
step:900/1395 train_time:112846ms step_avg:126.79ms
step:901/1395 train_time:112976ms step_avg:126.80ms
step:902/1395 train_time:113108ms step_avg:126.80ms
step:903/1395 train_time:113239ms step_avg:126.81ms
step:904/1395 train_time:113369ms step_avg:126.81ms
step:905/1395 train_time:113501ms step_avg:126.82ms
step:906/1395 train_time:113632ms step_avg:126.82ms
step:907/1395 train_time:113763ms step_avg:126.83ms
step:908/1395 train_time:113894ms step_avg:126.83ms
step:909/1395 train_time:114027ms step_avg:126.84ms
step:910/1395 train_time:114159ms step_avg:126.84ms
step:911/1395 train_time:114291ms step_avg:126.85ms
step:912/1395 train_time:114421ms step_avg:126.85ms
step:913/1395 train_time:114552ms step_avg:126.86ms
step:914/1395 train_time:114683ms step_avg:126.86ms
step:915/1395 train_time:114814ms step_avg:126.87ms
step:916/1395 train_time:114945ms step_avg:126.87ms
step:917/1395 train_time:115076ms step_avg:126.88ms
step:918/1395 train_time:115207ms step_avg:126.88ms
step:919/1395 train_time:115340ms step_avg:126.89ms
step:920/1395 train_time:115472ms step_avg:126.89ms
step:921/1395 train_time:115602ms step_avg:126.90ms
step:922/1395 train_time:115735ms step_avg:126.90ms
step:923/1395 train_time:115865ms step_avg:126.91ms
step:924/1395 train_time:115995ms step_avg:126.91ms
step:925/1395 train_time:116126ms step_avg:126.91ms
step:926/1395 train_time:116257ms step_avg:126.92ms
step:927/1395 train_time:116388ms step_avg:126.92ms
step:928/1395 train_time:116519ms step_avg:126.93ms
step:929/1395 train_time:116651ms step_avg:126.93ms
step:930/1395 train_time:116781ms step_avg:126.94ms
step:931/1395 train_time:116912ms step_avg:126.94ms
step:932/1395 train_time:117044ms step_avg:126.95ms
step:933/1395 train_time:117178ms step_avg:126.95ms
step:934/1395 train_time:117311ms step_avg:126.96ms
step:935/1395 train_time:117443ms step_avg:126.97ms
step:936/1395 train_time:117576ms step_avg:126.97ms
step:937/1395 train_time:117710ms step_avg:126.98ms
step:938/1395 train_time:117844ms step_avg:126.99ms
step:939/1395 train_time:117976ms step_avg:126.99ms
step:940/1395 train_time:118110ms step_avg:127.00ms
step:941/1395 train_time:118242ms step_avg:127.01ms
step:942/1395 train_time:118374ms step_avg:127.01ms
step:943/1395 train_time:118508ms step_avg:127.02ms
step:944/1395 train_time:118641ms step_avg:127.02ms
step:945/1395 train_time:118775ms step_avg:127.03ms
step:946/1395 train_time:118907ms step_avg:127.04ms
step:947/1395 train_time:119042ms step_avg:127.05ms
step:948/1395 train_time:119174ms step_avg:127.05ms
step:949/1395 train_time:119306ms step_avg:127.06ms
step:950/1395 train_time:119438ms step_avg:127.06ms
step:951/1395 train_time:119572ms step_avg:127.07ms
step:952/1395 train_time:119705ms step_avg:127.08ms
step:953/1395 train_time:119838ms step_avg:127.08ms
step:954/1395 train_time:119971ms step_avg:127.09ms
step:955/1395 train_time:120103ms step_avg:127.09ms
step:956/1395 train_time:120236ms step_avg:127.10ms
step:957/1395 train_time:120369ms step_avg:127.11ms
step:958/1395 train_time:120503ms step_avg:127.11ms
step:959/1395 train_time:120636ms step_avg:127.12ms
step:960/1395 train_time:120770ms step_avg:127.13ms
step:961/1395 train_time:120902ms step_avg:127.13ms
step:962/1395 train_time:121035ms step_avg:127.14ms
step:963/1395 train_time:121170ms step_avg:127.15ms
step:964/1395 train_time:121302ms step_avg:127.15ms
step:965/1395 train_time:121435ms step_avg:127.16ms
step:966/1395 train_time:121567ms step_avg:127.16ms
step:967/1395 train_time:121699ms step_avg:127.17ms
step:968/1395 train_time:121832ms step_avg:127.17ms
step:969/1395 train_time:121965ms step_avg:127.18ms
step:970/1395 train_time:122097ms step_avg:127.18ms
step:971/1395 train_time:122231ms step_avg:127.19ms
step:972/1395 train_time:122362ms step_avg:127.20ms
step:973/1395 train_time:122495ms step_avg:127.20ms
step:974/1395 train_time:122626ms step_avg:127.21ms
step:975/1395 train_time:122759ms step_avg:127.21ms
step:976/1395 train_time:122892ms step_avg:127.22ms
step:977/1395 train_time:123024ms step_avg:127.22ms
step:978/1395 train_time:123157ms step_avg:127.23ms
step:979/1395 train_time:123289ms step_avg:127.23ms
step:980/1395 train_time:123422ms step_avg:127.24ms
step:981/1395 train_time:123553ms step_avg:127.24ms
step:982/1395 train_time:123685ms step_avg:127.25ms
step:983/1395 train_time:123817ms step_avg:127.25ms
step:984/1395 train_time:123949ms step_avg:127.26ms
step:985/1395 train_time:124082ms step_avg:127.26ms
step:986/1395 train_time:124217ms step_avg:127.27ms
step:987/1395 train_time:124349ms step_avg:127.28ms
step:988/1395 train_time:124481ms step_avg:127.28ms
step:989/1395 train_time:124613ms step_avg:127.29ms
step:990/1395 train_time:124746ms step_avg:127.29ms
step:991/1395 train_time:124878ms step_avg:127.30ms
step:992/1395 train_time:125012ms step_avg:127.30ms
step:993/1395 train_time:125148ms step_avg:127.31ms
step:994/1395 train_time:125281ms step_avg:127.32ms
step:995/1395 train_time:125415ms step_avg:127.32ms
step:996/1395 train_time:125547ms step_avg:127.33ms
step:997/1395 train_time:125679ms step_avg:127.33ms
step:998/1395 train_time:125811ms step_avg:127.34ms
step:999/1395 train_time:125943ms step_avg:127.34ms
step:1000/1395 train_time:126075ms step_avg:127.35ms
step:1000/1395 val_loss:3.4141 train_time:126206ms step_avg:127.48ms
step:1001/1395 train_time:126227ms step_avg:127.37ms
step:1002/1395 train_time:126348ms step_avg:127.37ms
step:1003/1395 train_time:126482ms step_avg:127.37ms
step:1004/1395 train_time:126613ms step_avg:127.38ms
step:1005/1395 train_time:126747ms step_avg:127.38ms
step:1006/1395 train_time:126879ms step_avg:127.39ms
step:1007/1395 train_time:127012ms step_avg:127.39ms
step:1008/1395 train_time:127145ms step_avg:127.40ms
step:1009/1395 train_time:127279ms step_avg:127.41ms
step:1010/1395 train_time:127412ms step_avg:127.41ms
step:1011/1395 train_time:127547ms step_avg:127.42ms
step:1012/1395 train_time:127679ms step_avg:127.42ms
step:1013/1395 train_time:127812ms step_avg:127.43ms
step:1014/1395 train_time:127944ms step_avg:127.43ms
step:1015/1395 train_time:128076ms step_avg:127.44ms
step:1016/1395 train_time:128208ms step_avg:127.44ms
step:1017/1395 train_time:128341ms step_avg:127.45ms
step:1018/1395 train_time:128474ms step_avg:127.45ms
step:1019/1395 train_time:128608ms step_avg:127.46ms
step:1020/1395 train_time:128741ms step_avg:127.47ms
step:1021/1395 train_time:128873ms step_avg:127.47ms
step:1022/1395 train_time:129006ms step_avg:127.48ms
step:1023/1395 train_time:129139ms step_avg:127.48ms
step:1024/1395 train_time:129272ms step_avg:127.49ms
step:1025/1395 train_time:129405ms step_avg:127.49ms
step:1026/1395 train_time:129538ms step_avg:127.50ms
step:1027/1395 train_time:129670ms step_avg:127.50ms
step:1028/1395 train_time:129802ms step_avg:127.51ms
step:1029/1395 train_time:129936ms step_avg:127.51ms
step:1030/1395 train_time:130069ms step_avg:127.52ms
step:1031/1395 train_time:130201ms step_avg:127.52ms
step:1032/1395 train_time:130333ms step_avg:127.53ms
step:1033/1395 train_time:130466ms step_avg:127.53ms
step:1034/1395 train_time:130597ms step_avg:127.54ms
step:1035/1395 train_time:130730ms step_avg:127.54ms
step:1036/1395 train_time:130863ms step_avg:127.55ms
step:1037/1395 train_time:130995ms step_avg:127.55ms
step:1038/1395 train_time:131128ms step_avg:127.56ms
step:1039/1395 train_time:131260ms step_avg:127.56ms
step:1040/1395 train_time:131393ms step_avg:127.57ms
step:1041/1395 train_time:131527ms step_avg:127.57ms
step:1042/1395 train_time:131659ms step_avg:127.58ms
step:1043/1395 train_time:131792ms step_avg:127.58ms
step:1044/1395 train_time:131927ms step_avg:127.59ms
step:1045/1395 train_time:132060ms step_avg:127.59ms
step:1046/1395 train_time:132193ms step_avg:127.60ms
step:1047/1395 train_time:132325ms step_avg:127.60ms
step:1048/1395 train_time:132459ms step_avg:127.61ms
step:1049/1395 train_time:132591ms step_avg:127.61ms
step:1050/1395 train_time:132724ms step_avg:127.62ms
step:1051/1395 train_time:132858ms step_avg:127.63ms
step:1052/1395 train_time:132991ms step_avg:127.63ms
step:1053/1395 train_time:133124ms step_avg:127.64ms
step:1054/1395 train_time:133257ms step_avg:127.64ms
step:1055/1395 train_time:133390ms step_avg:127.65ms
step:1056/1395 train_time:133522ms step_avg:127.65ms
step:1057/1395 train_time:133654ms step_avg:127.65ms
step:1058/1395 train_time:133788ms step_avg:127.66ms
step:1059/1395 train_time:133920ms step_avg:127.66ms
step:1060/1395 train_time:134055ms step_avg:127.67ms
step:1061/1395 train_time:134188ms step_avg:127.68ms
step:1062/1395 train_time:134321ms step_avg:127.68ms
step:1063/1395 train_time:134452ms step_avg:127.68ms
step:1064/1395 train_time:134585ms step_avg:127.69ms
step:1065/1395 train_time:134718ms step_avg:127.69ms
step:1066/1395 train_time:134851ms step_avg:127.70ms
step:1067/1395 train_time:134986ms step_avg:127.71ms
step:1068/1395 train_time:135120ms step_avg:127.71ms
step:1069/1395 train_time:135253ms step_avg:127.72ms
step:1070/1395 train_time:135386ms step_avg:127.72ms
step:1071/1395 train_time:135519ms step_avg:127.73ms
step:1072/1395 train_time:135652ms step_avg:127.73ms
step:1073/1395 train_time:135784ms step_avg:127.74ms
step:1074/1395 train_time:135915ms step_avg:127.74ms
step:1075/1395 train_time:136049ms step_avg:127.75ms
step:1076/1395 train_time:136182ms step_avg:127.75ms
step:1077/1395 train_time:136314ms step_avg:127.75ms
step:1078/1395 train_time:136447ms step_avg:127.76ms
step:1079/1395 train_time:136584ms step_avg:127.77ms
step:1080/1395 train_time:136716ms step_avg:127.77ms
step:1081/1395 train_time:136849ms step_avg:127.78ms
step:1082/1395 train_time:136982ms step_avg:127.78ms
step:1083/1395 train_time:137115ms step_avg:127.79ms
step:1084/1395 train_time:137249ms step_avg:127.79ms
step:1085/1395 train_time:137382ms step_avg:127.80ms
step:1086/1395 train_time:137514ms step_avg:127.80ms
step:1087/1395 train_time:137648ms step_avg:127.81ms
step:1088/1395 train_time:137781ms step_avg:127.81ms
step:1089/1395 train_time:137917ms step_avg:127.82ms
step:1090/1395 train_time:138053ms step_avg:127.83ms
step:1091/1395 train_time:138186ms step_avg:127.83ms
step:1092/1395 train_time:138317ms step_avg:127.84ms
step:1093/1395 train_time:138450ms step_avg:127.84ms
step:1094/1395 train_time:138583ms step_avg:127.84ms
step:1095/1395 train_time:138714ms step_avg:127.85ms
step:1096/1395 train_time:138848ms step_avg:127.85ms
step:1097/1395 train_time:138982ms step_avg:127.86ms
step:1098/1395 train_time:139114ms step_avg:127.86ms
step:1099/1395 train_time:139247ms step_avg:127.87ms
step:1100/1395 train_time:139379ms step_avg:127.87ms
step:1101/1395 train_time:139511ms step_avg:127.87ms
step:1102/1395 train_time:139643ms step_avg:127.88ms
step:1103/1395 train_time:139777ms step_avg:127.88ms
step:1104/1395 train_time:139910ms step_avg:127.89ms
step:1105/1395 train_time:140044ms step_avg:127.89ms
step:1106/1395 train_time:140178ms step_avg:127.90ms
step:1107/1395 train_time:140311ms step_avg:127.90ms
step:1108/1395 train_time:140448ms step_avg:127.91ms
step:1109/1395 train_time:140582ms step_avg:127.92ms
step:1110/1395 train_time:140713ms step_avg:127.92ms
step:1111/1395 train_time:140847ms step_avg:127.93ms
step:1112/1395 train_time:140979ms step_avg:127.93ms
step:1113/1395 train_time:141111ms step_avg:127.93ms
step:1114/1395 train_time:141244ms step_avg:127.94ms
step:1115/1395 train_time:141377ms step_avg:127.94ms
step:1116/1395 train_time:141510ms step_avg:127.95ms
step:1117/1395 train_time:141643ms step_avg:127.95ms
step:1118/1395 train_time:141777ms step_avg:127.96ms
step:1119/1395 train_time:141910ms step_avg:127.96ms
step:1120/1395 train_time:142043ms step_avg:127.97ms
step:1121/1395 train_time:142176ms step_avg:127.97ms
step:1122/1395 train_time:142309ms step_avg:127.98ms
step:1123/1395 train_time:142441ms step_avg:127.98ms
step:1124/1395 train_time:142575ms step_avg:127.98ms
step:1125/1395 train_time:142708ms step_avg:127.99ms
step:1125/1395 val_loss:3.3633 train_time:142841ms step_avg:128.11ms
step:1126/1395 train_time:142862ms step_avg:128.01ms
step:1127/1395 train_time:142984ms step_avg:128.01ms
step:1128/1395 train_time:143117ms step_avg:128.01ms
step:1129/1395 train_time:143249ms step_avg:128.01ms
step:1130/1395 train_time:143381ms step_avg:128.02ms
step:1131/1395 train_time:143514ms step_avg:128.02ms
step:1132/1395 train_time:143646ms step_avg:128.03ms
step:1133/1395 train_time:143778ms step_avg:128.03ms
step:1134/1395 train_time:143914ms step_avg:128.04ms
step:1135/1395 train_time:144047ms step_avg:128.04ms
step:1136/1395 train_time:144184ms step_avg:128.05ms
step:1137/1395 train_time:144316ms step_avg:128.05ms
step:1138/1395 train_time:144450ms step_avg:128.06ms
step:1139/1395 train_time:144583ms step_avg:128.06ms
step:1140/1395 train_time:144719ms step_avg:128.07ms
step:1141/1395 train_time:144851ms step_avg:128.07ms
step:1142/1395 train_time:144986ms step_avg:128.08ms
step:1143/1395 train_time:145124ms step_avg:128.09ms
step:1144/1395 train_time:145259ms step_avg:128.09ms
step:1145/1395 train_time:145392ms step_avg:128.10ms
step:1146/1395 train_time:145526ms step_avg:128.10ms
step:1147/1395 train_time:145661ms step_avg:128.11ms
step:1148/1395 train_time:145794ms step_avg:128.11ms
step:1149/1395 train_time:145929ms step_avg:128.12ms
step:1150/1395 train_time:146063ms step_avg:128.13ms
step:1151/1395 train_time:146199ms step_avg:128.13ms
step:1152/1395 train_time:146333ms step_avg:128.14ms
step:1153/1395 train_time:146471ms step_avg:128.15ms
step:1154/1395 train_time:146604ms step_avg:128.15ms
step:1155/1395 train_time:146738ms step_avg:128.16ms
step:1156/1395 train_time:146874ms step_avg:128.16ms
step:1157/1395 train_time:147009ms step_avg:128.17ms
step:1158/1395 train_time:147143ms step_avg:128.17ms
step:1159/1395 train_time:147278ms step_avg:128.18ms
step:1160/1395 train_time:147411ms step_avg:128.18ms
step:1161/1395 train_time:147545ms step_avg:128.19ms
step:1162/1395 train_time:147680ms step_avg:128.19ms
step:1163/1395 train_time:147813ms step_avg:128.20ms
step:1164/1395 train_time:147948ms step_avg:128.20ms
step:1165/1395 train_time:148081ms step_avg:128.21ms
step:1166/1395 train_time:148214ms step_avg:128.21ms
step:1167/1395 train_time:148347ms step_avg:128.22ms
step:1168/1395 train_time:148483ms step_avg:128.22ms
step:1169/1395 train_time:148615ms step_avg:128.23ms
step:1170/1395 train_time:148750ms step_avg:128.23ms
step:1171/1395 train_time:148885ms step_avg:128.24ms
step:1172/1395 train_time:149020ms step_avg:128.24ms
step:1173/1395 train_time:149153ms step_avg:128.25ms
step:1174/1395 train_time:149293ms step_avg:128.26ms
step:1175/1395 train_time:149428ms step_avg:128.26ms
step:1176/1395 train_time:149563ms step_avg:128.27ms
step:1177/1395 train_time:149700ms step_avg:128.28ms
step:1178/1395 train_time:149833ms step_avg:128.28ms
step:1179/1395 train_time:149967ms step_avg:128.29ms
step:1180/1395 train_time:150104ms step_avg:128.29ms
step:1181/1395 train_time:150240ms step_avg:128.30ms
step:1182/1395 train_time:150374ms step_avg:128.31ms
step:1183/1395 train_time:150508ms step_avg:128.31ms
step:1184/1395 train_time:150643ms step_avg:128.32ms
step:1185/1395 train_time:150778ms step_avg:128.32ms
step:1186/1395 train_time:150912ms step_avg:128.33ms
step:1187/1395 train_time:151051ms step_avg:128.34ms
step:1188/1395 train_time:151185ms step_avg:128.34ms
step:1189/1395 train_time:151319ms step_avg:128.35ms
step:1190/1395 train_time:151454ms step_avg:128.35ms
step:1191/1395 train_time:151589ms step_avg:128.36ms
step:1192/1395 train_time:151722ms step_avg:128.36ms
step:1193/1395 train_time:151855ms step_avg:128.36ms
step:1194/1395 train_time:151989ms step_avg:128.37ms
step:1195/1395 train_time:152123ms step_avg:128.37ms
step:1196/1395 train_time:152257ms step_avg:128.38ms
step:1197/1395 train_time:152392ms step_avg:128.38ms
step:1198/1395 train_time:152531ms step_avg:128.39ms
step:1199/1395 train_time:152663ms step_avg:128.40ms
step:1200/1395 train_time:152797ms step_avg:128.40ms
step:1201/1395 train_time:152930ms step_avg:128.40ms
step:1202/1395 train_time:153069ms step_avg:128.41ms
step:1203/1395 train_time:153207ms step_avg:128.42ms
step:1204/1395 train_time:153341ms step_avg:128.43ms
step:1205/1395 train_time:153477ms step_avg:128.43ms
step:1206/1395 train_time:153611ms step_avg:128.44ms
step:1207/1395 train_time:153744ms step_avg:128.44ms
step:1208/1395 train_time:153878ms step_avg:128.45ms
step:1209/1395 train_time:154011ms step_avg:128.45ms
step:1210/1395 train_time:154148ms step_avg:128.46ms
step:1211/1395 train_time:154283ms step_avg:128.46ms
step:1212/1395 train_time:154417ms step_avg:128.47ms
step:1213/1395 train_time:154551ms step_avg:128.47ms
step:1214/1395 train_time:154687ms step_avg:128.48ms
step:1215/1395 train_time:154822ms step_avg:128.48ms
step:1216/1395 train_time:154955ms step_avg:128.49ms
step:1217/1395 train_time:155090ms step_avg:128.49ms
step:1218/1395 train_time:155223ms step_avg:128.50ms
step:1219/1395 train_time:155357ms step_avg:128.50ms
step:1220/1395 train_time:155491ms step_avg:128.50ms
step:1221/1395 train_time:155624ms step_avg:128.51ms
step:1222/1395 train_time:155758ms step_avg:128.51ms
step:1223/1395 train_time:155891ms step_avg:128.52ms
step:1224/1395 train_time:156025ms step_avg:128.52ms
step:1225/1395 train_time:156163ms step_avg:128.53ms
step:1226/1395 train_time:156297ms step_avg:128.53ms
step:1227/1395 train_time:156431ms step_avg:128.54ms
step:1228/1395 train_time:156568ms step_avg:128.55ms
step:1229/1395 train_time:156701ms step_avg:128.55ms
step:1230/1395 train_time:156836ms step_avg:128.55ms
step:1231/1395 train_time:156972ms step_avg:128.56ms
step:1232/1395 train_time:157109ms step_avg:128.57ms
step:1233/1395 train_time:157242ms step_avg:128.57ms
step:1234/1395 train_time:157375ms step_avg:128.57ms
step:1235/1395 train_time:157509ms step_avg:128.58ms
step:1236/1395 train_time:157645ms step_avg:128.58ms
step:1237/1395 train_time:157778ms step_avg:128.59ms
step:1238/1395 train_time:157917ms step_avg:128.60ms
step:1239/1395 train_time:158051ms step_avg:128.60ms
step:1240/1395 train_time:158186ms step_avg:128.61ms
step:1241/1395 train_time:158322ms step_avg:128.61ms
step:1242/1395 train_time:158457ms step_avg:128.62ms
step:1243/1395 train_time:158591ms step_avg:128.62ms
step:1244/1395 train_time:158727ms step_avg:128.63ms
step:1245/1395 train_time:158861ms step_avg:128.63ms
step:1246/1395 train_time:158995ms step_avg:128.64ms
step:1247/1395 train_time:159129ms step_avg:128.64ms
step:1248/1395 train_time:159262ms step_avg:128.64ms
step:1249/1395 train_time:159396ms step_avg:128.65ms
step:1250/1395 train_time:159530ms step_avg:128.65ms
step:1250/1395 val_loss:3.3162 train_time:159663ms step_avg:128.76ms
step:1251/1395 train_time:159684ms step_avg:128.67ms
step:1252/1395 train_time:159809ms step_avg:128.67ms
step:1253/1395 train_time:159944ms step_avg:128.68ms
step:1254/1395 train_time:160077ms step_avg:128.68ms
step:1255/1395 train_time:160216ms step_avg:128.69ms
step:1256/1395 train_time:160351ms step_avg:128.69ms
step:1257/1395 train_time:160485ms step_avg:128.70ms
step:1258/1395 train_time:160619ms step_avg:128.70ms
step:1259/1395 train_time:160755ms step_avg:128.71ms
step:1260/1395 train_time:160889ms step_avg:128.71ms
step:1261/1395 train_time:161023ms step_avg:128.72ms
step:1262/1395 train_time:161158ms step_avg:128.72ms
step:1263/1395 train_time:161294ms step_avg:128.73ms
step:1264/1395 train_time:161428ms step_avg:128.73ms
step:1265/1395 train_time:161562ms step_avg:128.73ms
step:1266/1395 train_time:161697ms step_avg:128.74ms
step:1267/1395 train_time:161830ms step_avg:128.74ms
step:1268/1395 train_time:161964ms step_avg:128.75ms
step:1269/1395 train_time:162099ms step_avg:128.75ms
step:1270/1395 train_time:162234ms step_avg:128.76ms
step:1271/1395 train_time:162370ms step_avg:128.76ms
step:1272/1395 train_time:162504ms step_avg:128.77ms
step:1273/1395 train_time:162637ms step_avg:128.77ms
step:1274/1395 train_time:162770ms step_avg:128.77ms
step:1275/1395 train_time:162906ms step_avg:128.78ms
step:1276/1395 train_time:163040ms step_avg:128.78ms
step:1277/1395 train_time:163174ms step_avg:128.79ms
step:1278/1395 train_time:163309ms step_avg:128.79ms
step:1279/1395 train_time:163444ms step_avg:128.80ms
step:1280/1395 train_time:163582ms step_avg:128.80ms
step:1281/1395 train_time:163716ms step_avg:128.81ms
step:1282/1395 train_time:163849ms step_avg:128.81ms
step:1283/1395 train_time:163982ms step_avg:128.82ms
step:1284/1395 train_time:164118ms step_avg:128.82ms
step:1285/1395 train_time:164250ms step_avg:128.82ms
step:1286/1395 train_time:164386ms step_avg:128.83ms
step:1287/1395 train_time:164521ms step_avg:128.83ms
step:1288/1395 train_time:164655ms step_avg:128.84ms
step:1289/1395 train_time:164791ms step_avg:128.84ms
step:1290/1395 train_time:164927ms step_avg:128.85ms
step:1291/1395 train_time:165063ms step_avg:128.85ms
step:1292/1395 train_time:165197ms step_avg:128.86ms
step:1293/1395 train_time:165335ms step_avg:128.87ms
step:1294/1395 train_time:165468ms step_avg:128.87ms
step:1295/1395 train_time:165604ms step_avg:128.87ms
step:1296/1395 train_time:165739ms step_avg:128.88ms
step:1297/1395 train_time:165874ms step_avg:128.88ms
step:1298/1395 train_time:166008ms step_avg:128.89ms
step:1299/1395 train_time:166143ms step_avg:128.89ms
step:1300/1395 train_time:166276ms step_avg:128.90ms
step:1301/1395 train_time:166409ms step_avg:128.90ms
step:1302/1395 train_time:166543ms step_avg:128.90ms
step:1303/1395 train_time:166679ms step_avg:128.91ms
step:1304/1395 train_time:166816ms step_avg:128.92ms
step:1305/1395 train_time:166951ms step_avg:128.92ms
step:1306/1395 train_time:167086ms step_avg:128.92ms
step:1307/1395 train_time:167221ms step_avg:128.93ms
step:1308/1395 train_time:167356ms step_avg:128.93ms
step:1309/1395 train_time:167490ms step_avg:128.94ms
step:1310/1395 train_time:167626ms step_avg:128.94ms
step:1311/1395 train_time:167759ms step_avg:128.95ms
step:1312/1395 train_time:167893ms step_avg:128.95ms
step:1313/1395 train_time:168029ms step_avg:128.96ms
step:1314/1395 train_time:168163ms step_avg:128.96ms
step:1315/1395 train_time:168299ms step_avg:128.96ms
step:1316/1395 train_time:168432ms step_avg:128.97ms
step:1317/1395 train_time:168566ms step_avg:128.97ms
step:1318/1395 train_time:168700ms step_avg:128.98ms
step:1319/1395 train_time:168837ms step_avg:128.98ms
step:1320/1395 train_time:168971ms step_avg:128.99ms
step:1321/1395 train_time:169106ms step_avg:128.99ms
step:1322/1395 train_time:169242ms step_avg:129.00ms
step:1323/1395 train_time:169376ms step_avg:129.00ms
step:1324/1395 train_time:169510ms step_avg:129.00ms
step:1325/1395 train_time:169647ms step_avg:129.01ms
step:1326/1395 train_time:169782ms step_avg:129.01ms
step:1327/1395 train_time:169916ms step_avg:129.02ms
step:1328/1395 train_time:170050ms step_avg:129.02ms
step:1329/1395 train_time:170190ms step_avg:129.03ms
step:1330/1395 train_time:170326ms step_avg:129.03ms
step:1331/1395 train_time:170463ms step_avg:129.04ms
step:1332/1395 train_time:170601ms step_avg:129.05ms
step:1333/1395 train_time:170736ms step_avg:129.05ms
step:1334/1395 train_time:170871ms step_avg:129.06ms
step:1335/1395 train_time:171005ms step_avg:129.06ms
step:1336/1395 train_time:171143ms step_avg:129.07ms
step:1337/1395 train_time:171277ms step_avg:129.07ms
step:1338/1395 train_time:171411ms step_avg:129.07ms
step:1339/1395 train_time:171545ms step_avg:129.08ms
step:1340/1395 train_time:171682ms step_avg:129.08ms
step:1341/1395 train_time:171815ms step_avg:129.09ms
step:1342/1395 train_time:171949ms step_avg:129.09ms
step:1343/1395 train_time:172084ms step_avg:129.10ms
step:1344/1395 train_time:172217ms step_avg:129.10ms
step:1345/1395 train_time:172353ms step_avg:129.10ms
step:1346/1395 train_time:172488ms step_avg:129.11ms
step:1347/1395 train_time:172624ms step_avg:129.11ms
step:1348/1395 train_time:172758ms step_avg:129.12ms
step:1349/1395 train_time:172895ms step_avg:129.12ms
step:1350/1395 train_time:173029ms step_avg:129.13ms
step:1351/1395 train_time:173165ms step_avg:129.13ms
step:1352/1395 train_time:173303ms step_avg:129.14ms
step:1353/1395 train_time:173440ms step_avg:129.14ms
step:1354/1395 train_time:173576ms step_avg:129.15ms
step:1355/1395 train_time:173711ms step_avg:129.15ms
step:1356/1395 train_time:173845ms step_avg:129.16ms
step:1357/1395 train_time:173981ms step_avg:129.16ms
step:1358/1395 train_time:174118ms step_avg:129.17ms
step:1359/1395 train_time:174255ms step_avg:129.17ms
step:1360/1395 train_time:174393ms step_avg:129.18ms
step:1361/1395 train_time:174528ms step_avg:129.18ms
step:1362/1395 train_time:174666ms step_avg:129.19ms
step:1363/1395 train_time:174803ms step_avg:129.20ms
step:1364/1395 train_time:174938ms step_avg:129.20ms
step:1365/1395 train_time:175071ms step_avg:129.20ms
step:1366/1395 train_time:175207ms step_avg:129.21ms
step:1367/1395 train_time:175343ms step_avg:129.21ms
step:1368/1395 train_time:175479ms step_avg:129.22ms
step:1369/1395 train_time:175617ms step_avg:129.23ms
step:1370/1395 train_time:175755ms step_avg:129.23ms
step:1371/1395 train_time:175892ms step_avg:129.24ms
step:1372/1395 train_time:176030ms step_avg:129.24ms
step:1373/1395 train_time:176165ms step_avg:129.25ms
step:1374/1395 train_time:176302ms step_avg:129.25ms
step:1375/1395 train_time:176437ms step_avg:129.26ms
step:1375/1395 val_loss:3.2819 train_time:176570ms step_avg:129.36ms
step:1376/1395 train_time:176590ms step_avg:129.28ms
step:1377/1395 train_time:176722ms step_avg:129.28ms
step:1378/1395 train_time:176856ms step_avg:129.28ms
step:1379/1395 train_time:176992ms step_avg:129.29ms
step:1380/1395 train_time:177127ms step_avg:129.29ms
step:1381/1395 train_time:177263ms step_avg:129.29ms
step:1382/1395 train_time:177399ms step_avg:129.30ms
step:1383/1395 train_time:177534ms step_avg:129.30ms
step:1384/1395 train_time:177671ms step_avg:129.31ms
step:1385/1395 train_time:177806ms step_avg:129.31ms
step:1386/1395 train_time:177942ms step_avg:129.32ms
step:1387/1395 train_time:178078ms step_avg:129.32ms
step:1388/1395 train_time:178213ms step_avg:129.33ms
step:1389/1395 train_time:178349ms step_avg:129.33ms
step:1390/1395 train_time:178486ms step_avg:129.34ms
step:1391/1395 train_time:178620ms step_avg:129.34ms
step:1392/1395 train_time:178757ms step_avg:129.35ms
step:1393/1395 train_time:178891ms step_avg:129.35ms
step:1394/1395 train_time:179026ms step_avg:129.35ms
step:1395/1395 train_time:179162ms step_avg:129.36ms
step:1395/1395 val_loss:3.2777 train_time:179297ms step_avg:129.46ms
peak memory allocated: 37653 MiB reserved: 39156 MiB
