import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True

# -----------------------------------------------------------------------------
# Custom operators

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

def lm_head_fp8(x: Tensor, w: Tensor) -> Tensor:
    _x = x.flatten(0, -2)
    out: Tensor = torch.ops.nanogpt.mm(_x, w, x_s=2.0, w_s=32.0, grad_s=2.0**29)[0]
    return out.reshape(*x.shape[:-1], -1)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev():
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int):
        super().__init__(in_features, out_features, bias=False)

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x):
        return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len=65536):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        # self.c_q = CastedLinear(dim, dim)
        # self.c_k = CastedLinear(dim, dim)
        # self.c_v = CastedLinear(dim, dim)
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std
        self.qkv_w = nn.Parameter(torch.empty(3, dim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads) # dim // num_heads = head_dim
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # Set attention scale such that the minimum attainable attention entropy
        # (but not necessary the attention entropy itself) is close to 0. By @leloykun
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        # q = self.c_q(x).view(B, T, self.num_heads, -1)
        # k = self.c_k(x).view(B, T, self.num_heads, -1)
        # v = self.c_v(x).view(B, T, self.num_heads, -1)
        # qkv_weight = torch.cat([self.c_q.weight, self.c_k.weight, self.c_v.weight], dim=0).type_as(x)
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3*self.num_heads, -1).chunk(3, dim=-2)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, model_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(model_dim, num_heads, layer_idx) if layer_idx != 7 else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, ve, x0, block_mask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, num_embeddings: int, embedding_dim: int):
        super().__init__()
        self.embed = nn.ModuleList([nn.Embedding(num_embeddings, embedding_dim) for _ in range(3)])

    def forward(self, input_seq) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2], None, None, None, None, None, None, ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual learning
        self.value_embeds = ValueEmbedding(vocab_size, model_dim)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128))
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        assert input_seq.ndim == 1
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        docs = (input_seq == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        def create_doc_swc_block_masks(sliding_window_num_blocks: Tensor):
            kv_idx = block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & document_bm
            full_bm  = causal_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            def build_bm(sw_num_blocks: Tensor) -> BlockMask:
                return BlockMask.from_kv_blocks(
                    torch.clamp_max(kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                    kv_indices,
                    torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                    full_kv_indices,
                    BLOCK_SIZE=BLOCK_SIZE,
                    mask_mod=document_causal,
                )
            return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

        # Long-short SWA block masks by @leloykun & @YouJiacheng
        long_bm, short_bm = create_doc_swc_block_masks(sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = lm_head_fp8(x, self.lm_head.weight) if self.training else self.lm_head(x)
        # @Grad62304977 added tanh softcapping, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use cycle(files) if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    batch_size = 8*64*1024 # batch size in tokens
    num_iterations = 1395 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 64*1024 # FlexAttention sequence length
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
             if console:
                 print(s)
             print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_loader = distributed_data_generator(args.train_files, args.batch_size, rank, world_size)

model = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim >= 2]
embed_params = [model.embed.weight, *model.value_embeds.parameters()]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), fused=True, eps=1e-10)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(it: int):
    t = 1 - it / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_bs = world_size * args.seq_len
        assert args.val_tokens % val_bs == 0
        val_steps = args.val_tokens // val_bs
        val_loader = distributed_data_generator(args.val_files, val_bs, rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION BEGIN -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB"
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250110+cu126 compiled for CUDA 12.6
Thu Jan 16 20:55:00 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   28C    P0             114W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   25C    P0             111W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   24C    P0             111W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   27C    P0             111W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   27C    P0             111W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   25C    P0             110W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1395 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1395 train_time:23612ms step_avg:nanms
step:2/1395 train_time:24580ms step_avg:nanms
step:3/1395 train_time:24736ms step_avg:nanms
step:4/1395 train_time:24911ms step_avg:nanms
step:5/1395 train_time:25031ms step_avg:nanms
step:6/1395 train_time:25152ms step_avg:nanms
step:7/1395 train_time:25273ms step_avg:nanms
step:8/1395 train_time:25394ms step_avg:nanms
step:9/1395 train_time:25515ms step_avg:nanms
step:10/1395 train_time:25636ms step_avg:nanms
step:11/1395 train_time:122ms step_avg:nanms
step:12/1395 train_time:248ms step_avg:nanms
step:13/1395 train_time:372ms step_avg:123.93ms
step:14/1395 train_time:494ms step_avg:123.42ms
step:15/1395 train_time:615ms step_avg:123.08ms
step:16/1395 train_time:737ms step_avg:122.80ms
step:17/1395 train_time:858ms step_avg:122.59ms
step:18/1395 train_time:979ms step_avg:122.42ms
step:19/1395 train_time:1100ms step_avg:122.26ms
step:20/1395 train_time:1223ms step_avg:122.26ms
step:21/1395 train_time:1347ms step_avg:122.42ms
step:22/1395 train_time:1468ms step_avg:122.30ms
step:23/1395 train_time:1590ms step_avg:122.28ms
step:24/1395 train_time:1711ms step_avg:122.22ms
step:25/1395 train_time:1833ms step_avg:122.19ms
step:26/1395 train_time:1955ms step_avg:122.18ms
step:27/1395 train_time:2077ms step_avg:122.16ms
step:28/1395 train_time:2198ms step_avg:122.11ms
step:29/1395 train_time:2321ms step_avg:122.16ms
step:30/1395 train_time:2443ms step_avg:122.14ms
step:31/1395 train_time:2564ms step_avg:122.10ms
step:32/1395 train_time:2687ms step_avg:122.16ms
step:33/1395 train_time:2809ms step_avg:122.12ms
step:34/1395 train_time:2930ms step_avg:122.10ms
step:35/1395 train_time:3052ms step_avg:122.09ms
step:36/1395 train_time:3175ms step_avg:122.12ms
step:37/1395 train_time:3296ms step_avg:122.09ms
step:38/1395 train_time:3419ms step_avg:122.09ms
step:39/1395 train_time:3542ms step_avg:122.13ms
step:40/1395 train_time:3665ms step_avg:122.17ms
step:41/1395 train_time:3788ms step_avg:122.18ms
step:42/1395 train_time:3910ms step_avg:122.19ms
step:43/1395 train_time:4032ms step_avg:122.19ms
step:44/1395 train_time:4154ms step_avg:122.18ms
step:45/1395 train_time:4275ms step_avg:122.15ms
step:46/1395 train_time:4397ms step_avg:122.14ms
step:47/1395 train_time:4519ms step_avg:122.14ms
step:48/1395 train_time:4641ms step_avg:122.13ms
step:49/1395 train_time:4763ms step_avg:122.12ms
step:50/1395 train_time:4886ms step_avg:122.14ms
step:51/1395 train_time:5007ms step_avg:122.13ms
step:52/1395 train_time:5130ms step_avg:122.14ms
step:53/1395 train_time:5252ms step_avg:122.13ms
step:54/1395 train_time:5373ms step_avg:122.12ms
step:55/1395 train_time:5495ms step_avg:122.11ms
step:56/1395 train_time:5618ms step_avg:122.13ms
step:57/1395 train_time:5739ms step_avg:122.11ms
step:58/1395 train_time:5862ms step_avg:122.12ms
step:59/1395 train_time:5986ms step_avg:122.16ms
step:60/1395 train_time:6106ms step_avg:122.12ms
step:61/1395 train_time:6228ms step_avg:122.12ms
step:62/1395 train_time:6350ms step_avg:122.11ms
step:63/1395 train_time:6471ms step_avg:122.10ms
step:64/1395 train_time:6594ms step_avg:122.10ms
step:65/1395 train_time:6716ms step_avg:122.12ms
step:66/1395 train_time:6838ms step_avg:122.11ms
step:67/1395 train_time:6960ms step_avg:122.10ms
step:68/1395 train_time:7081ms step_avg:122.08ms
step:69/1395 train_time:7203ms step_avg:122.09ms
step:70/1395 train_time:7324ms step_avg:122.07ms
step:71/1395 train_time:7447ms step_avg:122.08ms
step:72/1395 train_time:7570ms step_avg:122.10ms
step:73/1395 train_time:7693ms step_avg:122.11ms
step:74/1395 train_time:7815ms step_avg:122.10ms
step:75/1395 train_time:7936ms step_avg:122.10ms
step:76/1395 train_time:8058ms step_avg:122.08ms
step:77/1395 train_time:8179ms step_avg:122.08ms
step:78/1395 train_time:8300ms step_avg:122.06ms
step:79/1395 train_time:8421ms step_avg:122.05ms
step:80/1395 train_time:8544ms step_avg:122.06ms
step:81/1395 train_time:8668ms step_avg:122.08ms
step:82/1395 train_time:8789ms step_avg:122.07ms
step:83/1395 train_time:8911ms step_avg:122.07ms
step:84/1395 train_time:9033ms step_avg:122.07ms
step:85/1395 train_time:9155ms step_avg:122.06ms
step:86/1395 train_time:9276ms step_avg:122.05ms
step:87/1395 train_time:9400ms step_avg:122.08ms
step:88/1395 train_time:9521ms step_avg:122.07ms
step:89/1395 train_time:9644ms step_avg:122.08ms
step:90/1395 train_time:9766ms step_avg:122.08ms
step:91/1395 train_time:9888ms step_avg:122.07ms
step:92/1395 train_time:10010ms step_avg:122.07ms
step:93/1395 train_time:10132ms step_avg:122.07ms
step:94/1395 train_time:10254ms step_avg:122.07ms
step:95/1395 train_time:10375ms step_avg:122.06ms
step:96/1395 train_time:10497ms step_avg:122.05ms
step:97/1395 train_time:10618ms step_avg:122.05ms
step:98/1395 train_time:10741ms step_avg:122.05ms
step:99/1395 train_time:10863ms step_avg:122.05ms
step:100/1395 train_time:10988ms step_avg:122.09ms
step:101/1395 train_time:11108ms step_avg:122.06ms
step:102/1395 train_time:11229ms step_avg:122.06ms
step:103/1395 train_time:11352ms step_avg:122.06ms
step:104/1395 train_time:11473ms step_avg:122.06ms
step:105/1395 train_time:11595ms step_avg:122.05ms
step:106/1395 train_time:11717ms step_avg:122.05ms
step:107/1395 train_time:11840ms step_avg:122.06ms
step:108/1395 train_time:11963ms step_avg:122.08ms
step:109/1395 train_time:12085ms step_avg:122.07ms
step:110/1395 train_time:12208ms step_avg:122.08ms
step:111/1395 train_time:12331ms step_avg:122.09ms
step:112/1395 train_time:12454ms step_avg:122.10ms
step:113/1395 train_time:12576ms step_avg:122.10ms
step:114/1395 train_time:12699ms step_avg:122.10ms
step:115/1395 train_time:12821ms step_avg:122.11ms
step:116/1395 train_time:12945ms step_avg:122.12ms
step:117/1395 train_time:13067ms step_avg:122.12ms
step:118/1395 train_time:13190ms step_avg:122.13ms
step:119/1395 train_time:13312ms step_avg:122.13ms
step:120/1395 train_time:13435ms step_avg:122.14ms
step:121/1395 train_time:13557ms step_avg:122.13ms
step:122/1395 train_time:13679ms step_avg:122.13ms
step:123/1395 train_time:13803ms step_avg:122.15ms
step:124/1395 train_time:13925ms step_avg:122.15ms
step:125/1395 train_time:14047ms step_avg:122.15ms
step:125/1395 val_loss:4.4109 train_time:14168ms step_avg:123.20ms
step:126/1395 train_time:14189ms step_avg:122.32ms
step:127/1395 train_time:14305ms step_avg:122.27ms
step:128/1395 train_time:14431ms step_avg:122.29ms
step:129/1395 train_time:14554ms step_avg:122.30ms
step:130/1395 train_time:14676ms step_avg:122.30ms
step:131/1395 train_time:14798ms step_avg:122.30ms
step:132/1395 train_time:14921ms step_avg:122.30ms
step:133/1395 train_time:15043ms step_avg:122.30ms
step:134/1395 train_time:15165ms step_avg:122.30ms
step:135/1395 train_time:15287ms step_avg:122.30ms
step:136/1395 train_time:15410ms step_avg:122.30ms
step:137/1395 train_time:15534ms step_avg:122.32ms
step:138/1395 train_time:15656ms step_avg:122.31ms
step:139/1395 train_time:15779ms step_avg:122.32ms
step:140/1395 train_time:15901ms step_avg:122.31ms
step:141/1395 train_time:16023ms step_avg:122.31ms
step:142/1395 train_time:16145ms step_avg:122.31ms
step:143/1395 train_time:16267ms step_avg:122.31ms
step:144/1395 train_time:16390ms step_avg:122.31ms
step:145/1395 train_time:16515ms step_avg:122.33ms
step:146/1395 train_time:16637ms step_avg:122.33ms
step:147/1395 train_time:16760ms step_avg:122.34ms
step:148/1395 train_time:16884ms step_avg:122.35ms
step:149/1395 train_time:17005ms step_avg:122.34ms
step:150/1395 train_time:17128ms step_avg:122.34ms
step:151/1395 train_time:17251ms step_avg:122.35ms
step:152/1395 train_time:17374ms step_avg:122.35ms
step:153/1395 train_time:17496ms step_avg:122.35ms
step:154/1395 train_time:17620ms step_avg:122.36ms
step:155/1395 train_time:17742ms step_avg:122.36ms
step:156/1395 train_time:17864ms step_avg:122.35ms
step:157/1395 train_time:17986ms step_avg:122.35ms
step:158/1395 train_time:18108ms step_avg:122.35ms
step:159/1395 train_time:18231ms step_avg:122.36ms
step:160/1395 train_time:18353ms step_avg:122.35ms
step:161/1395 train_time:18476ms step_avg:122.36ms
step:162/1395 train_time:18600ms step_avg:122.37ms
step:163/1395 train_time:18723ms step_avg:122.37ms
step:164/1395 train_time:18847ms step_avg:122.38ms
step:165/1395 train_time:18970ms step_avg:122.38ms
step:166/1395 train_time:19092ms step_avg:122.39ms
step:167/1395 train_time:19215ms step_avg:122.39ms
step:168/1395 train_time:19338ms step_avg:122.39ms
step:169/1395 train_time:19460ms step_avg:122.39ms
step:170/1395 train_time:19584ms step_avg:122.40ms
step:171/1395 train_time:19707ms step_avg:122.40ms
step:172/1395 train_time:19830ms step_avg:122.41ms
step:173/1395 train_time:19954ms step_avg:122.42ms
step:174/1395 train_time:20077ms step_avg:122.42ms
step:175/1395 train_time:20200ms step_avg:122.42ms
step:176/1395 train_time:20323ms step_avg:122.43ms
step:177/1395 train_time:20445ms step_avg:122.43ms
step:178/1395 train_time:20567ms step_avg:122.42ms
step:179/1395 train_time:20690ms step_avg:122.43ms
step:180/1395 train_time:20814ms step_avg:122.43ms
step:181/1395 train_time:20937ms step_avg:122.44ms
step:182/1395 train_time:21059ms step_avg:122.43ms
step:183/1395 train_time:21180ms step_avg:122.43ms
step:184/1395 train_time:21303ms step_avg:122.43ms
step:185/1395 train_time:21425ms step_avg:122.43ms
step:186/1395 train_time:21548ms step_avg:122.43ms
step:187/1395 train_time:21670ms step_avg:122.43ms
step:188/1395 train_time:21793ms step_avg:122.43ms
step:189/1395 train_time:21917ms step_avg:122.44ms
step:190/1395 train_time:22039ms step_avg:122.44ms
step:191/1395 train_time:22161ms step_avg:122.44ms
step:192/1395 train_time:22284ms step_avg:122.44ms
step:193/1395 train_time:22406ms step_avg:122.44ms
step:194/1395 train_time:22528ms step_avg:122.44ms
step:195/1395 train_time:22652ms step_avg:122.44ms
step:196/1395 train_time:22774ms step_avg:122.44ms
step:197/1395 train_time:22897ms step_avg:122.44ms
step:198/1395 train_time:23021ms step_avg:122.45ms
step:199/1395 train_time:23143ms step_avg:122.45ms
step:200/1395 train_time:23266ms step_avg:122.45ms
step:201/1395 train_time:23389ms step_avg:122.45ms
step:202/1395 train_time:23511ms step_avg:122.45ms
step:203/1395 train_time:23634ms step_avg:122.46ms
step:204/1395 train_time:23757ms step_avg:122.46ms
step:205/1395 train_time:23880ms step_avg:122.46ms
step:206/1395 train_time:24004ms step_avg:122.47ms
step:207/1395 train_time:24128ms step_avg:122.48ms
step:208/1395 train_time:24251ms step_avg:122.48ms
step:209/1395 train_time:24375ms step_avg:122.49ms
step:210/1395 train_time:24499ms step_avg:122.50ms
step:211/1395 train_time:24623ms step_avg:122.50ms
step:212/1395 train_time:24747ms step_avg:122.51ms
step:213/1395 train_time:24870ms step_avg:122.51ms
step:214/1395 train_time:24994ms step_avg:122.52ms
step:215/1395 train_time:25117ms step_avg:122.52ms
step:216/1395 train_time:25240ms step_avg:122.53ms
step:217/1395 train_time:25363ms step_avg:122.53ms
step:218/1395 train_time:25486ms step_avg:122.53ms
step:219/1395 train_time:25610ms step_avg:122.53ms
step:220/1395 train_time:25734ms step_avg:122.54ms
step:221/1395 train_time:25860ms step_avg:122.56ms
step:222/1395 train_time:25981ms step_avg:122.55ms
step:223/1395 train_time:26103ms step_avg:122.55ms
step:224/1395 train_time:26228ms step_avg:122.56ms
step:225/1395 train_time:26353ms step_avg:122.57ms
step:226/1395 train_time:26477ms step_avg:122.58ms
step:227/1395 train_time:26600ms step_avg:122.58ms
step:228/1395 train_time:26723ms step_avg:122.58ms
step:229/1395 train_time:26845ms step_avg:122.58ms
step:230/1395 train_time:26968ms step_avg:122.58ms
step:231/1395 train_time:27091ms step_avg:122.59ms
step:232/1395 train_time:27216ms step_avg:122.59ms
step:233/1395 train_time:27340ms step_avg:122.60ms
step:234/1395 train_time:27464ms step_avg:122.61ms
step:235/1395 train_time:27586ms step_avg:122.60ms
step:236/1395 train_time:27711ms step_avg:122.61ms
step:237/1395 train_time:27835ms step_avg:122.62ms
step:238/1395 train_time:27964ms step_avg:122.65ms
step:239/1395 train_time:28081ms step_avg:122.63ms
step:240/1395 train_time:28203ms step_avg:122.62ms
step:241/1395 train_time:28327ms step_avg:122.63ms
step:242/1395 train_time:28451ms step_avg:122.63ms
step:243/1395 train_time:28574ms step_avg:122.64ms
step:244/1395 train_time:28697ms step_avg:122.64ms
step:245/1395 train_time:28820ms step_avg:122.64ms
step:246/1395 train_time:28944ms step_avg:122.64ms
step:247/1395 train_time:29068ms step_avg:122.65ms
step:248/1395 train_time:29191ms step_avg:122.65ms
step:249/1395 train_time:29317ms step_avg:122.66ms
step:250/1395 train_time:29440ms step_avg:122.67ms
step:250/1395 val_loss:3.9855 train_time:29562ms step_avg:123.17ms
step:251/1395 train_time:29582ms step_avg:122.75ms
step:252/1395 train_time:29700ms step_avg:122.73ms
step:253/1395 train_time:29825ms step_avg:122.74ms
step:254/1395 train_time:29948ms step_avg:122.74ms
step:255/1395 train_time:30071ms step_avg:122.74ms
step:256/1395 train_time:30194ms step_avg:122.74ms
step:257/1395 train_time:30317ms step_avg:122.74ms
step:258/1395 train_time:30439ms step_avg:122.74ms
step:259/1395 train_time:30563ms step_avg:122.74ms
step:260/1395 train_time:30689ms step_avg:122.76ms
step:261/1395 train_time:30813ms step_avg:122.76ms
step:262/1395 train_time:30936ms step_avg:122.76ms
step:263/1395 train_time:31059ms step_avg:122.76ms
step:264/1395 train_time:31186ms step_avg:122.78ms
step:265/1395 train_time:31306ms step_avg:122.77ms
step:266/1395 train_time:31430ms step_avg:122.77ms
step:267/1395 train_time:31553ms step_avg:122.77ms
step:268/1395 train_time:31677ms step_avg:122.78ms
step:269/1395 train_time:31801ms step_avg:122.78ms
step:270/1395 train_time:31923ms step_avg:122.78ms
step:271/1395 train_time:32046ms step_avg:122.78ms
step:272/1395 train_time:32170ms step_avg:122.79ms
step:273/1395 train_time:32293ms step_avg:122.79ms
step:274/1395 train_time:32416ms step_avg:122.79ms
step:275/1395 train_time:32538ms step_avg:122.79ms
step:276/1395 train_time:32663ms step_avg:122.80ms
step:277/1395 train_time:32789ms step_avg:122.80ms
step:278/1395 train_time:32912ms step_avg:122.81ms
step:279/1395 train_time:33035ms step_avg:122.81ms
step:280/1395 train_time:33157ms step_avg:122.81ms
step:281/1395 train_time:33281ms step_avg:122.81ms
step:282/1395 train_time:33403ms step_avg:122.81ms
step:283/1395 train_time:33527ms step_avg:122.81ms
step:284/1395 train_time:33650ms step_avg:122.81ms
step:285/1395 train_time:33774ms step_avg:122.81ms
step:286/1395 train_time:33899ms step_avg:122.82ms
step:287/1395 train_time:34022ms step_avg:122.82ms
step:288/1395 train_time:34145ms step_avg:122.82ms
step:289/1395 train_time:34269ms step_avg:122.83ms
step:290/1395 train_time:34393ms step_avg:122.83ms
step:291/1395 train_time:34516ms step_avg:122.83ms
step:292/1395 train_time:34639ms step_avg:122.83ms
step:293/1395 train_time:34764ms step_avg:122.84ms
step:294/1395 train_time:34889ms step_avg:122.85ms
step:295/1395 train_time:35012ms step_avg:122.85ms
step:296/1395 train_time:35136ms step_avg:122.85ms
step:297/1395 train_time:35259ms step_avg:122.85ms
step:298/1395 train_time:35382ms step_avg:122.86ms
step:299/1395 train_time:35506ms step_avg:122.86ms
step:300/1395 train_time:35629ms step_avg:122.86ms
step:301/1395 train_time:35752ms step_avg:122.86ms
step:302/1395 train_time:35877ms step_avg:122.87ms
step:303/1395 train_time:36000ms step_avg:122.87ms
step:304/1395 train_time:36122ms step_avg:122.87ms
step:305/1395 train_time:36246ms step_avg:122.87ms
step:306/1395 train_time:36371ms step_avg:122.87ms
step:307/1395 train_time:36495ms step_avg:122.88ms
step:308/1395 train_time:36618ms step_avg:122.88ms
step:309/1395 train_time:36742ms step_avg:122.88ms
step:310/1395 train_time:36865ms step_avg:122.88ms
step:311/1395 train_time:36989ms step_avg:122.89ms
step:312/1395 train_time:37113ms step_avg:122.89ms
step:313/1395 train_time:37238ms step_avg:122.90ms
step:314/1395 train_time:37364ms step_avg:122.91ms
step:315/1395 train_time:37490ms step_avg:122.92ms
step:316/1395 train_time:37615ms step_avg:122.93ms
step:317/1395 train_time:37742ms step_avg:122.94ms
step:318/1395 train_time:37868ms step_avg:122.95ms
step:319/1395 train_time:37994ms step_avg:122.96ms
step:320/1395 train_time:38120ms step_avg:122.97ms
step:321/1395 train_time:38247ms step_avg:122.98ms
step:322/1395 train_time:38373ms step_avg:122.99ms
step:323/1395 train_time:38499ms step_avg:123.00ms
step:324/1395 train_time:38625ms step_avg:123.01ms
step:325/1395 train_time:38752ms step_avg:123.02ms
step:326/1395 train_time:38877ms step_avg:123.03ms
step:327/1395 train_time:39002ms step_avg:123.04ms
step:328/1395 train_time:39128ms step_avg:123.04ms
step:329/1395 train_time:39254ms step_avg:123.05ms
step:330/1395 train_time:39381ms step_avg:123.06ms
step:331/1395 train_time:39506ms step_avg:123.07ms
step:332/1395 train_time:39632ms step_avg:123.08ms
step:333/1395 train_time:39759ms step_avg:123.09ms
step:334/1395 train_time:39884ms step_avg:123.10ms
step:335/1395 train_time:40010ms step_avg:123.11ms
step:336/1395 train_time:40136ms step_avg:123.12ms
step:337/1395 train_time:40262ms step_avg:123.12ms
step:338/1395 train_time:40387ms step_avg:123.13ms
step:339/1395 train_time:40513ms step_avg:123.14ms
step:340/1395 train_time:40639ms step_avg:123.15ms
step:341/1395 train_time:40765ms step_avg:123.16ms
step:342/1395 train_time:40891ms step_avg:123.17ms
step:343/1395 train_time:41018ms step_avg:123.18ms
step:344/1395 train_time:41144ms step_avg:123.19ms
step:345/1395 train_time:41270ms step_avg:123.19ms
step:346/1395 train_time:41395ms step_avg:123.20ms
step:347/1395 train_time:41520ms step_avg:123.21ms
step:348/1395 train_time:41646ms step_avg:123.21ms
step:349/1395 train_time:41773ms step_avg:123.22ms
step:350/1395 train_time:41899ms step_avg:123.23ms
step:351/1395 train_time:42024ms step_avg:123.24ms
step:352/1395 train_time:42151ms step_avg:123.25ms
step:353/1395 train_time:42277ms step_avg:123.26ms
step:354/1395 train_time:42403ms step_avg:123.26ms
step:355/1395 train_time:42529ms step_avg:123.27ms
step:356/1395 train_time:42654ms step_avg:123.28ms
step:357/1395 train_time:42780ms step_avg:123.28ms
step:358/1395 train_time:42906ms step_avg:123.29ms
step:359/1395 train_time:43032ms step_avg:123.30ms
step:360/1395 train_time:43157ms step_avg:123.31ms
step:361/1395 train_time:43284ms step_avg:123.31ms
step:362/1395 train_time:43409ms step_avg:123.32ms
step:363/1395 train_time:43536ms step_avg:123.33ms
step:364/1395 train_time:43662ms step_avg:123.34ms
step:365/1395 train_time:43790ms step_avg:123.35ms
step:366/1395 train_time:43914ms step_avg:123.35ms
step:367/1395 train_time:44040ms step_avg:123.36ms
step:368/1395 train_time:44166ms step_avg:123.37ms
step:369/1395 train_time:44292ms step_avg:123.38ms
step:370/1395 train_time:44418ms step_avg:123.38ms
step:371/1395 train_time:44543ms step_avg:123.39ms
step:372/1395 train_time:44670ms step_avg:123.40ms
step:373/1395 train_time:44797ms step_avg:123.41ms
step:374/1395 train_time:44923ms step_avg:123.41ms
step:375/1395 train_time:45049ms step_avg:123.42ms
step:375/1395 val_loss:3.7820 train_time:45173ms step_avg:123.76ms
step:376/1395 train_time:45193ms step_avg:123.48ms
step:377/1395 train_time:45312ms step_avg:123.46ms
step:378/1395 train_time:45440ms step_avg:123.48ms
step:379/1395 train_time:45565ms step_avg:123.48ms
step:380/1395 train_time:45691ms step_avg:123.49ms
step:381/1395 train_time:45816ms step_avg:123.49ms
step:382/1395 train_time:45941ms step_avg:123.50ms
step:383/1395 train_time:46066ms step_avg:123.50ms
step:384/1395 train_time:46192ms step_avg:123.51ms
step:385/1395 train_time:46319ms step_avg:123.52ms
step:386/1395 train_time:46446ms step_avg:123.53ms
step:387/1395 train_time:46572ms step_avg:123.53ms
step:388/1395 train_time:46697ms step_avg:123.54ms
step:389/1395 train_time:46822ms step_avg:123.54ms
step:390/1395 train_time:46948ms step_avg:123.55ms
step:391/1395 train_time:47073ms step_avg:123.55ms
step:392/1395 train_time:47198ms step_avg:123.56ms
step:393/1395 train_time:47324ms step_avg:123.56ms
step:394/1395 train_time:47450ms step_avg:123.57ms
step:395/1395 train_time:47576ms step_avg:123.57ms
step:396/1395 train_time:47701ms step_avg:123.58ms
step:397/1395 train_time:47826ms step_avg:123.58ms
step:398/1395 train_time:47952ms step_avg:123.59ms
step:399/1395 train_time:48077ms step_avg:123.59ms
step:400/1395 train_time:48203ms step_avg:123.60ms
step:401/1395 train_time:48329ms step_avg:123.60ms
step:402/1395 train_time:48454ms step_avg:123.61ms
step:403/1395 train_time:48580ms step_avg:123.61ms
step:404/1395 train_time:48706ms step_avg:123.62ms
step:405/1395 train_time:48832ms step_avg:123.63ms
step:406/1395 train_time:48957ms step_avg:123.63ms
step:407/1395 train_time:49083ms step_avg:123.63ms
step:408/1395 train_time:49209ms step_avg:123.64ms
step:409/1395 train_time:49335ms step_avg:123.65ms
step:410/1395 train_time:49461ms step_avg:123.65ms
step:411/1395 train_time:49585ms step_avg:123.65ms
step:412/1395 train_time:49711ms step_avg:123.66ms
step:413/1395 train_time:49836ms step_avg:123.66ms
step:414/1395 train_time:49961ms step_avg:123.67ms
step:415/1395 train_time:50086ms step_avg:123.67ms
step:416/1395 train_time:50213ms step_avg:123.68ms
step:417/1395 train_time:50339ms step_avg:123.68ms
step:418/1395 train_time:50465ms step_avg:123.69ms
step:419/1395 train_time:50591ms step_avg:123.70ms
step:420/1395 train_time:50717ms step_avg:123.70ms
step:421/1395 train_time:50843ms step_avg:123.71ms
step:422/1395 train_time:50968ms step_avg:123.71ms
step:423/1395 train_time:51094ms step_avg:123.72ms
step:424/1395 train_time:51222ms step_avg:123.72ms
step:425/1395 train_time:51347ms step_avg:123.73ms
step:426/1395 train_time:51474ms step_avg:123.74ms
step:427/1395 train_time:51600ms step_avg:123.74ms
step:428/1395 train_time:51726ms step_avg:123.75ms
step:429/1395 train_time:51853ms step_avg:123.75ms
step:430/1395 train_time:51980ms step_avg:123.76ms
step:431/1395 train_time:52106ms step_avg:123.77ms
step:432/1395 train_time:52233ms step_avg:123.78ms
step:433/1395 train_time:52361ms step_avg:123.78ms
step:434/1395 train_time:52487ms step_avg:123.79ms
step:435/1395 train_time:52613ms step_avg:123.79ms
step:436/1395 train_time:52739ms step_avg:123.80ms
step:437/1395 train_time:52866ms step_avg:123.81ms
step:438/1395 train_time:52993ms step_avg:123.81ms
step:439/1395 train_time:53118ms step_avg:123.82ms
step:440/1395 train_time:53244ms step_avg:123.82ms
step:441/1395 train_time:53370ms step_avg:123.83ms
step:442/1395 train_time:53497ms step_avg:123.84ms
step:443/1395 train_time:53623ms step_avg:123.84ms
step:444/1395 train_time:53749ms step_avg:123.84ms
step:445/1395 train_time:53875ms step_avg:123.85ms
step:446/1395 train_time:54001ms step_avg:123.86ms
step:447/1395 train_time:54127ms step_avg:123.86ms
step:448/1395 train_time:54253ms step_avg:123.87ms
step:449/1395 train_time:54379ms step_avg:123.87ms
step:450/1395 train_time:54505ms step_avg:123.88ms
step:451/1395 train_time:54631ms step_avg:123.88ms
step:452/1395 train_time:54757ms step_avg:123.88ms
step:453/1395 train_time:54884ms step_avg:123.89ms
step:454/1395 train_time:55010ms step_avg:123.90ms
step:455/1395 train_time:55136ms step_avg:123.90ms
step:456/1395 train_time:55263ms step_avg:123.91ms
step:457/1395 train_time:55389ms step_avg:123.91ms
step:458/1395 train_time:55515ms step_avg:123.92ms
step:459/1395 train_time:55641ms step_avg:123.92ms
step:460/1395 train_time:55768ms step_avg:123.93ms
step:461/1395 train_time:55894ms step_avg:123.93ms
step:462/1395 train_time:56021ms step_avg:123.94ms
step:463/1395 train_time:56147ms step_avg:123.94ms
step:464/1395 train_time:56274ms step_avg:123.95ms
step:465/1395 train_time:56400ms step_avg:123.96ms
step:466/1395 train_time:56526ms step_avg:123.96ms
step:467/1395 train_time:56653ms step_avg:123.97ms
step:468/1395 train_time:56779ms step_avg:123.97ms
step:469/1395 train_time:56905ms step_avg:123.98ms
step:470/1395 train_time:57031ms step_avg:123.98ms
step:471/1395 train_time:57157ms step_avg:123.98ms
step:472/1395 train_time:57283ms step_avg:123.99ms
step:473/1395 train_time:57410ms step_avg:124.00ms
step:474/1395 train_time:57536ms step_avg:124.00ms
step:475/1395 train_time:57663ms step_avg:124.01ms
step:476/1395 train_time:57790ms step_avg:124.01ms
step:477/1395 train_time:57916ms step_avg:124.02ms
step:478/1395 train_time:58042ms step_avg:124.02ms
step:479/1395 train_time:58167ms step_avg:124.02ms
step:480/1395 train_time:58294ms step_avg:124.03ms
step:481/1395 train_time:58422ms step_avg:124.04ms
step:482/1395 train_time:58548ms step_avg:124.04ms
step:483/1395 train_time:58674ms step_avg:124.05ms
step:484/1395 train_time:58800ms step_avg:124.05ms
step:485/1395 train_time:58926ms step_avg:124.06ms
step:486/1395 train_time:59052ms step_avg:124.06ms
step:487/1395 train_time:59178ms step_avg:124.06ms
step:488/1395 train_time:59305ms step_avg:124.07ms
step:489/1395 train_time:59431ms step_avg:124.07ms
step:490/1395 train_time:59558ms step_avg:124.08ms
step:491/1395 train_time:59684ms step_avg:124.08ms
step:492/1395 train_time:59811ms step_avg:124.09ms
step:493/1395 train_time:59936ms step_avg:124.09ms
step:494/1395 train_time:60062ms step_avg:124.10ms
step:495/1395 train_time:60188ms step_avg:124.10ms
step:496/1395 train_time:60314ms step_avg:124.10ms
step:497/1395 train_time:60440ms step_avg:124.11ms
step:498/1395 train_time:60566ms step_avg:124.11ms
step:499/1395 train_time:60693ms step_avg:124.12ms
step:500/1395 train_time:60820ms step_avg:124.12ms
step:500/1395 val_loss:3.6671 train_time:60945ms step_avg:124.38ms
step:501/1395 train_time:60965ms step_avg:124.17ms
step:502/1395 train_time:61086ms step_avg:124.16ms
step:503/1395 train_time:61214ms step_avg:124.17ms
step:504/1395 train_time:61340ms step_avg:124.17ms
step:505/1395 train_time:61466ms step_avg:124.17ms
step:506/1395 train_time:61591ms step_avg:124.18ms
step:507/1395 train_time:61717ms step_avg:124.18ms
step:508/1395 train_time:61843ms step_avg:124.18ms
step:509/1395 train_time:61969ms step_avg:124.19ms
step:510/1395 train_time:62099ms step_avg:124.20ms
step:511/1395 train_time:62223ms step_avg:124.20ms
step:512/1395 train_time:62350ms step_avg:124.20ms
step:513/1395 train_time:62477ms step_avg:124.21ms
step:514/1395 train_time:62602ms step_avg:124.21ms
step:515/1395 train_time:62728ms step_avg:124.21ms
step:516/1395 train_time:62854ms step_avg:124.22ms
step:517/1395 train_time:62980ms step_avg:124.22ms
step:518/1395 train_time:63106ms step_avg:124.22ms
step:519/1395 train_time:63234ms step_avg:124.23ms
step:520/1395 train_time:63363ms step_avg:124.24ms
step:521/1395 train_time:63492ms step_avg:124.25ms
step:522/1395 train_time:63620ms step_avg:124.26ms
step:523/1395 train_time:63748ms step_avg:124.27ms
step:524/1395 train_time:63876ms step_avg:124.27ms
step:525/1395 train_time:64005ms step_avg:124.28ms
step:526/1395 train_time:64133ms step_avg:124.29ms
step:527/1395 train_time:64261ms step_avg:124.30ms
step:528/1395 train_time:64391ms step_avg:124.31ms
step:529/1395 train_time:64519ms step_avg:124.31ms
step:530/1395 train_time:64648ms step_avg:124.32ms
step:531/1395 train_time:64776ms step_avg:124.33ms
step:532/1395 train_time:64904ms step_avg:124.34ms
step:533/1395 train_time:65034ms step_avg:124.35ms
step:534/1395 train_time:65162ms step_avg:124.35ms
step:535/1395 train_time:65290ms step_avg:124.36ms
step:536/1395 train_time:65418ms step_avg:124.37ms
step:537/1395 train_time:65546ms step_avg:124.38ms
step:538/1395 train_time:65674ms step_avg:124.38ms
step:539/1395 train_time:65802ms step_avg:124.39ms
step:540/1395 train_time:65930ms step_avg:124.40ms
step:541/1395 train_time:66059ms step_avg:124.40ms
step:542/1395 train_time:66191ms step_avg:124.42ms
step:543/1395 train_time:66316ms step_avg:124.42ms
step:544/1395 train_time:66444ms step_avg:124.43ms
step:545/1395 train_time:66574ms step_avg:124.44ms
step:546/1395 train_time:66702ms step_avg:124.44ms
step:547/1395 train_time:66830ms step_avg:124.45ms
step:548/1395 train_time:66958ms step_avg:124.46ms
step:549/1395 train_time:67086ms step_avg:124.46ms
step:550/1395 train_time:67214ms step_avg:124.47ms
step:551/1395 train_time:67343ms step_avg:124.48ms
step:552/1395 train_time:67473ms step_avg:124.49ms
step:553/1395 train_time:67601ms step_avg:124.50ms
step:554/1395 train_time:67730ms step_avg:124.50ms
step:555/1395 train_time:67858ms step_avg:124.51ms
step:556/1395 train_time:67988ms step_avg:124.52ms
step:557/1395 train_time:68115ms step_avg:124.53ms
step:558/1395 train_time:68243ms step_avg:124.53ms
step:559/1395 train_time:68371ms step_avg:124.54ms
step:560/1395 train_time:68499ms step_avg:124.54ms
step:561/1395 train_time:68628ms step_avg:124.55ms
step:562/1395 train_time:68757ms step_avg:124.56ms
step:563/1395 train_time:68885ms step_avg:124.57ms
step:564/1395 train_time:69012ms step_avg:124.57ms
step:565/1395 train_time:69140ms step_avg:124.58ms
step:566/1395 train_time:69268ms step_avg:124.58ms
step:567/1395 train_time:69399ms step_avg:124.59ms
step:568/1395 train_time:69524ms step_avg:124.59ms
step:569/1395 train_time:69652ms step_avg:124.60ms
step:570/1395 train_time:69781ms step_avg:124.61ms
step:571/1395 train_time:69910ms step_avg:124.62ms
step:572/1395 train_time:70038ms step_avg:124.62ms
step:573/1395 train_time:70168ms step_avg:124.63ms
step:574/1395 train_time:70296ms step_avg:124.64ms
step:575/1395 train_time:70425ms step_avg:124.65ms
step:576/1395 train_time:70553ms step_avg:124.65ms
step:577/1395 train_time:70681ms step_avg:124.66ms
step:578/1395 train_time:70809ms step_avg:124.66ms
step:579/1395 train_time:70938ms step_avg:124.67ms
step:580/1395 train_time:71067ms step_avg:124.68ms
step:581/1395 train_time:71196ms step_avg:124.69ms
step:582/1395 train_time:71324ms step_avg:124.69ms
step:583/1395 train_time:71454ms step_avg:124.70ms
step:584/1395 train_time:71582ms step_avg:124.71ms
step:585/1395 train_time:71711ms step_avg:124.71ms
step:586/1395 train_time:71839ms step_avg:124.72ms
step:587/1395 train_time:71968ms step_avg:124.73ms
step:588/1395 train_time:72096ms step_avg:124.73ms
step:589/1395 train_time:72226ms step_avg:124.74ms
step:590/1395 train_time:72354ms step_avg:124.75ms
step:591/1395 train_time:72482ms step_avg:124.75ms
step:592/1395 train_time:72611ms step_avg:124.76ms
step:593/1395 train_time:72739ms step_avg:124.77ms
step:594/1395 train_time:72868ms step_avg:124.77ms
step:595/1395 train_time:72997ms step_avg:124.78ms
step:596/1395 train_time:73125ms step_avg:124.79ms
step:597/1395 train_time:73253ms step_avg:124.79ms
step:598/1395 train_time:73381ms step_avg:124.80ms
step:599/1395 train_time:73510ms step_avg:124.80ms
step:600/1395 train_time:73638ms step_avg:124.81ms
step:601/1395 train_time:73767ms step_avg:124.82ms
step:602/1395 train_time:73895ms step_avg:124.82ms
step:603/1395 train_time:74023ms step_avg:124.83ms
step:604/1395 train_time:74152ms step_avg:124.83ms
step:605/1395 train_time:74280ms step_avg:124.84ms
step:606/1395 train_time:74409ms step_avg:124.85ms
step:607/1395 train_time:74538ms step_avg:124.85ms
step:608/1395 train_time:74666ms step_avg:124.86ms
step:609/1395 train_time:74795ms step_avg:124.87ms
step:610/1395 train_time:74924ms step_avg:124.87ms
step:611/1395 train_time:75052ms step_avg:124.88ms
step:612/1395 train_time:75180ms step_avg:124.88ms
step:613/1395 train_time:75308ms step_avg:124.89ms
step:614/1395 train_time:75436ms step_avg:124.89ms
step:615/1395 train_time:75565ms step_avg:124.90ms
step:616/1395 train_time:75694ms step_avg:124.91ms
step:617/1395 train_time:75822ms step_avg:124.91ms
step:618/1395 train_time:75950ms step_avg:124.92ms
step:619/1395 train_time:76078ms step_avg:124.92ms
step:620/1395 train_time:76207ms step_avg:124.93ms
step:621/1395 train_time:76335ms step_avg:124.94ms
step:622/1395 train_time:76463ms step_avg:124.94ms
step:623/1395 train_time:76596ms step_avg:124.95ms
step:624/1395 train_time:76720ms step_avg:124.95ms
step:625/1395 train_time:76849ms step_avg:124.96ms
step:625/1395 val_loss:3.5805 train_time:76976ms step_avg:125.16ms
step:626/1395 train_time:76997ms step_avg:124.99ms
step:627/1395 train_time:77115ms step_avg:124.98ms
step:628/1395 train_time:77246ms step_avg:124.99ms
step:629/1395 train_time:77374ms step_avg:125.00ms
step:630/1395 train_time:77502ms step_avg:125.00ms
step:631/1395 train_time:77630ms step_avg:125.01ms
step:632/1395 train_time:77758ms step_avg:125.01ms
step:633/1395 train_time:77885ms step_avg:125.02ms
step:634/1395 train_time:78014ms step_avg:125.02ms
step:635/1395 train_time:78143ms step_avg:125.03ms
step:636/1395 train_time:78274ms step_avg:125.04ms
step:637/1395 train_time:78404ms step_avg:125.05ms
step:638/1395 train_time:78533ms step_avg:125.05ms
step:639/1395 train_time:78662ms step_avg:125.06ms
step:640/1395 train_time:78792ms step_avg:125.07ms
step:641/1395 train_time:78920ms step_avg:125.07ms
step:642/1395 train_time:79050ms step_avg:125.08ms
step:643/1395 train_time:79179ms step_avg:125.08ms
step:644/1395 train_time:79308ms step_avg:125.09ms
step:645/1395 train_time:79436ms step_avg:125.10ms
step:646/1395 train_time:79565ms step_avg:125.10ms
step:647/1395 train_time:79693ms step_avg:125.11ms
step:648/1395 train_time:79823ms step_avg:125.11ms
step:649/1395 train_time:79952ms step_avg:125.12ms
step:650/1395 train_time:80081ms step_avg:125.13ms
step:651/1395 train_time:80210ms step_avg:125.13ms
step:652/1395 train_time:80340ms step_avg:125.14ms
step:653/1395 train_time:80469ms step_avg:125.15ms
step:654/1395 train_time:80597ms step_avg:125.15ms
step:655/1395 train_time:80726ms step_avg:125.16ms
step:656/1395 train_time:80854ms step_avg:125.16ms
step:657/1395 train_time:80984ms step_avg:125.17ms
step:658/1395 train_time:81112ms step_avg:125.17ms
step:659/1395 train_time:81241ms step_avg:125.18ms
step:660/1395 train_time:81370ms step_avg:125.18ms
step:661/1395 train_time:81499ms step_avg:125.19ms
step:662/1395 train_time:81628ms step_avg:125.20ms
step:663/1395 train_time:81756ms step_avg:125.20ms
step:664/1395 train_time:81885ms step_avg:125.21ms
step:665/1395 train_time:82014ms step_avg:125.21ms
step:666/1395 train_time:82143ms step_avg:125.22ms
step:667/1395 train_time:82272ms step_avg:125.22ms
step:668/1395 train_time:82401ms step_avg:125.23ms
step:669/1395 train_time:82529ms step_avg:125.23ms
step:670/1395 train_time:82658ms step_avg:125.24ms
step:671/1395 train_time:82787ms step_avg:125.24ms
step:672/1395 train_time:82915ms step_avg:125.25ms
step:673/1395 train_time:83045ms step_avg:125.26ms
step:674/1395 train_time:83173ms step_avg:125.26ms
step:675/1395 train_time:83302ms step_avg:125.27ms
step:676/1395 train_time:83431ms step_avg:125.27ms
step:677/1395 train_time:83561ms step_avg:125.28ms
step:678/1395 train_time:83689ms step_avg:125.28ms
step:679/1395 train_time:83817ms step_avg:125.29ms
step:680/1395 train_time:83946ms step_avg:125.29ms
step:681/1395 train_time:84075ms step_avg:125.30ms
step:682/1395 train_time:84204ms step_avg:125.30ms
step:683/1395 train_time:84333ms step_avg:125.31ms
step:684/1395 train_time:84461ms step_avg:125.31ms
step:685/1395 train_time:84590ms step_avg:125.32ms
step:686/1395 train_time:84719ms step_avg:125.32ms
step:687/1395 train_time:84846ms step_avg:125.33ms
step:688/1395 train_time:84975ms step_avg:125.33ms
step:689/1395 train_time:85105ms step_avg:125.34ms
step:690/1395 train_time:85235ms step_avg:125.35ms
step:691/1395 train_time:85364ms step_avg:125.35ms
step:692/1395 train_time:85492ms step_avg:125.35ms
step:693/1395 train_time:85620ms step_avg:125.36ms
step:694/1395 train_time:85749ms step_avg:125.36ms
step:695/1395 train_time:85877ms step_avg:125.37ms
step:696/1395 train_time:86007ms step_avg:125.37ms
step:697/1395 train_time:86135ms step_avg:125.38ms
step:698/1395 train_time:86264ms step_avg:125.38ms
step:699/1395 train_time:86393ms step_avg:125.39ms
step:700/1395 train_time:86522ms step_avg:125.39ms
step:701/1395 train_time:86651ms step_avg:125.40ms
step:702/1395 train_time:86780ms step_avg:125.40ms
step:703/1395 train_time:86909ms step_avg:125.41ms
step:704/1395 train_time:87037ms step_avg:125.41ms
step:705/1395 train_time:87166ms step_avg:125.42ms
step:706/1395 train_time:87296ms step_avg:125.42ms
step:707/1395 train_time:87424ms step_avg:125.43ms
step:708/1395 train_time:87553ms step_avg:125.43ms
step:709/1395 train_time:87681ms step_avg:125.44ms
step:710/1395 train_time:87811ms step_avg:125.44ms
step:711/1395 train_time:87939ms step_avg:125.45ms
step:712/1395 train_time:88069ms step_avg:125.45ms
step:713/1395 train_time:88198ms step_avg:125.46ms
step:714/1395 train_time:88327ms step_avg:125.46ms
step:715/1395 train_time:88455ms step_avg:125.47ms
step:716/1395 train_time:88586ms step_avg:125.48ms
step:717/1395 train_time:88713ms step_avg:125.48ms
step:718/1395 train_time:88841ms step_avg:125.48ms
step:719/1395 train_time:88970ms step_avg:125.49ms
step:720/1395 train_time:89098ms step_avg:125.49ms
step:721/1395 train_time:89227ms step_avg:125.50ms
step:722/1395 train_time:89355ms step_avg:125.50ms
step:723/1395 train_time:89484ms step_avg:125.50ms
step:724/1395 train_time:89614ms step_avg:125.51ms
step:725/1395 train_time:89742ms step_avg:125.51ms
step:726/1395 train_time:89873ms step_avg:125.52ms
step:727/1395 train_time:90003ms step_avg:125.53ms
step:728/1395 train_time:90133ms step_avg:125.53ms
step:729/1395 train_time:90263ms step_avg:125.54ms
step:730/1395 train_time:90394ms step_avg:125.55ms
step:731/1395 train_time:90525ms step_avg:125.55ms
step:732/1395 train_time:90655ms step_avg:125.56ms
step:733/1395 train_time:90786ms step_avg:125.57ms
step:734/1395 train_time:90916ms step_avg:125.58ms
step:735/1395 train_time:91047ms step_avg:125.58ms
step:736/1395 train_time:91178ms step_avg:125.59ms
step:737/1395 train_time:91309ms step_avg:125.60ms
step:738/1395 train_time:91439ms step_avg:125.60ms
step:739/1395 train_time:91570ms step_avg:125.61ms
step:740/1395 train_time:91701ms step_avg:125.62ms
step:741/1395 train_time:91832ms step_avg:125.63ms
step:742/1395 train_time:91962ms step_avg:125.63ms
step:743/1395 train_time:92092ms step_avg:125.64ms
step:744/1395 train_time:92223ms step_avg:125.64ms
step:745/1395 train_time:92355ms step_avg:125.65ms
step:746/1395 train_time:92486ms step_avg:125.66ms
step:747/1395 train_time:92616ms step_avg:125.67ms
step:748/1395 train_time:92746ms step_avg:125.67ms
step:749/1395 train_time:92876ms step_avg:125.68ms
step:750/1395 train_time:93009ms step_avg:125.69ms
step:750/1395 val_loss:3.5267 train_time:93138ms step_avg:125.86ms
step:751/1395 train_time:93158ms step_avg:125.72ms
step:752/1395 train_time:93280ms step_avg:125.71ms
step:753/1395 train_time:93411ms step_avg:125.72ms
step:754/1395 train_time:93541ms step_avg:125.73ms
step:755/1395 train_time:93671ms step_avg:125.73ms
step:756/1395 train_time:93801ms step_avg:125.74ms
step:757/1395 train_time:93933ms step_avg:125.75ms
step:758/1395 train_time:94064ms step_avg:125.75ms
step:759/1395 train_time:94194ms step_avg:125.76ms
step:760/1395 train_time:94326ms step_avg:125.77ms
step:761/1395 train_time:94457ms step_avg:125.77ms
step:762/1395 train_time:94586ms step_avg:125.78ms
step:763/1395 train_time:94717ms step_avg:125.79ms
step:764/1395 train_time:94847ms step_avg:125.79ms
step:765/1395 train_time:94978ms step_avg:125.80ms
step:766/1395 train_time:95109ms step_avg:125.81ms
step:767/1395 train_time:95241ms step_avg:125.81ms
step:768/1395 train_time:95372ms step_avg:125.82ms
step:769/1395 train_time:95502ms step_avg:125.83ms
step:770/1395 train_time:95634ms step_avg:125.83ms
step:771/1395 train_time:95764ms step_avg:125.84ms
step:772/1395 train_time:95894ms step_avg:125.85ms
step:773/1395 train_time:96025ms step_avg:125.85ms
step:774/1395 train_time:96154ms step_avg:125.86ms
step:775/1395 train_time:96285ms step_avg:125.86ms
step:776/1395 train_time:96416ms step_avg:125.87ms
step:777/1395 train_time:96546ms step_avg:125.87ms
step:778/1395 train_time:96677ms step_avg:125.88ms
step:779/1395 train_time:96808ms step_avg:125.89ms
step:780/1395 train_time:96939ms step_avg:125.89ms
step:781/1395 train_time:97069ms step_avg:125.90ms
step:782/1395 train_time:97199ms step_avg:125.91ms
step:783/1395 train_time:97329ms step_avg:125.91ms
step:784/1395 train_time:97460ms step_avg:125.92ms
step:785/1395 train_time:97591ms step_avg:125.92ms
step:786/1395 train_time:97721ms step_avg:125.93ms
step:787/1395 train_time:97851ms step_avg:125.93ms
step:788/1395 train_time:97981ms step_avg:125.94ms
step:789/1395 train_time:98112ms step_avg:125.95ms
step:790/1395 train_time:98243ms step_avg:125.95ms
step:791/1395 train_time:98373ms step_avg:125.96ms
step:792/1395 train_time:98504ms step_avg:125.96ms
step:793/1395 train_time:98635ms step_avg:125.97ms
step:794/1395 train_time:98765ms step_avg:125.98ms
step:795/1395 train_time:98897ms step_avg:125.98ms
step:796/1395 train_time:99027ms step_avg:125.99ms
step:797/1395 train_time:99159ms step_avg:126.00ms
step:798/1395 train_time:99290ms step_avg:126.00ms
step:799/1395 train_time:99422ms step_avg:126.01ms
step:800/1395 train_time:99552ms step_avg:126.02ms
step:801/1395 train_time:99682ms step_avg:126.02ms
step:802/1395 train_time:99813ms step_avg:126.03ms
step:803/1395 train_time:99943ms step_avg:126.03ms
step:804/1395 train_time:100073ms step_avg:126.04ms
step:805/1395 train_time:100204ms step_avg:126.04ms
step:806/1395 train_time:100334ms step_avg:126.05ms
step:807/1395 train_time:100465ms step_avg:126.05ms
step:808/1395 train_time:100595ms step_avg:126.06ms
step:809/1395 train_time:100725ms step_avg:126.06ms
step:810/1395 train_time:100855ms step_avg:126.07ms
step:811/1395 train_time:100985ms step_avg:126.07ms
step:812/1395 train_time:101117ms step_avg:126.08ms
step:813/1395 train_time:101246ms step_avg:126.09ms
step:814/1395 train_time:101378ms step_avg:126.09ms
step:815/1395 train_time:101508ms step_avg:126.10ms
step:816/1395 train_time:101639ms step_avg:126.10ms
step:817/1395 train_time:101770ms step_avg:126.11ms
step:818/1395 train_time:101900ms step_avg:126.11ms
step:819/1395 train_time:102030ms step_avg:126.12ms
step:820/1395 train_time:102161ms step_avg:126.12ms
step:821/1395 train_time:102290ms step_avg:126.13ms
step:822/1395 train_time:102421ms step_avg:126.13ms
step:823/1395 train_time:102555ms step_avg:126.14ms
step:824/1395 train_time:102683ms step_avg:126.15ms
step:825/1395 train_time:102815ms step_avg:126.15ms
step:826/1395 train_time:102945ms step_avg:126.16ms
step:827/1395 train_time:103076ms step_avg:126.16ms
step:828/1395 train_time:103207ms step_avg:126.17ms
step:829/1395 train_time:103337ms step_avg:126.17ms
step:830/1395 train_time:103468ms step_avg:126.18ms
step:831/1395 train_time:103599ms step_avg:126.19ms
step:832/1395 train_time:103730ms step_avg:126.19ms
step:833/1395 train_time:103861ms step_avg:126.20ms
step:834/1395 train_time:103993ms step_avg:126.20ms
step:835/1395 train_time:104125ms step_avg:126.21ms
step:836/1395 train_time:104258ms step_avg:126.22ms
step:837/1395 train_time:104386ms step_avg:126.22ms
step:838/1395 train_time:104516ms step_avg:126.23ms
step:839/1395 train_time:104647ms step_avg:126.23ms
step:840/1395 train_time:104778ms step_avg:126.24ms
step:841/1395 train_time:104909ms step_avg:126.24ms
step:842/1395 train_time:105039ms step_avg:126.25ms
step:843/1395 train_time:105170ms step_avg:126.25ms
step:844/1395 train_time:105302ms step_avg:126.26ms
step:845/1395 train_time:105432ms step_avg:126.27ms
step:846/1395 train_time:105563ms step_avg:126.27ms
step:847/1395 train_time:105695ms step_avg:126.28ms
step:848/1395 train_time:105825ms step_avg:126.28ms
step:849/1395 train_time:105957ms step_avg:126.29ms
step:850/1395 train_time:106088ms step_avg:126.30ms
step:851/1395 train_time:106220ms step_avg:126.30ms
step:852/1395 train_time:106351ms step_avg:126.31ms
step:853/1395 train_time:106482ms step_avg:126.31ms
step:854/1395 train_time:106611ms step_avg:126.32ms
step:855/1395 train_time:106742ms step_avg:126.32ms
step:856/1395 train_time:106873ms step_avg:126.33ms
step:857/1395 train_time:107004ms step_avg:126.33ms
step:858/1395 train_time:107136ms step_avg:126.34ms
step:859/1395 train_time:107267ms step_avg:126.35ms
step:860/1395 train_time:107398ms step_avg:126.35ms
step:861/1395 train_time:107528ms step_avg:126.36ms
step:862/1395 train_time:107659ms step_avg:126.36ms
step:863/1395 train_time:107790ms step_avg:126.37ms
step:864/1395 train_time:107921ms step_avg:126.37ms
step:865/1395 train_time:108051ms step_avg:126.38ms
step:866/1395 train_time:108183ms step_avg:126.38ms
step:867/1395 train_time:108315ms step_avg:126.39ms
step:868/1395 train_time:108446ms step_avg:126.39ms
step:869/1395 train_time:108578ms step_avg:126.40ms
step:870/1395 train_time:108710ms step_avg:126.41ms
step:871/1395 train_time:108840ms step_avg:126.41ms
step:872/1395 train_time:108971ms step_avg:126.42ms
step:873/1395 train_time:109102ms step_avg:126.42ms
step:874/1395 train_time:109232ms step_avg:126.43ms
step:875/1395 train_time:109363ms step_avg:126.43ms
step:875/1395 val_loss:3.4783 train_time:109493ms step_avg:126.58ms
step:876/1395 train_time:109513ms step_avg:126.46ms
step:877/1395 train_time:109635ms step_avg:126.45ms
step:878/1395 train_time:109766ms step_avg:126.46ms
step:879/1395 train_time:109896ms step_avg:126.46ms
step:880/1395 train_time:110027ms step_avg:126.47ms
step:881/1395 train_time:110157ms step_avg:126.47ms
step:882/1395 train_time:110288ms step_avg:126.48ms
step:883/1395 train_time:110418ms step_avg:126.48ms
step:884/1395 train_time:110550ms step_avg:126.49ms
step:885/1395 train_time:110683ms step_avg:126.50ms
step:886/1395 train_time:110815ms step_avg:126.50ms
step:887/1395 train_time:110945ms step_avg:126.51ms
step:888/1395 train_time:111076ms step_avg:126.51ms
step:889/1395 train_time:111209ms step_avg:126.52ms
step:890/1395 train_time:111339ms step_avg:126.52ms
step:891/1395 train_time:111470ms step_avg:126.53ms
step:892/1395 train_time:111601ms step_avg:126.53ms
step:893/1395 train_time:111732ms step_avg:126.54ms
step:894/1395 train_time:111863ms step_avg:126.54ms
step:895/1395 train_time:111995ms step_avg:126.55ms
step:896/1395 train_time:112126ms step_avg:126.55ms
step:897/1395 train_time:112255ms step_avg:126.56ms
step:898/1395 train_time:112387ms step_avg:126.56ms
step:899/1395 train_time:112519ms step_avg:126.57ms
step:900/1395 train_time:112650ms step_avg:126.57ms
step:901/1395 train_time:112782ms step_avg:126.58ms
step:902/1395 train_time:112913ms step_avg:126.58ms
step:903/1395 train_time:113045ms step_avg:126.59ms
step:904/1395 train_time:113174ms step_avg:126.59ms
step:905/1395 train_time:113306ms step_avg:126.60ms
step:906/1395 train_time:113436ms step_avg:126.60ms
step:907/1395 train_time:113567ms step_avg:126.61ms
step:908/1395 train_time:113697ms step_avg:126.61ms
step:909/1395 train_time:113828ms step_avg:126.62ms
step:910/1395 train_time:113961ms step_avg:126.62ms
step:911/1395 train_time:114092ms step_avg:126.63ms
step:912/1395 train_time:114222ms step_avg:126.63ms
step:913/1395 train_time:114353ms step_avg:126.64ms
step:914/1395 train_time:114483ms step_avg:126.64ms
step:915/1395 train_time:114614ms step_avg:126.65ms
step:916/1395 train_time:114747ms step_avg:126.65ms
step:917/1395 train_time:114878ms step_avg:126.66ms
step:918/1395 train_time:115008ms step_avg:126.66ms
step:919/1395 train_time:115140ms step_avg:126.67ms
step:920/1395 train_time:115270ms step_avg:126.67ms
step:921/1395 train_time:115402ms step_avg:126.68ms
step:922/1395 train_time:115533ms step_avg:126.68ms
step:923/1395 train_time:115664ms step_avg:126.69ms
step:924/1395 train_time:115793ms step_avg:126.69ms
step:925/1395 train_time:115924ms step_avg:126.69ms
step:926/1395 train_time:116054ms step_avg:126.70ms
step:927/1395 train_time:116185ms step_avg:126.70ms
step:928/1395 train_time:116316ms step_avg:126.71ms
step:929/1395 train_time:116447ms step_avg:126.71ms
step:930/1395 train_time:116578ms step_avg:126.72ms
step:931/1395 train_time:116708ms step_avg:126.72ms
step:932/1395 train_time:116840ms step_avg:126.72ms
step:933/1395 train_time:116973ms step_avg:126.73ms
step:934/1395 train_time:117105ms step_avg:126.74ms
step:935/1395 train_time:117240ms step_avg:126.75ms
step:936/1395 train_time:117372ms step_avg:126.75ms
step:937/1395 train_time:117506ms step_avg:126.76ms
step:938/1395 train_time:117638ms step_avg:126.76ms
step:939/1395 train_time:117771ms step_avg:126.77ms
step:940/1395 train_time:117905ms step_avg:126.78ms
step:941/1395 train_time:118036ms step_avg:126.78ms
step:942/1395 train_time:118168ms step_avg:126.79ms
step:943/1395 train_time:118300ms step_avg:126.80ms
step:944/1395 train_time:118433ms step_avg:126.80ms
step:945/1395 train_time:118566ms step_avg:126.81ms
step:946/1395 train_time:118700ms step_avg:126.82ms
step:947/1395 train_time:118833ms step_avg:126.82ms
step:948/1395 train_time:118966ms step_avg:126.83ms
step:949/1395 train_time:119099ms step_avg:126.84ms
step:950/1395 train_time:119231ms step_avg:126.84ms
step:951/1395 train_time:119366ms step_avg:126.85ms
step:952/1395 train_time:119497ms step_avg:126.85ms
step:953/1395 train_time:119629ms step_avg:126.86ms
step:954/1395 train_time:119761ms step_avg:126.87ms
step:955/1395 train_time:119893ms step_avg:126.87ms
step:956/1395 train_time:120027ms step_avg:126.88ms
step:957/1395 train_time:120159ms step_avg:126.88ms
step:958/1395 train_time:120293ms step_avg:126.89ms
step:959/1395 train_time:120426ms step_avg:126.90ms
step:960/1395 train_time:120559ms step_avg:126.90ms
step:961/1395 train_time:120691ms step_avg:126.91ms
step:962/1395 train_time:120824ms step_avg:126.92ms
step:963/1395 train_time:120956ms step_avg:126.92ms
step:964/1395 train_time:121089ms step_avg:126.93ms
step:965/1395 train_time:121223ms step_avg:126.94ms
step:966/1395 train_time:121355ms step_avg:126.94ms
step:967/1395 train_time:121488ms step_avg:126.95ms
step:968/1395 train_time:121620ms step_avg:126.95ms
step:969/1395 train_time:121754ms step_avg:126.96ms
step:970/1395 train_time:121886ms step_avg:126.97ms
step:971/1395 train_time:122019ms step_avg:126.97ms
step:972/1395 train_time:122152ms step_avg:126.98ms
step:973/1395 train_time:122287ms step_avg:126.99ms
step:974/1395 train_time:122420ms step_avg:126.99ms
step:975/1395 train_time:122552ms step_avg:127.00ms
step:976/1395 train_time:122685ms step_avg:127.00ms
step:977/1395 train_time:122817ms step_avg:127.01ms
step:978/1395 train_time:122950ms step_avg:127.01ms
step:979/1395 train_time:123082ms step_avg:127.02ms
step:980/1395 train_time:123215ms step_avg:127.03ms
step:981/1395 train_time:123348ms step_avg:127.03ms
step:982/1395 train_time:123480ms step_avg:127.04ms
step:983/1395 train_time:123611ms step_avg:127.04ms
step:984/1395 train_time:123745ms step_avg:127.05ms
step:985/1395 train_time:123877ms step_avg:127.05ms
step:986/1395 train_time:124012ms step_avg:127.06ms
step:987/1395 train_time:124145ms step_avg:127.07ms
step:988/1395 train_time:124277ms step_avg:127.07ms
step:989/1395 train_time:124410ms step_avg:127.08ms
step:990/1395 train_time:124543ms step_avg:127.09ms
step:991/1395 train_time:124676ms step_avg:127.09ms
step:992/1395 train_time:124810ms step_avg:127.10ms
step:993/1395 train_time:124945ms step_avg:127.11ms
step:994/1395 train_time:125078ms step_avg:127.11ms
step:995/1395 train_time:125209ms step_avg:127.12ms
step:996/1395 train_time:125343ms step_avg:127.12ms
step:997/1395 train_time:125474ms step_avg:127.13ms
step:998/1395 train_time:125607ms step_avg:127.13ms
step:999/1395 train_time:125739ms step_avg:127.14ms
step:1000/1395 train_time:125871ms step_avg:127.14ms
step:1000/1395 val_loss:3.4138 train_time:126003ms step_avg:127.28ms
step:1001/1395 train_time:126023ms step_avg:127.17ms
step:1002/1395 train_time:126147ms step_avg:127.16ms
step:1003/1395 train_time:126280ms step_avg:127.17ms
step:1004/1395 train_time:126413ms step_avg:127.18ms
step:1005/1395 train_time:126546ms step_avg:127.18ms
step:1006/1395 train_time:126677ms step_avg:127.19ms
step:1007/1395 train_time:126809ms step_avg:127.19ms
step:1008/1395 train_time:126941ms step_avg:127.20ms
step:1009/1395 train_time:127075ms step_avg:127.20ms
step:1010/1395 train_time:127208ms step_avg:127.21ms
step:1011/1395 train_time:127343ms step_avg:127.22ms
step:1012/1395 train_time:127475ms step_avg:127.22ms
step:1013/1395 train_time:127608ms step_avg:127.23ms
step:1014/1395 train_time:127739ms step_avg:127.23ms
step:1015/1395 train_time:127871ms step_avg:127.23ms
step:1016/1395 train_time:128003ms step_avg:127.24ms
step:1017/1395 train_time:128135ms step_avg:127.24ms
step:1018/1395 train_time:128268ms step_avg:127.25ms
step:1019/1395 train_time:128401ms step_avg:127.26ms
step:1020/1395 train_time:128533ms step_avg:127.26ms
step:1021/1395 train_time:128666ms step_avg:127.27ms
step:1022/1395 train_time:128797ms step_avg:127.27ms
step:1023/1395 train_time:128930ms step_avg:127.28ms
step:1024/1395 train_time:129063ms step_avg:127.28ms
step:1025/1395 train_time:129196ms step_avg:127.29ms
step:1026/1395 train_time:129329ms step_avg:127.29ms
step:1027/1395 train_time:129460ms step_avg:127.30ms
step:1028/1395 train_time:129593ms step_avg:127.30ms
step:1029/1395 train_time:129726ms step_avg:127.31ms
step:1030/1395 train_time:129858ms step_avg:127.31ms
step:1031/1395 train_time:129990ms step_avg:127.32ms
step:1032/1395 train_time:130123ms step_avg:127.32ms
step:1033/1395 train_time:130254ms step_avg:127.33ms
step:1034/1395 train_time:130387ms step_avg:127.33ms
step:1035/1395 train_time:130519ms step_avg:127.34ms
step:1036/1395 train_time:130652ms step_avg:127.34ms
step:1037/1395 train_time:130785ms step_avg:127.35ms
step:1038/1395 train_time:130917ms step_avg:127.35ms
step:1039/1395 train_time:131049ms step_avg:127.36ms
step:1040/1395 train_time:131182ms step_avg:127.36ms
step:1041/1395 train_time:131314ms step_avg:127.37ms
step:1042/1395 train_time:131447ms step_avg:127.37ms
step:1043/1395 train_time:131580ms step_avg:127.38ms
step:1044/1395 train_time:131715ms step_avg:127.38ms
step:1045/1395 train_time:131848ms step_avg:127.39ms
step:1046/1395 train_time:131980ms step_avg:127.39ms
step:1047/1395 train_time:132112ms step_avg:127.40ms
step:1048/1395 train_time:132244ms step_avg:127.40ms
step:1049/1395 train_time:132377ms step_avg:127.41ms
step:1050/1395 train_time:132511ms step_avg:127.41ms
step:1051/1395 train_time:132645ms step_avg:127.42ms
step:1052/1395 train_time:132778ms step_avg:127.43ms
step:1053/1395 train_time:132912ms step_avg:127.43ms
step:1054/1395 train_time:133043ms step_avg:127.44ms
step:1055/1395 train_time:133176ms step_avg:127.44ms
step:1056/1395 train_time:133309ms step_avg:127.45ms
step:1057/1395 train_time:133442ms step_avg:127.45ms
step:1058/1395 train_time:133576ms step_avg:127.46ms
step:1059/1395 train_time:133709ms step_avg:127.46ms
step:1060/1395 train_time:133845ms step_avg:127.47ms
step:1061/1395 train_time:133978ms step_avg:127.48ms
step:1062/1395 train_time:134112ms step_avg:127.48ms
step:1063/1395 train_time:134244ms step_avg:127.49ms
step:1064/1395 train_time:134376ms step_avg:127.49ms
step:1065/1395 train_time:134508ms step_avg:127.50ms
step:1066/1395 train_time:134641ms step_avg:127.50ms
step:1067/1395 train_time:134774ms step_avg:127.51ms
step:1068/1395 train_time:134908ms step_avg:127.51ms
step:1069/1395 train_time:135042ms step_avg:127.52ms
step:1070/1395 train_time:135173ms step_avg:127.52ms
step:1071/1395 train_time:135307ms step_avg:127.53ms
step:1072/1395 train_time:135439ms step_avg:127.53ms
step:1073/1395 train_time:135572ms step_avg:127.54ms
step:1074/1395 train_time:135704ms step_avg:127.54ms
step:1075/1395 train_time:135836ms step_avg:127.55ms
step:1076/1395 train_time:135968ms step_avg:127.55ms
step:1077/1395 train_time:136101ms step_avg:127.55ms
step:1078/1395 train_time:136233ms step_avg:127.56ms
step:1079/1395 train_time:136370ms step_avg:127.57ms
step:1080/1395 train_time:136503ms step_avg:127.57ms
step:1081/1395 train_time:136636ms step_avg:127.58ms
step:1082/1395 train_time:136768ms step_avg:127.58ms
step:1083/1395 train_time:136900ms step_avg:127.59ms
step:1084/1395 train_time:137034ms step_avg:127.59ms
step:1085/1395 train_time:137167ms step_avg:127.60ms
step:1086/1395 train_time:137299ms step_avg:127.60ms
step:1087/1395 train_time:137432ms step_avg:127.61ms
step:1088/1395 train_time:137565ms step_avg:127.61ms
step:1089/1395 train_time:137699ms step_avg:127.62ms
step:1090/1395 train_time:137833ms step_avg:127.62ms
step:1091/1395 train_time:137966ms step_avg:127.63ms
step:1092/1395 train_time:138098ms step_avg:127.63ms
step:1093/1395 train_time:138231ms step_avg:127.64ms
step:1094/1395 train_time:138364ms step_avg:127.64ms
step:1095/1395 train_time:138496ms step_avg:127.65ms
step:1096/1395 train_time:138630ms step_avg:127.65ms
step:1097/1395 train_time:138763ms step_avg:127.66ms
step:1098/1395 train_time:138897ms step_avg:127.66ms
step:1099/1395 train_time:139029ms step_avg:127.67ms
step:1100/1395 train_time:139161ms step_avg:127.67ms
step:1101/1395 train_time:139293ms step_avg:127.67ms
step:1102/1395 train_time:139425ms step_avg:127.68ms
step:1103/1395 train_time:139560ms step_avg:127.68ms
step:1104/1395 train_time:139692ms step_avg:127.69ms
step:1105/1395 train_time:139826ms step_avg:127.70ms
step:1106/1395 train_time:139959ms step_avg:127.70ms
step:1107/1395 train_time:140091ms step_avg:127.70ms
step:1108/1395 train_time:140226ms step_avg:127.71ms
step:1109/1395 train_time:140358ms step_avg:127.71ms
step:1110/1395 train_time:140491ms step_avg:127.72ms
step:1111/1395 train_time:140623ms step_avg:127.72ms
step:1112/1395 train_time:140756ms step_avg:127.73ms
step:1113/1395 train_time:140889ms step_avg:127.73ms
step:1114/1395 train_time:141021ms step_avg:127.74ms
step:1115/1395 train_time:141153ms step_avg:127.74ms
step:1116/1395 train_time:141286ms step_avg:127.75ms
step:1117/1395 train_time:141419ms step_avg:127.75ms
step:1118/1395 train_time:141554ms step_avg:127.76ms
step:1119/1395 train_time:141687ms step_avg:127.76ms
step:1120/1395 train_time:141819ms step_avg:127.76ms
step:1121/1395 train_time:141951ms step_avg:127.77ms
step:1122/1395 train_time:142083ms step_avg:127.77ms
step:1123/1395 train_time:142215ms step_avg:127.78ms
step:1124/1395 train_time:142349ms step_avg:127.78ms
step:1125/1395 train_time:142482ms step_avg:127.79ms
step:1125/1395 val_loss:3.3639 train_time:142614ms step_avg:127.91ms
step:1126/1395 train_time:142635ms step_avg:127.81ms
step:1127/1395 train_time:142757ms step_avg:127.80ms
step:1128/1395 train_time:142889ms step_avg:127.81ms
step:1129/1395 train_time:143022ms step_avg:127.81ms
step:1130/1395 train_time:143154ms step_avg:127.82ms
step:1131/1395 train_time:143287ms step_avg:127.82ms
step:1132/1395 train_time:143419ms step_avg:127.82ms
step:1133/1395 train_time:143551ms step_avg:127.83ms
step:1134/1395 train_time:143686ms step_avg:127.83ms
step:1135/1395 train_time:143821ms step_avg:127.84ms
step:1136/1395 train_time:143955ms step_avg:127.85ms
step:1137/1395 train_time:144086ms step_avg:127.85ms
step:1138/1395 train_time:144219ms step_avg:127.85ms
step:1139/1395 train_time:144353ms step_avg:127.86ms
step:1140/1395 train_time:144488ms step_avg:127.87ms
step:1141/1395 train_time:144622ms step_avg:127.87ms
step:1142/1395 train_time:144756ms step_avg:127.88ms
step:1143/1395 train_time:144891ms step_avg:127.88ms
step:1144/1395 train_time:145026ms step_avg:127.89ms
step:1145/1395 train_time:145159ms step_avg:127.89ms
step:1146/1395 train_time:145294ms step_avg:127.90ms
step:1147/1395 train_time:145428ms step_avg:127.90ms
step:1148/1395 train_time:145562ms step_avg:127.91ms
step:1149/1395 train_time:145695ms step_avg:127.91ms
step:1150/1395 train_time:145828ms step_avg:127.92ms
step:1151/1395 train_time:145962ms step_avg:127.92ms
step:1152/1395 train_time:146096ms step_avg:127.93ms
step:1153/1395 train_time:146232ms step_avg:127.94ms
step:1154/1395 train_time:146367ms step_avg:127.94ms
step:1155/1395 train_time:146502ms step_avg:127.95ms
step:1156/1395 train_time:146639ms step_avg:127.96ms
step:1157/1395 train_time:146773ms step_avg:127.96ms
step:1158/1395 train_time:146906ms step_avg:127.97ms
step:1159/1395 train_time:147040ms step_avg:127.97ms
step:1160/1395 train_time:147174ms step_avg:127.98ms
step:1161/1395 train_time:147309ms step_avg:127.98ms
step:1162/1395 train_time:147445ms step_avg:127.99ms
step:1163/1395 train_time:147579ms step_avg:128.00ms
step:1164/1395 train_time:147713ms step_avg:128.00ms
step:1165/1395 train_time:147846ms step_avg:128.01ms
step:1166/1395 train_time:147980ms step_avg:128.01ms
step:1167/1395 train_time:148112ms step_avg:128.01ms
step:1168/1395 train_time:148247ms step_avg:128.02ms
step:1169/1395 train_time:148381ms step_avg:128.02ms
step:1170/1395 train_time:148516ms step_avg:128.03ms
step:1171/1395 train_time:148650ms step_avg:128.04ms
step:1172/1395 train_time:148784ms step_avg:128.04ms
step:1173/1395 train_time:148917ms step_avg:128.05ms
step:1174/1395 train_time:149057ms step_avg:128.06ms
step:1175/1395 train_time:149190ms step_avg:128.06ms
step:1176/1395 train_time:149326ms step_avg:128.07ms
step:1177/1395 train_time:149461ms step_avg:128.07ms
step:1178/1395 train_time:149595ms step_avg:128.08ms
step:1179/1395 train_time:149729ms step_avg:128.08ms
step:1180/1395 train_time:149865ms step_avg:128.09ms
step:1181/1395 train_time:150001ms step_avg:128.10ms
step:1182/1395 train_time:150135ms step_avg:128.10ms
step:1183/1395 train_time:150269ms step_avg:128.11ms
step:1184/1395 train_time:150404ms step_avg:128.11ms
step:1185/1395 train_time:150538ms step_avg:128.12ms
step:1186/1395 train_time:150672ms step_avg:128.12ms
step:1187/1395 train_time:150812ms step_avg:128.13ms
step:1188/1395 train_time:150946ms step_avg:128.14ms
step:1189/1395 train_time:151080ms step_avg:128.14ms
step:1190/1395 train_time:151214ms step_avg:128.15ms
step:1191/1395 train_time:151348ms step_avg:128.15ms
step:1192/1395 train_time:151484ms step_avg:128.16ms
step:1193/1395 train_time:151616ms step_avg:128.16ms
step:1194/1395 train_time:151750ms step_avg:128.17ms
step:1195/1395 train_time:151885ms step_avg:128.17ms
step:1196/1395 train_time:152019ms step_avg:128.18ms
step:1197/1395 train_time:152154ms step_avg:128.18ms
step:1198/1395 train_time:152289ms step_avg:128.19ms
step:1199/1395 train_time:152424ms step_avg:128.20ms
step:1200/1395 train_time:152558ms step_avg:128.20ms
step:1201/1395 train_time:152691ms step_avg:128.20ms
step:1202/1395 train_time:152829ms step_avg:128.21ms
step:1203/1395 train_time:152967ms step_avg:128.22ms
step:1204/1395 train_time:153100ms step_avg:128.22ms
step:1205/1395 train_time:153234ms step_avg:128.23ms
step:1206/1395 train_time:153370ms step_avg:128.24ms
step:1207/1395 train_time:153502ms step_avg:128.24ms
step:1208/1395 train_time:153636ms step_avg:128.24ms
step:1209/1395 train_time:153770ms step_avg:128.25ms
step:1210/1395 train_time:153908ms step_avg:128.26ms
step:1211/1395 train_time:154043ms step_avg:128.26ms
step:1212/1395 train_time:154176ms step_avg:128.27ms
step:1213/1395 train_time:154310ms step_avg:128.27ms
step:1214/1395 train_time:154446ms step_avg:128.28ms
step:1215/1395 train_time:154582ms step_avg:128.28ms
step:1216/1395 train_time:154715ms step_avg:128.29ms
step:1217/1395 train_time:154849ms step_avg:128.29ms
step:1218/1395 train_time:154982ms step_avg:128.30ms
step:1219/1395 train_time:155115ms step_avg:128.30ms
step:1220/1395 train_time:155248ms step_avg:128.30ms
step:1221/1395 train_time:155381ms step_avg:128.31ms
step:1222/1395 train_time:155516ms step_avg:128.31ms
step:1223/1395 train_time:155650ms step_avg:128.32ms
step:1224/1395 train_time:155785ms step_avg:128.32ms
step:1225/1395 train_time:155921ms step_avg:128.33ms
step:1226/1395 train_time:156056ms step_avg:128.34ms
step:1227/1395 train_time:156189ms step_avg:128.34ms
step:1228/1395 train_time:156325ms step_avg:128.35ms
step:1229/1395 train_time:156458ms step_avg:128.35ms
step:1230/1395 train_time:156594ms step_avg:128.36ms
step:1231/1395 train_time:156729ms step_avg:128.36ms
step:1232/1395 train_time:156864ms step_avg:128.37ms
step:1233/1395 train_time:156998ms step_avg:128.37ms
step:1234/1395 train_time:157131ms step_avg:128.38ms
step:1235/1395 train_time:157267ms step_avg:128.38ms
step:1236/1395 train_time:157403ms step_avg:128.39ms
step:1237/1395 train_time:157536ms step_avg:128.39ms
step:1238/1395 train_time:157674ms step_avg:128.40ms
step:1239/1395 train_time:157807ms step_avg:128.40ms
step:1240/1395 train_time:157943ms step_avg:128.41ms
step:1241/1395 train_time:158079ms step_avg:128.42ms
step:1242/1395 train_time:158212ms step_avg:128.42ms
step:1243/1395 train_time:158347ms step_avg:128.42ms
step:1244/1395 train_time:158481ms step_avg:128.43ms
step:1245/1395 train_time:158614ms step_avg:128.43ms
step:1246/1395 train_time:158748ms step_avg:128.44ms
step:1247/1395 train_time:158884ms step_avg:128.44ms
step:1248/1395 train_time:159017ms step_avg:128.45ms
step:1249/1395 train_time:159150ms step_avg:128.45ms
step:1250/1395 train_time:159284ms step_avg:128.45ms
step:1250/1395 val_loss:3.3169 train_time:159417ms step_avg:128.56ms
step:1251/1395 train_time:159437ms step_avg:128.47ms
step:1252/1395 train_time:159564ms step_avg:128.47ms
step:1253/1395 train_time:159696ms step_avg:128.48ms
step:1254/1395 train_time:159828ms step_avg:128.48ms
step:1255/1395 train_time:159967ms step_avg:128.49ms
step:1256/1395 train_time:160099ms step_avg:128.49ms
step:1257/1395 train_time:160233ms step_avg:128.49ms
step:1258/1395 train_time:160367ms step_avg:128.50ms
step:1259/1395 train_time:160503ms step_avg:128.51ms
step:1260/1395 train_time:160637ms step_avg:128.51ms
step:1261/1395 train_time:160771ms step_avg:128.51ms
step:1262/1395 train_time:160906ms step_avg:128.52ms
step:1263/1395 train_time:161042ms step_avg:128.52ms
step:1264/1395 train_time:161175ms step_avg:128.53ms
step:1265/1395 train_time:161309ms step_avg:128.53ms
step:1266/1395 train_time:161444ms step_avg:128.54ms
step:1267/1395 train_time:161577ms step_avg:128.54ms
step:1268/1395 train_time:161712ms step_avg:128.55ms
step:1269/1395 train_time:161847ms step_avg:128.55ms
step:1270/1395 train_time:161981ms step_avg:128.56ms
step:1271/1395 train_time:162117ms step_avg:128.56ms
step:1272/1395 train_time:162251ms step_avg:128.57ms
step:1273/1395 train_time:162383ms step_avg:128.57ms
step:1274/1395 train_time:162516ms step_avg:128.57ms
step:1275/1395 train_time:162653ms step_avg:128.58ms
step:1276/1395 train_time:162789ms step_avg:128.59ms
step:1277/1395 train_time:162922ms step_avg:128.59ms
step:1278/1395 train_time:163056ms step_avg:128.59ms
step:1279/1395 train_time:163190ms step_avg:128.60ms
step:1280/1395 train_time:163325ms step_avg:128.60ms
step:1281/1395 train_time:163460ms step_avg:128.61ms
step:1282/1395 train_time:163593ms step_avg:128.61ms
step:1283/1395 train_time:163727ms step_avg:128.62ms
step:1284/1395 train_time:163862ms step_avg:128.62ms
step:1285/1395 train_time:163996ms step_avg:128.62ms
step:1286/1395 train_time:164132ms step_avg:128.63ms
step:1287/1395 train_time:164266ms step_avg:128.63ms
step:1288/1395 train_time:164400ms step_avg:128.64ms
step:1289/1395 train_time:164536ms step_avg:128.64ms
step:1290/1395 train_time:164671ms step_avg:128.65ms
step:1291/1395 train_time:164810ms step_avg:128.66ms
step:1292/1395 train_time:164942ms step_avg:128.66ms
step:1293/1395 train_time:165078ms step_avg:128.67ms
step:1294/1395 train_time:165212ms step_avg:128.67ms
step:1295/1395 train_time:165348ms step_avg:128.68ms
step:1296/1395 train_time:165483ms step_avg:128.68ms
step:1297/1395 train_time:165618ms step_avg:128.69ms
step:1298/1395 train_time:165753ms step_avg:128.69ms
step:1299/1395 train_time:165888ms step_avg:128.70ms
step:1300/1395 train_time:166022ms step_avg:128.70ms
step:1301/1395 train_time:166156ms step_avg:128.70ms
step:1302/1395 train_time:166290ms step_avg:128.71ms
step:1303/1395 train_time:166425ms step_avg:128.71ms
step:1304/1395 train_time:166562ms step_avg:128.72ms
step:1305/1395 train_time:166697ms step_avg:128.72ms
step:1306/1395 train_time:166832ms step_avg:128.73ms
step:1307/1395 train_time:166967ms step_avg:128.73ms
step:1308/1395 train_time:167101ms step_avg:128.74ms
step:1309/1395 train_time:167235ms step_avg:128.74ms
step:1310/1395 train_time:167370ms step_avg:128.75ms
step:1311/1395 train_time:167505ms step_avg:128.75ms
step:1312/1395 train_time:167638ms step_avg:128.75ms
step:1313/1395 train_time:167771ms step_avg:128.76ms
step:1314/1395 train_time:167906ms step_avg:128.76ms
step:1315/1395 train_time:168042ms step_avg:128.77ms
step:1316/1395 train_time:168175ms step_avg:128.77ms
step:1317/1395 train_time:168308ms step_avg:128.77ms
step:1318/1395 train_time:168442ms step_avg:128.78ms
step:1319/1395 train_time:168578ms step_avg:128.78ms
step:1320/1395 train_time:168713ms step_avg:128.79ms
step:1321/1395 train_time:168848ms step_avg:128.79ms
step:1322/1395 train_time:168986ms step_avg:128.80ms
step:1323/1395 train_time:169120ms step_avg:128.80ms
step:1324/1395 train_time:169253ms step_avg:128.81ms
step:1325/1395 train_time:169387ms step_avg:128.81ms
step:1326/1395 train_time:169522ms step_avg:128.82ms
step:1327/1395 train_time:169655ms step_avg:128.82ms
step:1328/1395 train_time:169790ms step_avg:128.82ms
step:1329/1395 train_time:169928ms step_avg:128.83ms
step:1330/1395 train_time:170065ms step_avg:128.84ms
step:1331/1395 train_time:170202ms step_avg:128.84ms
step:1332/1395 train_time:170338ms step_avg:128.85ms
step:1333/1395 train_time:170473ms step_avg:128.85ms
step:1334/1395 train_time:170607ms step_avg:128.86ms
step:1335/1395 train_time:170739ms step_avg:128.86ms
step:1336/1395 train_time:170875ms step_avg:128.87ms
step:1337/1395 train_time:171010ms step_avg:128.87ms
step:1338/1395 train_time:171144ms step_avg:128.87ms
step:1339/1395 train_time:171279ms step_avg:128.88ms
step:1340/1395 train_time:171416ms step_avg:128.88ms
step:1341/1395 train_time:171549ms step_avg:128.89ms
step:1342/1395 train_time:171682ms step_avg:128.89ms
step:1343/1395 train_time:171817ms step_avg:128.89ms
step:1344/1395 train_time:171950ms step_avg:128.90ms
step:1345/1395 train_time:172084ms step_avg:128.90ms
step:1346/1395 train_time:172220ms step_avg:128.91ms
step:1347/1395 train_time:172356ms step_avg:128.91ms
step:1348/1395 train_time:172491ms step_avg:128.92ms
step:1349/1395 train_time:172627ms step_avg:128.92ms
step:1350/1395 train_time:172761ms step_avg:128.93ms
step:1351/1395 train_time:172896ms step_avg:128.93ms
step:1352/1395 train_time:173034ms step_avg:128.94ms
step:1353/1395 train_time:173171ms step_avg:128.94ms
step:1354/1395 train_time:173308ms step_avg:128.95ms
step:1355/1395 train_time:173444ms step_avg:128.95ms
step:1356/1395 train_time:173577ms step_avg:128.96ms
step:1357/1395 train_time:173713ms step_avg:128.96ms
step:1358/1395 train_time:173851ms step_avg:128.97ms
step:1359/1395 train_time:173986ms step_avg:128.97ms
step:1360/1395 train_time:174122ms step_avg:128.98ms
step:1361/1395 train_time:174258ms step_avg:128.98ms
step:1362/1395 train_time:174396ms step_avg:128.99ms
step:1363/1395 train_time:174534ms step_avg:129.00ms
step:1364/1395 train_time:174670ms step_avg:129.00ms
step:1365/1395 train_time:174804ms step_avg:129.01ms
step:1366/1395 train_time:174938ms step_avg:129.01ms
step:1367/1395 train_time:175075ms step_avg:129.02ms
step:1368/1395 train_time:175212ms step_avg:129.02ms
step:1369/1395 train_time:175351ms step_avg:129.03ms
step:1370/1395 train_time:175489ms step_avg:129.04ms
step:1371/1395 train_time:175624ms step_avg:129.04ms
step:1372/1395 train_time:175763ms step_avg:129.05ms
step:1373/1395 train_time:175897ms step_avg:129.05ms
step:1374/1395 train_time:176034ms step_avg:129.06ms
step:1375/1395 train_time:176168ms step_avg:129.06ms
step:1375/1395 val_loss:3.2828 train_time:176302ms step_avg:129.16ms
step:1376/1395 train_time:176322ms step_avg:129.08ms
step:1377/1395 train_time:176447ms step_avg:129.08ms
step:1378/1395 train_time:176582ms step_avg:129.08ms
step:1379/1395 train_time:176718ms step_avg:129.09ms
step:1380/1395 train_time:176854ms step_avg:129.09ms
step:1381/1395 train_time:176990ms step_avg:129.10ms
step:1382/1395 train_time:177127ms step_avg:129.10ms
step:1383/1395 train_time:177262ms step_avg:129.11ms
step:1384/1395 train_time:177400ms step_avg:129.11ms
step:1385/1395 train_time:177534ms step_avg:129.12ms
step:1386/1395 train_time:177669ms step_avg:129.12ms
step:1387/1395 train_time:177808ms step_avg:129.13ms
step:1388/1395 train_time:177943ms step_avg:129.13ms
step:1389/1395 train_time:178080ms step_avg:129.14ms
step:1390/1395 train_time:178216ms step_avg:129.14ms
step:1391/1395 train_time:178351ms step_avg:129.15ms
step:1392/1395 train_time:178487ms step_avg:129.15ms
step:1393/1395 train_time:178621ms step_avg:129.15ms
step:1394/1395 train_time:178757ms step_avg:129.16ms
step:1395/1395 train_time:178891ms step_avg:129.16ms
step:1395/1395 val_loss:3.2786 train_time:179026ms step_avg:129.26ms
peak memory allocated: 37653 MiB reserved: 39236 MiB
