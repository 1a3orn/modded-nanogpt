import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True

# -----------------------------------------------------------------------------
# Custom operators

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

def lm_head_fp8(x: Tensor, w: Tensor) -> Tensor:
    _x = x.flatten(0, -2)
    out: Tensor = torch.ops.nanogpt.mm(_x, w, x_s=2.0, w_s=32.0, grad_s=2.0**29)[0]
    return out.reshape(*x.shape[:-1], -1)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev():
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int):
        super().__init__(in_features, out_features, bias=False)

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x):
        return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len=65536):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        # self.c_q = CastedLinear(dim, dim)
        # self.c_k = CastedLinear(dim, dim)
        # self.c_v = CastedLinear(dim, dim)
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std
        self.qkv_w = nn.Parameter(torch.empty(3, dim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads) # dim // num_heads = head_dim
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # Set attention scale such that the minimum attainable attention entropy
        # (but not necessary the attention entropy itself) is close to 0. By @leloykun
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        # q = self.c_q(x).view(B, T, self.num_heads, -1)
        # k = self.c_k(x).view(B, T, self.num_heads, -1)
        # v = self.c_v(x).view(B, T, self.num_heads, -1)
        # qkv_weight = torch.cat([self.c_q.weight, self.c_k.weight, self.c_v.weight], dim=0).type_as(x)
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3*self.num_heads, -1).chunk(3, dim=-2)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, model_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(model_dim, num_heads, layer_idx) if layer_idx != 7 else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, ve, x0, block_mask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, num_embeddings: int, embedding_dim: int):
        super().__init__()
        self.embed = nn.ModuleList([nn.Embedding(num_embeddings, embedding_dim) for _ in range(3)])

    def forward(self, input_seq) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2], None, None, None, None, None, None, ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual learning
        self.value_embeds = ValueEmbedding(vocab_size, model_dim)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128))
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        assert input_seq.ndim == 1
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        docs = (input_seq == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        def create_doc_swc_block_masks(sliding_window_num_blocks: Tensor):
            kv_idx = block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & document_bm
            full_bm  = causal_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            def build_bm(sw_num_blocks: Tensor) -> BlockMask:
                return BlockMask.from_kv_blocks(
                    torch.clamp_max(kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                    kv_indices,
                    torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                    full_kv_indices,
                    BLOCK_SIZE=BLOCK_SIZE,
                    mask_mod=document_causal,
                )
            return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

        # Long-short SWA block masks by @leloykun & @YouJiacheng
        long_bm, short_bm = create_doc_swc_block_masks(sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = lm_head_fp8(x, self.lm_head.weight) if self.training else self.lm_head(x)
        # @Grad62304977 added tanh softcapping, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use cycle(files) if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    batch_size = 8*64*1024 # batch size in tokens
    num_iterations = 1395 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 64*1024 # FlexAttention sequence length
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
             if console:
                 print(s)
             print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_loader = distributed_data_generator(args.train_files, args.batch_size, rank, world_size)

model = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim >= 2]
embed_params = [model.embed.weight, *model.value_embeds.parameters()]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), fused=True, eps=1e-10)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(it: int):
    t = 1 - it / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_bs = world_size * args.seq_len
        assert args.val_tokens % val_bs == 0
        val_steps = args.val_tokens // val_bs
        val_loader = distributed_data_generator(args.val_files, val_bs, rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION BEGIN -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB"
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250110+cu126 compiled for CUDA 12.6
Thu Jan 16 21:30:16 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   37C    P0             120W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   28C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0             118W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   37C    P0             119W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   36C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1395 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1395 train_time:23438ms step_avg:nanms
step:2/1395 train_time:24357ms step_avg:nanms
step:3/1395 train_time:24476ms step_avg:nanms
step:4/1395 train_time:24596ms step_avg:nanms
step:5/1395 train_time:24718ms step_avg:nanms
step:6/1395 train_time:24838ms step_avg:nanms
step:7/1395 train_time:24961ms step_avg:nanms
step:8/1395 train_time:25082ms step_avg:nanms
step:9/1395 train_time:25203ms step_avg:nanms
step:10/1395 train_time:25324ms step_avg:nanms
step:11/1395 train_time:122ms step_avg:nanms
step:12/1395 train_time:244ms step_avg:nanms
step:13/1395 train_time:366ms step_avg:121.96ms
step:14/1395 train_time:488ms step_avg:121.97ms
step:15/1395 train_time:610ms step_avg:121.98ms
step:16/1395 train_time:732ms step_avg:122.00ms
step:17/1395 train_time:853ms step_avg:121.90ms
step:18/1395 train_time:975ms step_avg:121.93ms
step:19/1395 train_time:1100ms step_avg:122.18ms
step:20/1395 train_time:1221ms step_avg:122.12ms
step:21/1395 train_time:1343ms step_avg:122.09ms
step:22/1395 train_time:1464ms step_avg:121.99ms
step:23/1395 train_time:1586ms step_avg:121.97ms
step:24/1395 train_time:1708ms step_avg:121.99ms
step:25/1395 train_time:1829ms step_avg:121.94ms
step:26/1395 train_time:1951ms step_avg:121.94ms
step:27/1395 train_time:2075ms step_avg:122.06ms
step:28/1395 train_time:2197ms step_avg:122.07ms
step:29/1395 train_time:2319ms step_avg:122.04ms
step:30/1395 train_time:2440ms step_avg:122.02ms
step:31/1395 train_time:2563ms step_avg:122.07ms
step:32/1395 train_time:2686ms step_avg:122.11ms
step:33/1395 train_time:2809ms step_avg:122.11ms
step:34/1395 train_time:2932ms step_avg:122.18ms
step:35/1395 train_time:3056ms step_avg:122.22ms
step:36/1395 train_time:3176ms step_avg:122.17ms
step:37/1395 train_time:3298ms step_avg:122.15ms
step:38/1395 train_time:3421ms step_avg:122.16ms
step:39/1395 train_time:3542ms step_avg:122.12ms
step:40/1395 train_time:3664ms step_avg:122.14ms
step:41/1395 train_time:3787ms step_avg:122.15ms
step:42/1395 train_time:3910ms step_avg:122.18ms
step:43/1395 train_time:4033ms step_avg:122.20ms
step:44/1395 train_time:4154ms step_avg:122.18ms
step:45/1395 train_time:4277ms step_avg:122.19ms
step:46/1395 train_time:4399ms step_avg:122.20ms
step:47/1395 train_time:4522ms step_avg:122.21ms
step:48/1395 train_time:4643ms step_avg:122.19ms
step:49/1395 train_time:4765ms step_avg:122.18ms
step:50/1395 train_time:4887ms step_avg:122.18ms
step:51/1395 train_time:5010ms step_avg:122.20ms
step:52/1395 train_time:5132ms step_avg:122.19ms
step:53/1395 train_time:5254ms step_avg:122.18ms
step:54/1395 train_time:5376ms step_avg:122.17ms
step:55/1395 train_time:5500ms step_avg:122.22ms
step:56/1395 train_time:5622ms step_avg:122.22ms
step:57/1395 train_time:5745ms step_avg:122.23ms
step:58/1395 train_time:5867ms step_avg:122.22ms
step:59/1395 train_time:5990ms step_avg:122.25ms
step:60/1395 train_time:6113ms step_avg:122.26ms
step:61/1395 train_time:6235ms step_avg:122.26ms
step:62/1395 train_time:6356ms step_avg:122.24ms
step:63/1395 train_time:6479ms step_avg:122.25ms
step:64/1395 train_time:6601ms step_avg:122.24ms
step:65/1395 train_time:6724ms step_avg:122.26ms
step:66/1395 train_time:6847ms step_avg:122.27ms
step:67/1395 train_time:6969ms step_avg:122.27ms
step:68/1395 train_time:7093ms step_avg:122.29ms
step:69/1395 train_time:7215ms step_avg:122.29ms
step:70/1395 train_time:7337ms step_avg:122.29ms
step:71/1395 train_time:7460ms step_avg:122.29ms
step:72/1395 train_time:7581ms step_avg:122.27ms
step:73/1395 train_time:7704ms step_avg:122.29ms
step:74/1395 train_time:7827ms step_avg:122.30ms
step:75/1395 train_time:7951ms step_avg:122.32ms
step:76/1395 train_time:8073ms step_avg:122.32ms
step:77/1395 train_time:8196ms step_avg:122.33ms
step:78/1395 train_time:8318ms step_avg:122.32ms
step:79/1395 train_time:8440ms step_avg:122.32ms
step:80/1395 train_time:8560ms step_avg:122.29ms
step:81/1395 train_time:8683ms step_avg:122.30ms
step:82/1395 train_time:8804ms step_avg:122.28ms
step:83/1395 train_time:8928ms step_avg:122.30ms
step:84/1395 train_time:9049ms step_avg:122.29ms
step:85/1395 train_time:9171ms step_avg:122.28ms
step:86/1395 train_time:9294ms step_avg:122.29ms
step:87/1395 train_time:9417ms step_avg:122.30ms
step:88/1395 train_time:9540ms step_avg:122.30ms
step:89/1395 train_time:9662ms step_avg:122.30ms
step:90/1395 train_time:9783ms step_avg:122.29ms
step:91/1395 train_time:9906ms step_avg:122.29ms
step:92/1395 train_time:10028ms step_avg:122.29ms
step:93/1395 train_time:10150ms step_avg:122.29ms
step:94/1395 train_time:10272ms step_avg:122.28ms
step:95/1395 train_time:10395ms step_avg:122.30ms
step:96/1395 train_time:10517ms step_avg:122.29ms
step:97/1395 train_time:10639ms step_avg:122.29ms
step:98/1395 train_time:10761ms step_avg:122.29ms
step:99/1395 train_time:10884ms step_avg:122.29ms
step:100/1395 train_time:11007ms step_avg:122.30ms
step:101/1395 train_time:11130ms step_avg:122.31ms
step:102/1395 train_time:11252ms step_avg:122.30ms
step:103/1395 train_time:11375ms step_avg:122.31ms
step:104/1395 train_time:11496ms step_avg:122.29ms
step:105/1395 train_time:11619ms step_avg:122.30ms
step:106/1395 train_time:11741ms step_avg:122.30ms
step:107/1395 train_time:11864ms step_avg:122.31ms
step:108/1395 train_time:11986ms step_avg:122.31ms
step:109/1395 train_time:12110ms step_avg:122.33ms
step:110/1395 train_time:12232ms step_avg:122.32ms
step:111/1395 train_time:12355ms step_avg:122.33ms
step:112/1395 train_time:12478ms step_avg:122.33ms
step:113/1395 train_time:12602ms step_avg:122.35ms
step:114/1395 train_time:12724ms step_avg:122.35ms
step:115/1395 train_time:12846ms step_avg:122.35ms
step:116/1395 train_time:12969ms step_avg:122.35ms
step:117/1395 train_time:13093ms step_avg:122.36ms
step:118/1395 train_time:13216ms step_avg:122.37ms
step:119/1395 train_time:13340ms step_avg:122.38ms
step:120/1395 train_time:13463ms step_avg:122.39ms
step:121/1395 train_time:13585ms step_avg:122.39ms
step:122/1395 train_time:13708ms step_avg:122.39ms
step:123/1395 train_time:13831ms step_avg:122.40ms
step:124/1395 train_time:13953ms step_avg:122.40ms
step:125/1395 train_time:14076ms step_avg:122.40ms
step:125/1395 val_loss:4.3844 train_time:14198ms step_avg:123.46ms
step:126/1395 train_time:14222ms step_avg:122.60ms
step:127/1395 train_time:14334ms step_avg:122.51ms
step:128/1395 train_time:14462ms step_avg:122.56ms
step:129/1395 train_time:14585ms step_avg:122.57ms
step:130/1395 train_time:14707ms step_avg:122.56ms
step:131/1395 train_time:14829ms step_avg:122.55ms
step:132/1395 train_time:14951ms step_avg:122.55ms
step:133/1395 train_time:15073ms step_avg:122.55ms
step:134/1395 train_time:15196ms step_avg:122.55ms
step:135/1395 train_time:15319ms step_avg:122.55ms
step:136/1395 train_time:15442ms step_avg:122.56ms
step:137/1395 train_time:15565ms step_avg:122.56ms
step:138/1395 train_time:15688ms step_avg:122.57ms
step:139/1395 train_time:15811ms step_avg:122.56ms
step:140/1395 train_time:15933ms step_avg:122.56ms
step:141/1395 train_time:16055ms step_avg:122.56ms
step:142/1395 train_time:16178ms step_avg:122.56ms
step:143/1395 train_time:16301ms step_avg:122.56ms
step:144/1395 train_time:16425ms step_avg:122.57ms
step:145/1395 train_time:16547ms step_avg:122.57ms
step:146/1395 train_time:16670ms step_avg:122.57ms
step:147/1395 train_time:16792ms step_avg:122.57ms
step:148/1395 train_time:16915ms step_avg:122.57ms
step:149/1395 train_time:17037ms step_avg:122.57ms
step:150/1395 train_time:17161ms step_avg:122.58ms
step:151/1395 train_time:17284ms step_avg:122.58ms
step:152/1395 train_time:17407ms step_avg:122.59ms
step:153/1395 train_time:17530ms step_avg:122.59ms
step:154/1395 train_time:17652ms step_avg:122.58ms
step:155/1395 train_time:17775ms step_avg:122.59ms
step:156/1395 train_time:17898ms step_avg:122.59ms
step:157/1395 train_time:18020ms step_avg:122.59ms
step:158/1395 train_time:18143ms step_avg:122.59ms
step:159/1395 train_time:18266ms step_avg:122.59ms
step:160/1395 train_time:18389ms step_avg:122.60ms
step:161/1395 train_time:18512ms step_avg:122.60ms
step:162/1395 train_time:18636ms step_avg:122.61ms
step:163/1395 train_time:18761ms step_avg:122.62ms
step:164/1395 train_time:18883ms step_avg:122.62ms
step:165/1395 train_time:19006ms step_avg:122.62ms
step:166/1395 train_time:19129ms step_avg:122.62ms
step:167/1395 train_time:19252ms step_avg:122.63ms
step:168/1395 train_time:19376ms step_avg:122.63ms
step:169/1395 train_time:19498ms step_avg:122.63ms
step:170/1395 train_time:19622ms step_avg:122.63ms
step:171/1395 train_time:19744ms step_avg:122.63ms
step:172/1395 train_time:19867ms step_avg:122.64ms
step:173/1395 train_time:19990ms step_avg:122.64ms
step:174/1395 train_time:20113ms step_avg:122.64ms
step:175/1395 train_time:20235ms step_avg:122.64ms
step:176/1395 train_time:20359ms step_avg:122.65ms
step:177/1395 train_time:20482ms step_avg:122.65ms
step:178/1395 train_time:20605ms step_avg:122.65ms
step:179/1395 train_time:20728ms step_avg:122.65ms
step:180/1395 train_time:20851ms step_avg:122.65ms
step:181/1395 train_time:20974ms step_avg:122.65ms
step:182/1395 train_time:21097ms step_avg:122.65ms
step:183/1395 train_time:21219ms step_avg:122.65ms
step:184/1395 train_time:21342ms step_avg:122.66ms
step:185/1395 train_time:21466ms step_avg:122.66ms
step:186/1395 train_time:21588ms step_avg:122.66ms
step:187/1395 train_time:21710ms step_avg:122.66ms
step:188/1395 train_time:21833ms step_avg:122.66ms
step:189/1395 train_time:21956ms step_avg:122.66ms
step:190/1395 train_time:22080ms step_avg:122.67ms
step:191/1395 train_time:22202ms step_avg:122.66ms
step:192/1395 train_time:22325ms step_avg:122.66ms
step:193/1395 train_time:22448ms step_avg:122.66ms
step:194/1395 train_time:22571ms step_avg:122.67ms
step:195/1395 train_time:22693ms step_avg:122.67ms
step:196/1395 train_time:22816ms step_avg:122.67ms
step:197/1395 train_time:22939ms step_avg:122.67ms
step:198/1395 train_time:23063ms step_avg:122.68ms
step:199/1395 train_time:23184ms step_avg:122.67ms
step:200/1395 train_time:23307ms step_avg:122.67ms
step:201/1395 train_time:23429ms step_avg:122.66ms
step:202/1395 train_time:23553ms step_avg:122.67ms
step:203/1395 train_time:23676ms step_avg:122.67ms
step:204/1395 train_time:23799ms step_avg:122.68ms
step:205/1395 train_time:23921ms step_avg:122.67ms
step:206/1395 train_time:24045ms step_avg:122.68ms
step:207/1395 train_time:24167ms step_avg:122.67ms
step:208/1395 train_time:24289ms step_avg:122.67ms
step:209/1395 train_time:24413ms step_avg:122.68ms
step:210/1395 train_time:24535ms step_avg:122.68ms
step:211/1395 train_time:24660ms step_avg:122.68ms
step:212/1395 train_time:24784ms step_avg:122.69ms
step:213/1395 train_time:24907ms step_avg:122.70ms
step:214/1395 train_time:25031ms step_avg:122.70ms
step:215/1395 train_time:25154ms step_avg:122.70ms
step:216/1395 train_time:25277ms step_avg:122.71ms
step:217/1395 train_time:25400ms step_avg:122.71ms
step:218/1395 train_time:25524ms step_avg:122.71ms
step:219/1395 train_time:25647ms step_avg:122.71ms
step:220/1395 train_time:25769ms step_avg:122.71ms
step:221/1395 train_time:25893ms step_avg:122.71ms
step:222/1395 train_time:26018ms step_avg:122.72ms
step:223/1395 train_time:26141ms step_avg:122.73ms
step:224/1395 train_time:26265ms step_avg:122.73ms
step:225/1395 train_time:26388ms step_avg:122.73ms
step:226/1395 train_time:26511ms step_avg:122.74ms
step:227/1395 train_time:26635ms step_avg:122.74ms
step:228/1395 train_time:26758ms step_avg:122.74ms
step:229/1395 train_time:26882ms step_avg:122.75ms
step:230/1395 train_time:27007ms step_avg:122.76ms
step:231/1395 train_time:27131ms step_avg:122.76ms
step:232/1395 train_time:27254ms step_avg:122.76ms
step:233/1395 train_time:27377ms step_avg:122.77ms
step:234/1395 train_time:27500ms step_avg:122.77ms
step:235/1395 train_time:27623ms step_avg:122.77ms
step:236/1395 train_time:27747ms step_avg:122.77ms
step:237/1395 train_time:27869ms step_avg:122.77ms
step:238/1395 train_time:27992ms step_avg:122.77ms
step:239/1395 train_time:28116ms step_avg:122.78ms
step:240/1395 train_time:28240ms step_avg:122.78ms
step:241/1395 train_time:28363ms step_avg:122.78ms
step:242/1395 train_time:28487ms step_avg:122.79ms
step:243/1395 train_time:28610ms step_avg:122.79ms
step:244/1395 train_time:28733ms step_avg:122.79ms
step:245/1395 train_time:28856ms step_avg:122.79ms
step:246/1395 train_time:28981ms step_avg:122.80ms
step:247/1395 train_time:29104ms step_avg:122.80ms
step:248/1395 train_time:29227ms step_avg:122.80ms
step:249/1395 train_time:29350ms step_avg:122.80ms
step:250/1395 train_time:29474ms step_avg:122.81ms
step:250/1395 val_loss:3.9845 train_time:29595ms step_avg:123.31ms
step:251/1395 train_time:29619ms step_avg:122.90ms
step:252/1395 train_time:29732ms step_avg:122.86ms
step:253/1395 train_time:29856ms step_avg:122.86ms
step:254/1395 train_time:29979ms step_avg:122.86ms
step:255/1395 train_time:30101ms step_avg:122.86ms
step:256/1395 train_time:30223ms step_avg:122.86ms
step:257/1395 train_time:30346ms step_avg:122.86ms
step:258/1395 train_time:30469ms step_avg:122.86ms
step:259/1395 train_time:30592ms step_avg:122.86ms
step:260/1395 train_time:30716ms step_avg:122.86ms
step:261/1395 train_time:30841ms step_avg:122.87ms
step:262/1395 train_time:30965ms step_avg:122.88ms
step:263/1395 train_time:31090ms step_avg:122.88ms
step:264/1395 train_time:31212ms step_avg:122.88ms
step:265/1395 train_time:31336ms step_avg:122.89ms
step:266/1395 train_time:31458ms step_avg:122.88ms
step:267/1395 train_time:31581ms step_avg:122.88ms
step:268/1395 train_time:31704ms step_avg:122.88ms
step:269/1395 train_time:31826ms step_avg:122.88ms
step:270/1395 train_time:31949ms step_avg:122.88ms
step:271/1395 train_time:32074ms step_avg:122.89ms
step:272/1395 train_time:32197ms step_avg:122.89ms
step:273/1395 train_time:32321ms step_avg:122.89ms
step:274/1395 train_time:32444ms step_avg:122.90ms
step:275/1395 train_time:32568ms step_avg:122.90ms
step:276/1395 train_time:32690ms step_avg:122.90ms
step:277/1395 train_time:32815ms step_avg:122.90ms
step:278/1395 train_time:32937ms step_avg:122.90ms
step:279/1395 train_time:33061ms step_avg:122.90ms
step:280/1395 train_time:33185ms step_avg:122.91ms
step:281/1395 train_time:33309ms step_avg:122.91ms
step:282/1395 train_time:33433ms step_avg:122.91ms
step:283/1395 train_time:33557ms step_avg:122.92ms
step:284/1395 train_time:33680ms step_avg:122.92ms
step:285/1395 train_time:33802ms step_avg:122.92ms
step:286/1395 train_time:33924ms step_avg:122.91ms
step:287/1395 train_time:34048ms step_avg:122.92ms
step:288/1395 train_time:34170ms step_avg:122.92ms
step:289/1395 train_time:34294ms step_avg:122.92ms
step:290/1395 train_time:34417ms step_avg:122.92ms
step:291/1395 train_time:34540ms step_avg:122.92ms
step:292/1395 train_time:34663ms step_avg:122.92ms
step:293/1395 train_time:34787ms step_avg:122.92ms
step:294/1395 train_time:34911ms step_avg:122.93ms
step:295/1395 train_time:35034ms step_avg:122.93ms
step:296/1395 train_time:35159ms step_avg:122.94ms
step:297/1395 train_time:35282ms step_avg:122.94ms
step:298/1395 train_time:35405ms step_avg:122.93ms
step:299/1395 train_time:35528ms step_avg:122.93ms
step:300/1395 train_time:35652ms step_avg:122.94ms
step:301/1395 train_time:35775ms step_avg:122.94ms
step:302/1395 train_time:35898ms step_avg:122.94ms
step:303/1395 train_time:36021ms step_avg:122.94ms
step:304/1395 train_time:36145ms step_avg:122.94ms
step:305/1395 train_time:36268ms step_avg:122.94ms
step:306/1395 train_time:36391ms step_avg:122.94ms
step:307/1395 train_time:36515ms step_avg:122.95ms
step:308/1395 train_time:36639ms step_avg:122.95ms
step:309/1395 train_time:36762ms step_avg:122.95ms
step:310/1395 train_time:36884ms step_avg:122.95ms
step:311/1395 train_time:37008ms step_avg:122.95ms
step:312/1395 train_time:37132ms step_avg:122.95ms
step:313/1395 train_time:37258ms step_avg:122.96ms
step:314/1395 train_time:37383ms step_avg:122.97ms
step:315/1395 train_time:37510ms step_avg:122.98ms
step:316/1395 train_time:37635ms step_avg:122.99ms
step:317/1395 train_time:37761ms step_avg:123.00ms
step:318/1395 train_time:37887ms step_avg:123.01ms
step:319/1395 train_time:38013ms step_avg:123.02ms
step:320/1395 train_time:38139ms step_avg:123.03ms
step:321/1395 train_time:38266ms step_avg:123.04ms
step:322/1395 train_time:38392ms step_avg:123.05ms
step:323/1395 train_time:38518ms step_avg:123.06ms
step:324/1395 train_time:38644ms step_avg:123.07ms
step:325/1395 train_time:38770ms step_avg:123.08ms
step:326/1395 train_time:38896ms step_avg:123.09ms
step:327/1395 train_time:39023ms step_avg:123.10ms
step:328/1395 train_time:39149ms step_avg:123.11ms
step:329/1395 train_time:39275ms step_avg:123.12ms
step:330/1395 train_time:39401ms step_avg:123.13ms
step:331/1395 train_time:39527ms step_avg:123.14ms
step:332/1395 train_time:39653ms step_avg:123.15ms
step:333/1395 train_time:39780ms step_avg:123.16ms
step:334/1395 train_time:39906ms step_avg:123.17ms
step:335/1395 train_time:40033ms step_avg:123.18ms
step:336/1395 train_time:40159ms step_avg:123.19ms
step:337/1395 train_time:40285ms step_avg:123.20ms
step:338/1395 train_time:40411ms step_avg:123.20ms
step:339/1395 train_time:40537ms step_avg:123.21ms
step:340/1395 train_time:40663ms step_avg:123.22ms
step:341/1395 train_time:40789ms step_avg:123.23ms
step:342/1395 train_time:40914ms step_avg:123.23ms
step:343/1395 train_time:41039ms step_avg:123.24ms
step:344/1395 train_time:41166ms step_avg:123.25ms
step:345/1395 train_time:41292ms step_avg:123.26ms
step:346/1395 train_time:41417ms step_avg:123.27ms
step:347/1395 train_time:41543ms step_avg:123.27ms
step:348/1395 train_time:41669ms step_avg:123.28ms
step:349/1395 train_time:41796ms step_avg:123.29ms
step:350/1395 train_time:41922ms step_avg:123.30ms
step:351/1395 train_time:42048ms step_avg:123.31ms
step:352/1395 train_time:42174ms step_avg:123.31ms
step:353/1395 train_time:42299ms step_avg:123.32ms
step:354/1395 train_time:42425ms step_avg:123.33ms
step:355/1395 train_time:42552ms step_avg:123.34ms
step:356/1395 train_time:42677ms step_avg:123.35ms
step:357/1395 train_time:42803ms step_avg:123.35ms
step:358/1395 train_time:42930ms step_avg:123.36ms
step:359/1395 train_time:43057ms step_avg:123.37ms
step:360/1395 train_time:43183ms step_avg:123.38ms
step:361/1395 train_time:43309ms step_avg:123.39ms
step:362/1395 train_time:43435ms step_avg:123.39ms
step:363/1395 train_time:43560ms step_avg:123.40ms
step:364/1395 train_time:43687ms step_avg:123.41ms
step:365/1395 train_time:43813ms step_avg:123.42ms
step:366/1395 train_time:43939ms step_avg:123.42ms
step:367/1395 train_time:44065ms step_avg:123.43ms
step:368/1395 train_time:44191ms step_avg:123.44ms
step:369/1395 train_time:44317ms step_avg:123.45ms
step:370/1395 train_time:44444ms step_avg:123.45ms
step:371/1395 train_time:44570ms step_avg:123.46ms
step:372/1395 train_time:44695ms step_avg:123.47ms
step:373/1395 train_time:44821ms step_avg:123.47ms
step:374/1395 train_time:44948ms step_avg:123.48ms
step:375/1395 train_time:45073ms step_avg:123.49ms
step:375/1395 val_loss:3.7863 train_time:45198ms step_avg:123.83ms
step:376/1395 train_time:45220ms step_avg:123.55ms
step:377/1395 train_time:45338ms step_avg:123.54ms
step:378/1395 train_time:45468ms step_avg:123.55ms
step:379/1395 train_time:45593ms step_avg:123.56ms
step:380/1395 train_time:45718ms step_avg:123.56ms
step:381/1395 train_time:45843ms step_avg:123.57ms
step:382/1395 train_time:45969ms step_avg:123.57ms
step:383/1395 train_time:46095ms step_avg:123.58ms
step:384/1395 train_time:46220ms step_avg:123.58ms
step:385/1395 train_time:46346ms step_avg:123.59ms
step:386/1395 train_time:46472ms step_avg:123.60ms
step:387/1395 train_time:46599ms step_avg:123.60ms
step:388/1395 train_time:46724ms step_avg:123.61ms
step:389/1395 train_time:46850ms step_avg:123.61ms
step:390/1395 train_time:46975ms step_avg:123.62ms
step:391/1395 train_time:47100ms step_avg:123.62ms
step:392/1395 train_time:47226ms step_avg:123.63ms
step:393/1395 train_time:47352ms step_avg:123.64ms
step:394/1395 train_time:47478ms step_avg:123.64ms
step:395/1395 train_time:47604ms step_avg:123.65ms
step:396/1395 train_time:47729ms step_avg:123.65ms
step:397/1395 train_time:47857ms step_avg:123.66ms
step:398/1395 train_time:47983ms step_avg:123.67ms
step:399/1395 train_time:48108ms step_avg:123.67ms
step:400/1395 train_time:48234ms step_avg:123.68ms
step:401/1395 train_time:48359ms step_avg:123.68ms
step:402/1395 train_time:48485ms step_avg:123.69ms
step:403/1395 train_time:48613ms step_avg:123.70ms
step:404/1395 train_time:48738ms step_avg:123.70ms
step:405/1395 train_time:48864ms step_avg:123.71ms
step:406/1395 train_time:48990ms step_avg:123.71ms
step:407/1395 train_time:49117ms step_avg:123.72ms
step:408/1395 train_time:49243ms step_avg:123.73ms
step:409/1395 train_time:49368ms step_avg:123.73ms
step:410/1395 train_time:49494ms step_avg:123.73ms
step:411/1395 train_time:49619ms step_avg:123.74ms
step:412/1395 train_time:49745ms step_avg:123.74ms
step:413/1395 train_time:49871ms step_avg:123.75ms
step:414/1395 train_time:49997ms step_avg:123.76ms
step:415/1395 train_time:50123ms step_avg:123.76ms
step:416/1395 train_time:50250ms step_avg:123.77ms
step:417/1395 train_time:50375ms step_avg:123.77ms
step:418/1395 train_time:50502ms step_avg:123.78ms
step:419/1395 train_time:50630ms step_avg:123.79ms
step:420/1395 train_time:50757ms step_avg:123.80ms
step:421/1395 train_time:50882ms step_avg:123.80ms
step:422/1395 train_time:51010ms step_avg:123.81ms
step:423/1395 train_time:51136ms step_avg:123.82ms
step:424/1395 train_time:51263ms step_avg:123.82ms
step:425/1395 train_time:51389ms step_avg:123.83ms
step:426/1395 train_time:51516ms step_avg:123.84ms
step:427/1395 train_time:51642ms step_avg:123.84ms
step:428/1395 train_time:51769ms step_avg:123.85ms
step:429/1395 train_time:51895ms step_avg:123.85ms
step:430/1395 train_time:52021ms step_avg:123.86ms
step:431/1395 train_time:52148ms step_avg:123.87ms
step:432/1395 train_time:52275ms step_avg:123.88ms
step:433/1395 train_time:52402ms step_avg:123.88ms
step:434/1395 train_time:52528ms step_avg:123.89ms
step:435/1395 train_time:52654ms step_avg:123.89ms
step:436/1395 train_time:52781ms step_avg:123.90ms
step:437/1395 train_time:52907ms step_avg:123.90ms
step:438/1395 train_time:53033ms step_avg:123.91ms
step:439/1395 train_time:53159ms step_avg:123.91ms
step:440/1395 train_time:53285ms step_avg:123.92ms
step:441/1395 train_time:53413ms step_avg:123.93ms
step:442/1395 train_time:53539ms step_avg:123.93ms
step:443/1395 train_time:53666ms step_avg:123.94ms
step:444/1395 train_time:53793ms step_avg:123.95ms
step:445/1395 train_time:53919ms step_avg:123.95ms
step:446/1395 train_time:54045ms step_avg:123.96ms
step:447/1395 train_time:54172ms step_avg:123.96ms
step:448/1395 train_time:54298ms step_avg:123.97ms
step:449/1395 train_time:54424ms step_avg:123.97ms
step:450/1395 train_time:54550ms step_avg:123.98ms
step:451/1395 train_time:54677ms step_avg:123.98ms
step:452/1395 train_time:54805ms step_avg:123.99ms
step:453/1395 train_time:54931ms step_avg:124.00ms
step:454/1395 train_time:55057ms step_avg:124.00ms
step:455/1395 train_time:55184ms step_avg:124.01ms
step:456/1395 train_time:55310ms step_avg:124.01ms
step:457/1395 train_time:55436ms step_avg:124.02ms
step:458/1395 train_time:55562ms step_avg:124.02ms
step:459/1395 train_time:55689ms step_avg:124.03ms
step:460/1395 train_time:55816ms step_avg:124.04ms
step:461/1395 train_time:55943ms step_avg:124.04ms
step:462/1395 train_time:56069ms step_avg:124.05ms
step:463/1395 train_time:56197ms step_avg:124.06ms
step:464/1395 train_time:56323ms step_avg:124.06ms
step:465/1395 train_time:56449ms step_avg:124.06ms
step:466/1395 train_time:56576ms step_avg:124.07ms
step:467/1395 train_time:56702ms step_avg:124.07ms
step:468/1395 train_time:56829ms step_avg:124.08ms
step:469/1395 train_time:56955ms step_avg:124.08ms
step:470/1395 train_time:57081ms step_avg:124.09ms
step:471/1395 train_time:57208ms step_avg:124.09ms
step:472/1395 train_time:57335ms step_avg:124.10ms
step:473/1395 train_time:57460ms step_avg:124.10ms
step:474/1395 train_time:57587ms step_avg:124.11ms
step:475/1395 train_time:57714ms step_avg:124.12ms
step:476/1395 train_time:57841ms step_avg:124.12ms
step:477/1395 train_time:57966ms step_avg:124.12ms
step:478/1395 train_time:58092ms step_avg:124.13ms
step:479/1395 train_time:58218ms step_avg:124.13ms
step:480/1395 train_time:58345ms step_avg:124.14ms
step:481/1395 train_time:58471ms step_avg:124.14ms
step:482/1395 train_time:58597ms step_avg:124.15ms
step:483/1395 train_time:58723ms step_avg:124.15ms
step:484/1395 train_time:58849ms step_avg:124.15ms
step:485/1395 train_time:58975ms step_avg:124.16ms
step:486/1395 train_time:59102ms step_avg:124.16ms
step:487/1395 train_time:59228ms step_avg:124.17ms
step:488/1395 train_time:59356ms step_avg:124.18ms
step:489/1395 train_time:59483ms step_avg:124.18ms
step:490/1395 train_time:59610ms step_avg:124.19ms
step:491/1395 train_time:59736ms step_avg:124.19ms
step:492/1395 train_time:59862ms step_avg:124.20ms
step:493/1395 train_time:59988ms step_avg:124.20ms
step:494/1395 train_time:60115ms step_avg:124.21ms
step:495/1395 train_time:60242ms step_avg:124.21ms
step:496/1395 train_time:60369ms step_avg:124.22ms
step:497/1395 train_time:60495ms step_avg:124.22ms
step:498/1395 train_time:60622ms step_avg:124.23ms
step:499/1395 train_time:60749ms step_avg:124.23ms
step:500/1395 train_time:60876ms step_avg:124.24ms
step:500/1395 val_loss:3.6665 train_time:61001ms step_avg:124.49ms
step:501/1395 train_time:61022ms step_avg:124.28ms
step:502/1395 train_time:61143ms step_avg:124.27ms
step:503/1395 train_time:61271ms step_avg:124.28ms
step:504/1395 train_time:61397ms step_avg:124.29ms
step:505/1395 train_time:61523ms step_avg:124.29ms
step:506/1395 train_time:61648ms step_avg:124.29ms
step:507/1395 train_time:61773ms step_avg:124.29ms
step:508/1395 train_time:61899ms step_avg:124.30ms
step:509/1395 train_time:62025ms step_avg:124.30ms
step:510/1395 train_time:62153ms step_avg:124.31ms
step:511/1395 train_time:62281ms step_avg:124.31ms
step:512/1395 train_time:62407ms step_avg:124.32ms
step:513/1395 train_time:62533ms step_avg:124.32ms
step:514/1395 train_time:62658ms step_avg:124.32ms
step:515/1395 train_time:62783ms step_avg:124.32ms
step:516/1395 train_time:62909ms step_avg:124.33ms
step:517/1395 train_time:63035ms step_avg:124.33ms
step:518/1395 train_time:63162ms step_avg:124.33ms
step:519/1395 train_time:63291ms step_avg:124.34ms
step:520/1395 train_time:63419ms step_avg:124.35ms
step:521/1395 train_time:63548ms step_avg:124.36ms
step:522/1395 train_time:63676ms step_avg:124.37ms
step:523/1395 train_time:63804ms step_avg:124.38ms
step:524/1395 train_time:63932ms step_avg:124.38ms
step:525/1395 train_time:64061ms step_avg:124.39ms
step:526/1395 train_time:64189ms step_avg:124.40ms
step:527/1395 train_time:64318ms step_avg:124.41ms
step:528/1395 train_time:64446ms step_avg:124.41ms
step:529/1395 train_time:64575ms step_avg:124.42ms
step:530/1395 train_time:64703ms step_avg:124.43ms
step:531/1395 train_time:64831ms step_avg:124.44ms
step:532/1395 train_time:64959ms step_avg:124.44ms
step:533/1395 train_time:65088ms step_avg:124.45ms
step:534/1395 train_time:65217ms step_avg:124.46ms
step:535/1395 train_time:65345ms step_avg:124.47ms
step:536/1395 train_time:65472ms step_avg:124.47ms
step:537/1395 train_time:65600ms step_avg:124.48ms
step:538/1395 train_time:65729ms step_avg:124.49ms
step:539/1395 train_time:65857ms step_avg:124.49ms
step:540/1395 train_time:65986ms step_avg:124.50ms
step:541/1395 train_time:66114ms step_avg:124.51ms
step:542/1395 train_time:66243ms step_avg:124.52ms
step:543/1395 train_time:66371ms step_avg:124.52ms
step:544/1395 train_time:66499ms step_avg:124.53ms
step:545/1395 train_time:66628ms step_avg:124.54ms
step:546/1395 train_time:66757ms step_avg:124.55ms
step:547/1395 train_time:66886ms step_avg:124.55ms
step:548/1395 train_time:67015ms step_avg:124.56ms
step:549/1395 train_time:67144ms step_avg:124.57ms
step:550/1395 train_time:67271ms step_avg:124.58ms
step:551/1395 train_time:67400ms step_avg:124.58ms
step:552/1395 train_time:67530ms step_avg:124.59ms
step:553/1395 train_time:67658ms step_avg:124.60ms
step:554/1395 train_time:67786ms step_avg:124.61ms
step:555/1395 train_time:67914ms step_avg:124.61ms
step:556/1395 train_time:68043ms step_avg:124.62ms
step:557/1395 train_time:68172ms step_avg:124.63ms
step:558/1395 train_time:68300ms step_avg:124.63ms
step:559/1395 train_time:68429ms step_avg:124.64ms
step:560/1395 train_time:68558ms step_avg:124.65ms
step:561/1395 train_time:68685ms step_avg:124.66ms
step:562/1395 train_time:68813ms step_avg:124.66ms
step:563/1395 train_time:68942ms step_avg:124.67ms
step:564/1395 train_time:69070ms step_avg:124.68ms
step:565/1395 train_time:69199ms step_avg:124.68ms
step:566/1395 train_time:69327ms step_avg:124.69ms
step:567/1395 train_time:69455ms step_avg:124.70ms
step:568/1395 train_time:69584ms step_avg:124.70ms
step:569/1395 train_time:69713ms step_avg:124.71ms
step:570/1395 train_time:69840ms step_avg:124.72ms
step:571/1395 train_time:69969ms step_avg:124.72ms
step:572/1395 train_time:70098ms step_avg:124.73ms
step:573/1395 train_time:70227ms step_avg:124.74ms
step:574/1395 train_time:70357ms step_avg:124.75ms
step:575/1395 train_time:70484ms step_avg:124.75ms
step:576/1395 train_time:70613ms step_avg:124.76ms
step:577/1395 train_time:70740ms step_avg:124.76ms
step:578/1395 train_time:70868ms step_avg:124.77ms
step:579/1395 train_time:70997ms step_avg:124.77ms
step:580/1395 train_time:71126ms step_avg:124.78ms
step:581/1395 train_time:71254ms step_avg:124.79ms
step:582/1395 train_time:71382ms step_avg:124.79ms
step:583/1395 train_time:71510ms step_avg:124.80ms
step:584/1395 train_time:71638ms step_avg:124.81ms
step:585/1395 train_time:71767ms step_avg:124.81ms
step:586/1395 train_time:71895ms step_avg:124.82ms
step:587/1395 train_time:72023ms step_avg:124.82ms
step:588/1395 train_time:72151ms step_avg:124.83ms
step:589/1395 train_time:72280ms step_avg:124.84ms
step:590/1395 train_time:72408ms step_avg:124.84ms
step:591/1395 train_time:72536ms step_avg:124.85ms
step:592/1395 train_time:72665ms step_avg:124.85ms
step:593/1395 train_time:72794ms step_avg:124.86ms
step:594/1395 train_time:72922ms step_avg:124.87ms
step:595/1395 train_time:73051ms step_avg:124.87ms
step:596/1395 train_time:73179ms step_avg:124.88ms
step:597/1395 train_time:73309ms step_avg:124.89ms
step:598/1395 train_time:73438ms step_avg:124.89ms
step:599/1395 train_time:73567ms step_avg:124.90ms
step:600/1395 train_time:73696ms step_avg:124.91ms
step:601/1395 train_time:73824ms step_avg:124.91ms
step:602/1395 train_time:73953ms step_avg:124.92ms
step:603/1395 train_time:74081ms step_avg:124.93ms
step:604/1395 train_time:74209ms step_avg:124.93ms
step:605/1395 train_time:74338ms step_avg:124.94ms
step:606/1395 train_time:74466ms step_avg:124.94ms
step:607/1395 train_time:74595ms step_avg:124.95ms
step:608/1395 train_time:74723ms step_avg:124.96ms
step:609/1395 train_time:74852ms step_avg:124.96ms
step:610/1395 train_time:74980ms step_avg:124.97ms
step:611/1395 train_time:75108ms step_avg:124.97ms
step:612/1395 train_time:75236ms step_avg:124.98ms
step:613/1395 train_time:75364ms step_avg:124.98ms
step:614/1395 train_time:75493ms step_avg:124.99ms
step:615/1395 train_time:75621ms step_avg:124.99ms
step:616/1395 train_time:75749ms step_avg:125.00ms
step:617/1395 train_time:75877ms step_avg:125.00ms
step:618/1395 train_time:76005ms step_avg:125.01ms
step:619/1395 train_time:76134ms step_avg:125.01ms
step:620/1395 train_time:76264ms step_avg:125.02ms
step:621/1395 train_time:76393ms step_avg:125.03ms
step:622/1395 train_time:76520ms step_avg:125.03ms
step:623/1395 train_time:76649ms step_avg:125.04ms
step:624/1395 train_time:76777ms step_avg:125.04ms
step:625/1395 train_time:76906ms step_avg:125.05ms
step:625/1395 val_loss:3.5831 train_time:77033ms step_avg:125.26ms
step:626/1395 train_time:77055ms step_avg:125.09ms
step:627/1395 train_time:77177ms step_avg:125.08ms
step:628/1395 train_time:77306ms step_avg:125.09ms
step:629/1395 train_time:77434ms step_avg:125.10ms
step:630/1395 train_time:77562ms step_avg:125.10ms
step:631/1395 train_time:77690ms step_avg:125.10ms
step:632/1395 train_time:77817ms step_avg:125.11ms
step:633/1395 train_time:77945ms step_avg:125.11ms
step:634/1395 train_time:78074ms step_avg:125.12ms
step:635/1395 train_time:78203ms step_avg:125.13ms
step:636/1395 train_time:78335ms step_avg:125.14ms
step:637/1395 train_time:78463ms step_avg:125.14ms
step:638/1395 train_time:78593ms step_avg:125.15ms
step:639/1395 train_time:78721ms step_avg:125.15ms
step:640/1395 train_time:78849ms step_avg:125.16ms
step:641/1395 train_time:78978ms step_avg:125.16ms
step:642/1395 train_time:79107ms step_avg:125.17ms
step:643/1395 train_time:79236ms step_avg:125.18ms
step:644/1395 train_time:79365ms step_avg:125.18ms
step:645/1395 train_time:79495ms step_avg:125.19ms
step:646/1395 train_time:79623ms step_avg:125.19ms
step:647/1395 train_time:79751ms step_avg:125.20ms
step:648/1395 train_time:79880ms step_avg:125.20ms
step:649/1395 train_time:80009ms step_avg:125.21ms
step:650/1395 train_time:80138ms step_avg:125.22ms
step:651/1395 train_time:80267ms step_avg:125.22ms
step:652/1395 train_time:80397ms step_avg:125.23ms
step:653/1395 train_time:80525ms step_avg:125.23ms
step:654/1395 train_time:80653ms step_avg:125.24ms
step:655/1395 train_time:80782ms step_avg:125.24ms
step:656/1395 train_time:80911ms step_avg:125.25ms
step:657/1395 train_time:81039ms step_avg:125.25ms
step:658/1395 train_time:81169ms step_avg:125.26ms
step:659/1395 train_time:81298ms step_avg:125.27ms
step:660/1395 train_time:81427ms step_avg:125.27ms
step:661/1395 train_time:81558ms step_avg:125.28ms
step:662/1395 train_time:81688ms step_avg:125.29ms
step:663/1395 train_time:81816ms step_avg:125.29ms
step:664/1395 train_time:81944ms step_avg:125.30ms
step:665/1395 train_time:82073ms step_avg:125.30ms
step:666/1395 train_time:82202ms step_avg:125.31ms
step:667/1395 train_time:82331ms step_avg:125.31ms
step:668/1395 train_time:82459ms step_avg:125.32ms
step:669/1395 train_time:82587ms step_avg:125.32ms
step:670/1395 train_time:82716ms step_avg:125.33ms
step:671/1395 train_time:82844ms step_avg:125.33ms
step:672/1395 train_time:82972ms step_avg:125.33ms
step:673/1395 train_time:83101ms step_avg:125.34ms
step:674/1395 train_time:83231ms step_avg:125.35ms
step:675/1395 train_time:83360ms step_avg:125.35ms
step:676/1395 train_time:83488ms step_avg:125.36ms
step:677/1395 train_time:83616ms step_avg:125.36ms
step:678/1395 train_time:83745ms step_avg:125.37ms
step:679/1395 train_time:83872ms step_avg:125.37ms
step:680/1395 train_time:84002ms step_avg:125.38ms
step:681/1395 train_time:84131ms step_avg:125.38ms
step:682/1395 train_time:84260ms step_avg:125.39ms
step:683/1395 train_time:84389ms step_avg:125.39ms
step:684/1395 train_time:84518ms step_avg:125.40ms
step:685/1395 train_time:84646ms step_avg:125.40ms
step:686/1395 train_time:84777ms step_avg:125.41ms
step:687/1395 train_time:84905ms step_avg:125.41ms
step:688/1395 train_time:85034ms step_avg:125.42ms
step:689/1395 train_time:85162ms step_avg:125.42ms
step:690/1395 train_time:85291ms step_avg:125.43ms
step:691/1395 train_time:85420ms step_avg:125.43ms
step:692/1395 train_time:85549ms step_avg:125.44ms
step:693/1395 train_time:85677ms step_avg:125.44ms
step:694/1395 train_time:85806ms step_avg:125.45ms
step:695/1395 train_time:85935ms step_avg:125.45ms
step:696/1395 train_time:86064ms step_avg:125.46ms
step:697/1395 train_time:86193ms step_avg:125.46ms
step:698/1395 train_time:86322ms step_avg:125.47ms
step:699/1395 train_time:86452ms step_avg:125.47ms
step:700/1395 train_time:86581ms step_avg:125.48ms
step:701/1395 train_time:86709ms step_avg:125.48ms
step:702/1395 train_time:86838ms step_avg:125.49ms
step:703/1395 train_time:86966ms step_avg:125.49ms
step:704/1395 train_time:87095ms step_avg:125.50ms
step:705/1395 train_time:87223ms step_avg:125.50ms
step:706/1395 train_time:87353ms step_avg:125.51ms
step:707/1395 train_time:87482ms step_avg:125.51ms
step:708/1395 train_time:87611ms step_avg:125.52ms
step:709/1395 train_time:87739ms step_avg:125.52ms
step:710/1395 train_time:87868ms step_avg:125.53ms
step:711/1395 train_time:87997ms step_avg:125.53ms
step:712/1395 train_time:88125ms step_avg:125.53ms
step:713/1395 train_time:88254ms step_avg:125.54ms
step:714/1395 train_time:88382ms step_avg:125.54ms
step:715/1395 train_time:88511ms step_avg:125.55ms
step:716/1395 train_time:88641ms step_avg:125.55ms
step:717/1395 train_time:88769ms step_avg:125.56ms
step:718/1395 train_time:88898ms step_avg:125.56ms
step:719/1395 train_time:89027ms step_avg:125.57ms
step:720/1395 train_time:89156ms step_avg:125.57ms
step:721/1395 train_time:89286ms step_avg:125.58ms
step:722/1395 train_time:89414ms step_avg:125.58ms
step:723/1395 train_time:89543ms step_avg:125.59ms
step:724/1395 train_time:89672ms step_avg:125.59ms
step:725/1395 train_time:89801ms step_avg:125.60ms
step:726/1395 train_time:89931ms step_avg:125.60ms
step:727/1395 train_time:90061ms step_avg:125.61ms
step:728/1395 train_time:90191ms step_avg:125.61ms
step:729/1395 train_time:90321ms step_avg:125.62ms
step:730/1395 train_time:90454ms step_avg:125.63ms
step:731/1395 train_time:90584ms step_avg:125.64ms
step:732/1395 train_time:90714ms step_avg:125.64ms
step:733/1395 train_time:90845ms step_avg:125.65ms
step:734/1395 train_time:90976ms step_avg:125.66ms
step:735/1395 train_time:91107ms step_avg:125.66ms
step:736/1395 train_time:91238ms step_avg:125.67ms
step:737/1395 train_time:91368ms step_avg:125.68ms
step:738/1395 train_time:91499ms step_avg:125.69ms
step:739/1395 train_time:91630ms step_avg:125.69ms
step:740/1395 train_time:91759ms step_avg:125.70ms
step:741/1395 train_time:91891ms step_avg:125.71ms
step:742/1395 train_time:92021ms step_avg:125.71ms
step:743/1395 train_time:92152ms step_avg:125.72ms
step:744/1395 train_time:92282ms step_avg:125.73ms
step:745/1395 train_time:92414ms step_avg:125.73ms
step:746/1395 train_time:92544ms step_avg:125.74ms
step:747/1395 train_time:92674ms step_avg:125.75ms
step:748/1395 train_time:92805ms step_avg:125.75ms
step:749/1395 train_time:92935ms step_avg:125.76ms
step:750/1395 train_time:93067ms step_avg:125.77ms
step:750/1395 val_loss:3.5271 train_time:93196ms step_avg:125.94ms
step:751/1395 train_time:93217ms step_avg:125.80ms
step:752/1395 train_time:93338ms step_avg:125.79ms
step:753/1395 train_time:93469ms step_avg:125.80ms
step:754/1395 train_time:93598ms step_avg:125.80ms
step:755/1395 train_time:93728ms step_avg:125.81ms
step:756/1395 train_time:93858ms step_avg:125.81ms
step:757/1395 train_time:93989ms step_avg:125.82ms
step:758/1395 train_time:94119ms step_avg:125.83ms
step:759/1395 train_time:94250ms step_avg:125.83ms
step:760/1395 train_time:94382ms step_avg:125.84ms
step:761/1395 train_time:94513ms step_avg:125.85ms
step:762/1395 train_time:94644ms step_avg:125.86ms
step:763/1395 train_time:94774ms step_avg:125.86ms
step:764/1395 train_time:94905ms step_avg:125.87ms
step:765/1395 train_time:95036ms step_avg:125.88ms
step:766/1395 train_time:95167ms step_avg:125.88ms
step:767/1395 train_time:95297ms step_avg:125.89ms
step:768/1395 train_time:95428ms step_avg:125.89ms
step:769/1395 train_time:95559ms step_avg:125.90ms
step:770/1395 train_time:95690ms step_avg:125.91ms
step:771/1395 train_time:95821ms step_avg:125.91ms
step:772/1395 train_time:95952ms step_avg:125.92ms
step:773/1395 train_time:96083ms step_avg:125.93ms
step:774/1395 train_time:96213ms step_avg:125.93ms
step:775/1395 train_time:96344ms step_avg:125.94ms
step:776/1395 train_time:96474ms step_avg:125.95ms
step:777/1395 train_time:96605ms step_avg:125.95ms
step:778/1395 train_time:96735ms step_avg:125.96ms
step:779/1395 train_time:96865ms step_avg:125.96ms
step:780/1395 train_time:96996ms step_avg:125.97ms
step:781/1395 train_time:97126ms step_avg:125.97ms
step:782/1395 train_time:97257ms step_avg:125.98ms
step:783/1395 train_time:97388ms step_avg:125.99ms
step:784/1395 train_time:97519ms step_avg:125.99ms
step:785/1395 train_time:97650ms step_avg:126.00ms
step:786/1395 train_time:97780ms step_avg:126.01ms
step:787/1395 train_time:97910ms step_avg:126.01ms
step:788/1395 train_time:98041ms step_avg:126.02ms
step:789/1395 train_time:98172ms step_avg:126.02ms
step:790/1395 train_time:98303ms step_avg:126.03ms
step:791/1395 train_time:98433ms step_avg:126.03ms
step:792/1395 train_time:98564ms step_avg:126.04ms
step:793/1395 train_time:98695ms step_avg:126.05ms
step:794/1395 train_time:98825ms step_avg:126.05ms
step:795/1395 train_time:98957ms step_avg:126.06ms
step:796/1395 train_time:99087ms step_avg:126.06ms
step:797/1395 train_time:99216ms step_avg:126.07ms
step:798/1395 train_time:99347ms step_avg:126.07ms
step:799/1395 train_time:99478ms step_avg:126.08ms
step:800/1395 train_time:99610ms step_avg:126.09ms
step:801/1395 train_time:99740ms step_avg:126.09ms
step:802/1395 train_time:99870ms step_avg:126.10ms
step:803/1395 train_time:100002ms step_avg:126.11ms
step:804/1395 train_time:100131ms step_avg:126.11ms
step:805/1395 train_time:100264ms step_avg:126.12ms
step:806/1395 train_time:100394ms step_avg:126.12ms
step:807/1395 train_time:100525ms step_avg:126.13ms
step:808/1395 train_time:100657ms step_avg:126.14ms
step:809/1395 train_time:100787ms step_avg:126.14ms
step:810/1395 train_time:100917ms step_avg:126.15ms
step:811/1395 train_time:101048ms step_avg:126.15ms
step:812/1395 train_time:101179ms step_avg:126.16ms
step:813/1395 train_time:101309ms step_avg:126.16ms
step:814/1395 train_time:101438ms step_avg:126.17ms
step:815/1395 train_time:101569ms step_avg:126.17ms
step:816/1395 train_time:101700ms step_avg:126.18ms
step:817/1395 train_time:101830ms step_avg:126.18ms
step:818/1395 train_time:101961ms step_avg:126.19ms
step:819/1395 train_time:102093ms step_avg:126.20ms
step:820/1395 train_time:102224ms step_avg:126.20ms
step:821/1395 train_time:102354ms step_avg:126.21ms
step:822/1395 train_time:102484ms step_avg:126.21ms
step:823/1395 train_time:102614ms step_avg:126.22ms
step:824/1395 train_time:102744ms step_avg:126.22ms
step:825/1395 train_time:102875ms step_avg:126.23ms
step:826/1395 train_time:103006ms step_avg:126.23ms
step:827/1395 train_time:103136ms step_avg:126.24ms
step:828/1395 train_time:103267ms step_avg:126.24ms
step:829/1395 train_time:103398ms step_avg:126.25ms
step:830/1395 train_time:103530ms step_avg:126.26ms
step:831/1395 train_time:103661ms step_avg:126.26ms
step:832/1395 train_time:103792ms step_avg:126.27ms
step:833/1395 train_time:103922ms step_avg:126.27ms
step:834/1395 train_time:104054ms step_avg:126.28ms
step:835/1395 train_time:104185ms step_avg:126.28ms
step:836/1395 train_time:104317ms step_avg:126.29ms
step:837/1395 train_time:104448ms step_avg:126.30ms
step:838/1395 train_time:104579ms step_avg:126.30ms
step:839/1395 train_time:104710ms step_avg:126.31ms
step:840/1395 train_time:104840ms step_avg:126.31ms
step:841/1395 train_time:104971ms step_avg:126.32ms
step:842/1395 train_time:105101ms step_avg:126.32ms
step:843/1395 train_time:105232ms step_avg:126.33ms
step:844/1395 train_time:105363ms step_avg:126.33ms
step:845/1395 train_time:105494ms step_avg:126.34ms
step:846/1395 train_time:105625ms step_avg:126.35ms
step:847/1395 train_time:105756ms step_avg:126.35ms
step:848/1395 train_time:105886ms step_avg:126.36ms
step:849/1395 train_time:106016ms step_avg:126.36ms
step:850/1395 train_time:106147ms step_avg:126.37ms
step:851/1395 train_time:106280ms step_avg:126.37ms
step:852/1395 train_time:106410ms step_avg:126.38ms
step:853/1395 train_time:106542ms step_avg:126.38ms
step:854/1395 train_time:106673ms step_avg:126.39ms
step:855/1395 train_time:106805ms step_avg:126.40ms
step:856/1395 train_time:106935ms step_avg:126.40ms
step:857/1395 train_time:107066ms step_avg:126.41ms
step:858/1395 train_time:107197ms step_avg:126.41ms
step:859/1395 train_time:107329ms step_avg:126.42ms
step:860/1395 train_time:107460ms step_avg:126.42ms
step:861/1395 train_time:107591ms step_avg:126.43ms
step:862/1395 train_time:107723ms step_avg:126.44ms
step:863/1395 train_time:107854ms step_avg:126.44ms
step:864/1395 train_time:107985ms step_avg:126.45ms
step:865/1395 train_time:108115ms step_avg:126.45ms
step:866/1395 train_time:108247ms step_avg:126.46ms
step:867/1395 train_time:108377ms step_avg:126.46ms
step:868/1395 train_time:108507ms step_avg:126.46ms
step:869/1395 train_time:108638ms step_avg:126.47ms
step:870/1395 train_time:108769ms step_avg:126.48ms
step:871/1395 train_time:108899ms step_avg:126.48ms
step:872/1395 train_time:109031ms step_avg:126.49ms
step:873/1395 train_time:109161ms step_avg:126.49ms
step:874/1395 train_time:109291ms step_avg:126.49ms
step:875/1395 train_time:109423ms step_avg:126.50ms
step:875/1395 val_loss:3.4769 train_time:109553ms step_avg:126.65ms
step:876/1395 train_time:109575ms step_avg:126.53ms
step:877/1395 train_time:109699ms step_avg:126.53ms
step:878/1395 train_time:109834ms step_avg:126.54ms
step:879/1395 train_time:109964ms step_avg:126.54ms
step:880/1395 train_time:110094ms step_avg:126.54ms
step:881/1395 train_time:110224ms step_avg:126.55ms
step:882/1395 train_time:110354ms step_avg:126.55ms
step:883/1395 train_time:110484ms step_avg:126.56ms
step:884/1395 train_time:110614ms step_avg:126.56ms
step:885/1395 train_time:110746ms step_avg:126.57ms
step:886/1395 train_time:110878ms step_avg:126.57ms
step:887/1395 train_time:111008ms step_avg:126.58ms
step:888/1395 train_time:111139ms step_avg:126.58ms
step:889/1395 train_time:111271ms step_avg:126.59ms
step:890/1395 train_time:111401ms step_avg:126.59ms
step:891/1395 train_time:111531ms step_avg:126.60ms
step:892/1395 train_time:111664ms step_avg:126.60ms
step:893/1395 train_time:111793ms step_avg:126.61ms
step:894/1395 train_time:111925ms step_avg:126.61ms
step:895/1395 train_time:112056ms step_avg:126.62ms
step:896/1395 train_time:112187ms step_avg:126.62ms
step:897/1395 train_time:112318ms step_avg:126.63ms
step:898/1395 train_time:112449ms step_avg:126.63ms
step:899/1395 train_time:112581ms step_avg:126.64ms
step:900/1395 train_time:112712ms step_avg:126.64ms
step:901/1395 train_time:112844ms step_avg:126.65ms
step:902/1395 train_time:112974ms step_avg:126.65ms
step:903/1395 train_time:113104ms step_avg:126.66ms
step:904/1395 train_time:113235ms step_avg:126.66ms
step:905/1395 train_time:113366ms step_avg:126.67ms
step:906/1395 train_time:113497ms step_avg:126.67ms
step:907/1395 train_time:113629ms step_avg:126.68ms
step:908/1395 train_time:113760ms step_avg:126.68ms
step:909/1395 train_time:113890ms step_avg:126.69ms
step:910/1395 train_time:114023ms step_avg:126.69ms
step:911/1395 train_time:114154ms step_avg:126.70ms
step:912/1395 train_time:114285ms step_avg:126.70ms
step:913/1395 train_time:114416ms step_avg:126.71ms
step:914/1395 train_time:114548ms step_avg:126.71ms
step:915/1395 train_time:114679ms step_avg:126.72ms
step:916/1395 train_time:114810ms step_avg:126.72ms
step:917/1395 train_time:114941ms step_avg:126.73ms
step:918/1395 train_time:115073ms step_avg:126.73ms
step:919/1395 train_time:115207ms step_avg:126.74ms
step:920/1395 train_time:115338ms step_avg:126.75ms
step:921/1395 train_time:115469ms step_avg:126.75ms
step:922/1395 train_time:115601ms step_avg:126.76ms
step:923/1395 train_time:115731ms step_avg:126.76ms
step:924/1395 train_time:115862ms step_avg:126.76ms
step:925/1395 train_time:115993ms step_avg:126.77ms
step:926/1395 train_time:116124ms step_avg:126.77ms
step:927/1395 train_time:116255ms step_avg:126.78ms
step:928/1395 train_time:116385ms step_avg:126.78ms
step:929/1395 train_time:116516ms step_avg:126.79ms
step:930/1395 train_time:116648ms step_avg:126.79ms
step:931/1395 train_time:116780ms step_avg:126.80ms
step:932/1395 train_time:116910ms step_avg:126.80ms
step:933/1395 train_time:117042ms step_avg:126.81ms
step:934/1395 train_time:117175ms step_avg:126.81ms
step:935/1395 train_time:117308ms step_avg:126.82ms
step:936/1395 train_time:117441ms step_avg:126.83ms
step:937/1395 train_time:117575ms step_avg:126.83ms
step:938/1395 train_time:117708ms step_avg:126.84ms
step:939/1395 train_time:117839ms step_avg:126.85ms
step:940/1395 train_time:117973ms step_avg:126.85ms
step:941/1395 train_time:118105ms step_avg:126.86ms
step:942/1395 train_time:118237ms step_avg:126.86ms
step:943/1395 train_time:118370ms step_avg:126.87ms
step:944/1395 train_time:118503ms step_avg:126.88ms
step:945/1395 train_time:118636ms step_avg:126.88ms
step:946/1395 train_time:118768ms step_avg:126.89ms
step:947/1395 train_time:118902ms step_avg:126.90ms
step:948/1395 train_time:119034ms step_avg:126.90ms
step:949/1395 train_time:119168ms step_avg:126.91ms
step:950/1395 train_time:119300ms step_avg:126.91ms
step:951/1395 train_time:119435ms step_avg:126.92ms
step:952/1395 train_time:119567ms step_avg:126.93ms
step:953/1395 train_time:119700ms step_avg:126.93ms
step:954/1395 train_time:119832ms step_avg:126.94ms
step:955/1395 train_time:119964ms step_avg:126.95ms
step:956/1395 train_time:120097ms step_avg:126.95ms
step:957/1395 train_time:120228ms step_avg:126.96ms
step:958/1395 train_time:120361ms step_avg:126.96ms
step:959/1395 train_time:120494ms step_avg:126.97ms
step:960/1395 train_time:120627ms step_avg:126.98ms
step:961/1395 train_time:120760ms step_avg:126.98ms
step:962/1395 train_time:120893ms step_avg:126.99ms
step:963/1395 train_time:121027ms step_avg:127.00ms
step:964/1395 train_time:121159ms step_avg:127.00ms
step:965/1395 train_time:121291ms step_avg:127.01ms
step:966/1395 train_time:121424ms step_avg:127.01ms
step:967/1395 train_time:121556ms step_avg:127.02ms
step:968/1395 train_time:121688ms step_avg:127.02ms
step:969/1395 train_time:121821ms step_avg:127.03ms
step:970/1395 train_time:121954ms step_avg:127.04ms
step:971/1395 train_time:122087ms step_avg:127.04ms
step:972/1395 train_time:122220ms step_avg:127.05ms
step:973/1395 train_time:122352ms step_avg:127.05ms
step:974/1395 train_time:122484ms step_avg:127.06ms
step:975/1395 train_time:122617ms step_avg:127.06ms
step:976/1395 train_time:122749ms step_avg:127.07ms
step:977/1395 train_time:122881ms step_avg:127.07ms
step:978/1395 train_time:123012ms step_avg:127.08ms
step:979/1395 train_time:123146ms step_avg:127.09ms
step:980/1395 train_time:123279ms step_avg:127.09ms
step:981/1395 train_time:123410ms step_avg:127.10ms
step:982/1395 train_time:123543ms step_avg:127.10ms
step:983/1395 train_time:123674ms step_avg:127.11ms
step:984/1395 train_time:123806ms step_avg:127.11ms
step:985/1395 train_time:123938ms step_avg:127.12ms
step:986/1395 train_time:124072ms step_avg:127.12ms
step:987/1395 train_time:124204ms step_avg:127.13ms
step:988/1395 train_time:124336ms step_avg:127.13ms
step:989/1395 train_time:124468ms step_avg:127.14ms
step:990/1395 train_time:124601ms step_avg:127.14ms
step:991/1395 train_time:124734ms step_avg:127.15ms
step:992/1395 train_time:124868ms step_avg:127.16ms
step:993/1395 train_time:125004ms step_avg:127.17ms
step:994/1395 train_time:125136ms step_avg:127.17ms
step:995/1395 train_time:125268ms step_avg:127.18ms
step:996/1395 train_time:125400ms step_avg:127.18ms
step:997/1395 train_time:125533ms step_avg:127.19ms
step:998/1395 train_time:125665ms step_avg:127.19ms
step:999/1395 train_time:125796ms step_avg:127.20ms
step:1000/1395 train_time:125929ms step_avg:127.20ms
step:1000/1395 val_loss:3.4145 train_time:126060ms step_avg:127.33ms
step:1001/1395 train_time:126081ms step_avg:127.23ms
step:1002/1395 train_time:126205ms step_avg:127.22ms
step:1003/1395 train_time:126338ms step_avg:127.23ms
step:1004/1395 train_time:126470ms step_avg:127.23ms
step:1005/1395 train_time:126603ms step_avg:127.24ms
step:1006/1395 train_time:126734ms step_avg:127.24ms
step:1007/1395 train_time:126867ms step_avg:127.25ms
step:1008/1395 train_time:126999ms step_avg:127.25ms
step:1009/1395 train_time:127134ms step_avg:127.26ms
step:1010/1395 train_time:127266ms step_avg:127.27ms
step:1011/1395 train_time:127400ms step_avg:127.27ms
step:1012/1395 train_time:127531ms step_avg:127.28ms
step:1013/1395 train_time:127664ms step_avg:127.28ms
step:1014/1395 train_time:127796ms step_avg:127.29ms
step:1015/1395 train_time:127930ms step_avg:127.29ms
step:1016/1395 train_time:128062ms step_avg:127.30ms
step:1017/1395 train_time:128196ms step_avg:127.30ms
step:1018/1395 train_time:128329ms step_avg:127.31ms
step:1019/1395 train_time:128461ms step_avg:127.32ms
step:1020/1395 train_time:128594ms step_avg:127.32ms
step:1021/1395 train_time:128726ms step_avg:127.33ms
step:1022/1395 train_time:128858ms step_avg:127.33ms
step:1023/1395 train_time:128990ms step_avg:127.33ms
step:1024/1395 train_time:129123ms step_avg:127.34ms
step:1025/1395 train_time:129256ms step_avg:127.35ms
step:1026/1395 train_time:129388ms step_avg:127.35ms
step:1027/1395 train_time:129523ms step_avg:127.36ms
step:1028/1395 train_time:129655ms step_avg:127.36ms
step:1029/1395 train_time:129788ms step_avg:127.37ms
step:1030/1395 train_time:129921ms step_avg:127.37ms
step:1031/1395 train_time:130053ms step_avg:127.38ms
step:1032/1395 train_time:130184ms step_avg:127.38ms
step:1033/1395 train_time:130317ms step_avg:127.39ms
step:1034/1395 train_time:130449ms step_avg:127.39ms
step:1035/1395 train_time:130582ms step_avg:127.40ms
step:1036/1395 train_time:130716ms step_avg:127.40ms
step:1037/1395 train_time:130848ms step_avg:127.41ms
step:1038/1395 train_time:130980ms step_avg:127.41ms
step:1039/1395 train_time:131112ms step_avg:127.42ms
step:1040/1395 train_time:131244ms step_avg:127.42ms
step:1041/1395 train_time:131377ms step_avg:127.43ms
step:1042/1395 train_time:131509ms step_avg:127.43ms
step:1043/1395 train_time:131642ms step_avg:127.44ms
step:1044/1395 train_time:131778ms step_avg:127.44ms
step:1045/1395 train_time:131911ms step_avg:127.45ms
step:1046/1395 train_time:132044ms step_avg:127.46ms
step:1047/1395 train_time:132176ms step_avg:127.46ms
step:1048/1395 train_time:132309ms step_avg:127.46ms
step:1049/1395 train_time:132442ms step_avg:127.47ms
step:1050/1395 train_time:132575ms step_avg:127.48ms
step:1051/1395 train_time:132710ms step_avg:127.48ms
step:1052/1395 train_time:132842ms step_avg:127.49ms
step:1053/1395 train_time:132974ms step_avg:127.49ms
step:1054/1395 train_time:133107ms step_avg:127.50ms
step:1055/1395 train_time:133242ms step_avg:127.50ms
step:1056/1395 train_time:133375ms step_avg:127.51ms
step:1057/1395 train_time:133507ms step_avg:127.51ms
step:1058/1395 train_time:133641ms step_avg:127.52ms
step:1059/1395 train_time:133774ms step_avg:127.53ms
step:1060/1395 train_time:133907ms step_avg:127.53ms
step:1061/1395 train_time:134039ms step_avg:127.53ms
step:1062/1395 train_time:134172ms step_avg:127.54ms
step:1063/1395 train_time:134305ms step_avg:127.55ms
step:1064/1395 train_time:134438ms step_avg:127.55ms
step:1065/1395 train_time:134569ms step_avg:127.55ms
step:1066/1395 train_time:134704ms step_avg:127.56ms
step:1067/1395 train_time:134837ms step_avg:127.57ms
step:1068/1395 train_time:134970ms step_avg:127.57ms
step:1069/1395 train_time:135106ms step_avg:127.58ms
step:1070/1395 train_time:135239ms step_avg:127.58ms
step:1071/1395 train_time:135374ms step_avg:127.59ms
step:1072/1395 train_time:135505ms step_avg:127.59ms
step:1073/1395 train_time:135638ms step_avg:127.60ms
step:1074/1395 train_time:135770ms step_avg:127.60ms
step:1075/1395 train_time:135903ms step_avg:127.61ms
step:1076/1395 train_time:136035ms step_avg:127.61ms
step:1077/1395 train_time:136167ms step_avg:127.62ms
step:1078/1395 train_time:136300ms step_avg:127.62ms
step:1079/1395 train_time:136438ms step_avg:127.63ms
step:1080/1395 train_time:136570ms step_avg:127.64ms
step:1081/1395 train_time:136703ms step_avg:127.64ms
step:1082/1395 train_time:136835ms step_avg:127.64ms
step:1083/1395 train_time:136968ms step_avg:127.65ms
step:1084/1395 train_time:137102ms step_avg:127.66ms
step:1085/1395 train_time:137234ms step_avg:127.66ms
step:1086/1395 train_time:137367ms step_avg:127.66ms
step:1087/1395 train_time:137500ms step_avg:127.67ms
step:1088/1395 train_time:137632ms step_avg:127.67ms
step:1089/1395 train_time:137765ms step_avg:127.68ms
step:1090/1395 train_time:137898ms step_avg:127.68ms
step:1091/1395 train_time:138032ms step_avg:127.69ms
step:1092/1395 train_time:138164ms step_avg:127.69ms
step:1093/1395 train_time:138298ms step_avg:127.70ms
step:1094/1395 train_time:138430ms step_avg:127.70ms
step:1095/1395 train_time:138563ms step_avg:127.71ms
step:1096/1395 train_time:138697ms step_avg:127.71ms
step:1097/1395 train_time:138829ms step_avg:127.72ms
step:1098/1395 train_time:138961ms step_avg:127.72ms
step:1099/1395 train_time:139095ms step_avg:127.73ms
step:1100/1395 train_time:139227ms step_avg:127.73ms
step:1101/1395 train_time:139359ms step_avg:127.74ms
step:1102/1395 train_time:139492ms step_avg:127.74ms
step:1103/1395 train_time:139626ms step_avg:127.75ms
step:1104/1395 train_time:139760ms step_avg:127.75ms
step:1105/1395 train_time:139893ms step_avg:127.76ms
step:1106/1395 train_time:140026ms step_avg:127.76ms
step:1107/1395 train_time:140159ms step_avg:127.77ms
step:1108/1395 train_time:140294ms step_avg:127.77ms
step:1109/1395 train_time:140426ms step_avg:127.78ms
step:1110/1395 train_time:140559ms step_avg:127.78ms
step:1111/1395 train_time:140692ms step_avg:127.79ms
step:1112/1395 train_time:140824ms step_avg:127.79ms
step:1113/1395 train_time:140957ms step_avg:127.79ms
step:1114/1395 train_time:141089ms step_avg:127.80ms
step:1115/1395 train_time:141222ms step_avg:127.80ms
step:1116/1395 train_time:141354ms step_avg:127.81ms
step:1117/1395 train_time:141489ms step_avg:127.81ms
step:1118/1395 train_time:141625ms step_avg:127.82ms
step:1119/1395 train_time:141757ms step_avg:127.82ms
step:1120/1395 train_time:141889ms step_avg:127.83ms
step:1121/1395 train_time:142021ms step_avg:127.83ms
step:1122/1395 train_time:142154ms step_avg:127.84ms
step:1123/1395 train_time:142286ms step_avg:127.84ms
step:1124/1395 train_time:142419ms step_avg:127.85ms
step:1125/1395 train_time:142552ms step_avg:127.85ms
step:1125/1395 val_loss:3.3640 train_time:142685ms step_avg:127.97ms
step:1126/1395 train_time:142706ms step_avg:127.87ms
step:1127/1395 train_time:142828ms step_avg:127.87ms
step:1128/1395 train_time:142964ms step_avg:127.87ms
step:1129/1395 train_time:143096ms step_avg:127.88ms
step:1130/1395 train_time:143228ms step_avg:127.88ms
step:1131/1395 train_time:143359ms step_avg:127.89ms
step:1132/1395 train_time:143492ms step_avg:127.89ms
step:1133/1395 train_time:143623ms step_avg:127.89ms
step:1134/1395 train_time:143761ms step_avg:127.90ms
step:1135/1395 train_time:143896ms step_avg:127.91ms
step:1136/1395 train_time:144031ms step_avg:127.91ms
step:1137/1395 train_time:144162ms step_avg:127.92ms
step:1138/1395 train_time:144296ms step_avg:127.92ms
step:1139/1395 train_time:144429ms step_avg:127.93ms
step:1140/1395 train_time:144563ms step_avg:127.93ms
step:1141/1395 train_time:144696ms step_avg:127.94ms
step:1142/1395 train_time:144832ms step_avg:127.94ms
step:1143/1395 train_time:144967ms step_avg:127.95ms
step:1144/1395 train_time:145102ms step_avg:127.96ms
step:1145/1395 train_time:145236ms step_avg:127.96ms
step:1146/1395 train_time:145371ms step_avg:127.97ms
step:1147/1395 train_time:145505ms step_avg:127.97ms
step:1148/1395 train_time:145638ms step_avg:127.98ms
step:1149/1395 train_time:145772ms step_avg:127.98ms
step:1150/1395 train_time:145906ms step_avg:127.99ms
step:1151/1395 train_time:146041ms step_avg:127.99ms
step:1152/1395 train_time:146174ms step_avg:128.00ms
step:1153/1395 train_time:146312ms step_avg:128.01ms
step:1154/1395 train_time:146445ms step_avg:128.01ms
step:1155/1395 train_time:146580ms step_avg:128.02ms
step:1156/1395 train_time:146716ms step_avg:128.02ms
step:1157/1395 train_time:146851ms step_avg:128.03ms
step:1158/1395 train_time:146985ms step_avg:128.04ms
step:1159/1395 train_time:147119ms step_avg:128.04ms
step:1160/1395 train_time:147254ms step_avg:128.05ms
step:1161/1395 train_time:147387ms step_avg:128.05ms
step:1162/1395 train_time:147521ms step_avg:128.06ms
step:1163/1395 train_time:147656ms step_avg:128.06ms
step:1164/1395 train_time:147792ms step_avg:128.07ms
step:1165/1395 train_time:147926ms step_avg:128.07ms
step:1166/1395 train_time:148058ms step_avg:128.08ms
step:1167/1395 train_time:148192ms step_avg:128.08ms
step:1168/1395 train_time:148326ms step_avg:128.09ms
step:1169/1395 train_time:148459ms step_avg:128.09ms
step:1170/1395 train_time:148593ms step_avg:128.10ms
step:1171/1395 train_time:148729ms step_avg:128.10ms
step:1172/1395 train_time:148865ms step_avg:128.11ms
step:1173/1395 train_time:148998ms step_avg:128.12ms
step:1174/1395 train_time:149137ms step_avg:128.12ms
step:1175/1395 train_time:149271ms step_avg:128.13ms
step:1176/1395 train_time:149406ms step_avg:128.14ms
step:1177/1395 train_time:149541ms step_avg:128.14ms
step:1178/1395 train_time:149675ms step_avg:128.15ms
step:1179/1395 train_time:149809ms step_avg:128.15ms
step:1180/1395 train_time:149946ms step_avg:128.16ms
step:1181/1395 train_time:150082ms step_avg:128.17ms
step:1182/1395 train_time:150216ms step_avg:128.17ms
step:1183/1395 train_time:150351ms step_avg:128.18ms
step:1184/1395 train_time:150485ms step_avg:128.18ms
step:1185/1395 train_time:150620ms step_avg:128.19ms
step:1186/1395 train_time:150755ms step_avg:128.19ms
step:1187/1395 train_time:150893ms step_avg:128.20ms
step:1188/1395 train_time:151026ms step_avg:128.21ms
step:1189/1395 train_time:151159ms step_avg:128.21ms
step:1190/1395 train_time:151293ms step_avg:128.21ms
step:1191/1395 train_time:151428ms step_avg:128.22ms
step:1192/1395 train_time:151561ms step_avg:128.22ms
step:1193/1395 train_time:151695ms step_avg:128.23ms
step:1194/1395 train_time:151830ms step_avg:128.23ms
step:1195/1395 train_time:151964ms step_avg:128.24ms
step:1196/1395 train_time:152098ms step_avg:128.24ms
step:1197/1395 train_time:152234ms step_avg:128.25ms
step:1198/1395 train_time:152369ms step_avg:128.26ms
step:1199/1395 train_time:152502ms step_avg:128.26ms
step:1200/1395 train_time:152636ms step_avg:128.27ms
step:1201/1395 train_time:152769ms step_avg:128.27ms
step:1202/1395 train_time:152907ms step_avg:128.28ms
step:1203/1395 train_time:153045ms step_avg:128.29ms
step:1204/1395 train_time:153179ms step_avg:128.29ms
step:1205/1395 train_time:153313ms step_avg:128.30ms
step:1206/1395 train_time:153448ms step_avg:128.30ms
step:1207/1395 train_time:153581ms step_avg:128.31ms
step:1208/1395 train_time:153716ms step_avg:128.31ms
step:1209/1395 train_time:153849ms step_avg:128.31ms
step:1210/1395 train_time:153986ms step_avg:128.32ms
step:1211/1395 train_time:154119ms step_avg:128.33ms
step:1212/1395 train_time:154253ms step_avg:128.33ms
step:1213/1395 train_time:154386ms step_avg:128.33ms
step:1214/1395 train_time:154521ms step_avg:128.34ms
step:1215/1395 train_time:154658ms step_avg:128.35ms
step:1216/1395 train_time:154791ms step_avg:128.35ms
step:1217/1395 train_time:154925ms step_avg:128.36ms
step:1218/1395 train_time:155059ms step_avg:128.36ms
step:1219/1395 train_time:155191ms step_avg:128.36ms
step:1220/1395 train_time:155325ms step_avg:128.37ms
step:1221/1395 train_time:155459ms step_avg:128.37ms
step:1222/1395 train_time:155594ms step_avg:128.38ms
step:1223/1395 train_time:155727ms step_avg:128.38ms
step:1224/1395 train_time:155863ms step_avg:128.39ms
step:1225/1395 train_time:155999ms step_avg:128.39ms
step:1226/1395 train_time:156133ms step_avg:128.40ms
step:1227/1395 train_time:156267ms step_avg:128.40ms
step:1228/1395 train_time:156402ms step_avg:128.41ms
step:1229/1395 train_time:156536ms step_avg:128.41ms
step:1230/1395 train_time:156672ms step_avg:128.42ms
step:1231/1395 train_time:156806ms step_avg:128.42ms
step:1232/1395 train_time:156942ms step_avg:128.43ms
step:1233/1395 train_time:157075ms step_avg:128.43ms
step:1234/1395 train_time:157209ms step_avg:128.44ms
step:1235/1395 train_time:157343ms step_avg:128.44ms
step:1236/1395 train_time:157478ms step_avg:128.45ms
step:1237/1395 train_time:157613ms step_avg:128.45ms
step:1238/1395 train_time:157750ms step_avg:128.46ms
step:1239/1395 train_time:157883ms step_avg:128.46ms
step:1240/1395 train_time:158018ms step_avg:128.47ms
step:1241/1395 train_time:158154ms step_avg:128.48ms
step:1242/1395 train_time:158288ms step_avg:128.48ms
step:1243/1395 train_time:158423ms step_avg:128.49ms
step:1244/1395 train_time:158558ms step_avg:128.49ms
step:1245/1395 train_time:158692ms step_avg:128.50ms
step:1246/1395 train_time:158825ms step_avg:128.50ms
step:1247/1395 train_time:158960ms step_avg:128.50ms
step:1248/1395 train_time:159093ms step_avg:128.51ms
step:1249/1395 train_time:159226ms step_avg:128.51ms
step:1250/1395 train_time:159362ms step_avg:128.52ms
step:1250/1395 val_loss:3.3168 train_time:159496ms step_avg:128.63ms
step:1251/1395 train_time:159516ms step_avg:128.54ms
step:1252/1395 train_time:159641ms step_avg:128.54ms
step:1253/1395 train_time:159777ms step_avg:128.54ms
step:1254/1395 train_time:159910ms step_avg:128.55ms
step:1255/1395 train_time:160049ms step_avg:128.55ms
step:1256/1395 train_time:160182ms step_avg:128.56ms
step:1257/1395 train_time:160315ms step_avg:128.56ms
step:1258/1395 train_time:160449ms step_avg:128.56ms
step:1259/1395 train_time:160585ms step_avg:128.57ms
step:1260/1395 train_time:160719ms step_avg:128.58ms
step:1261/1395 train_time:160854ms step_avg:128.58ms
step:1262/1395 train_time:160990ms step_avg:128.59ms
step:1263/1395 train_time:161125ms step_avg:128.59ms
step:1264/1395 train_time:161259ms step_avg:128.60ms
step:1265/1395 train_time:161394ms step_avg:128.60ms
step:1266/1395 train_time:161528ms step_avg:128.60ms
step:1267/1395 train_time:161662ms step_avg:128.61ms
step:1268/1395 train_time:161796ms step_avg:128.61ms
step:1269/1395 train_time:161932ms step_avg:128.62ms
step:1270/1395 train_time:162066ms step_avg:128.62ms
step:1271/1395 train_time:162202ms step_avg:128.63ms
step:1272/1395 train_time:162336ms step_avg:128.63ms
step:1273/1395 train_time:162469ms step_avg:128.64ms
step:1274/1395 train_time:162603ms step_avg:128.64ms
step:1275/1395 train_time:162738ms step_avg:128.65ms
step:1276/1395 train_time:162871ms step_avg:128.65ms
step:1277/1395 train_time:163005ms step_avg:128.65ms
step:1278/1395 train_time:163139ms step_avg:128.66ms
step:1279/1395 train_time:163273ms step_avg:128.66ms
step:1280/1395 train_time:163409ms step_avg:128.67ms
step:1281/1395 train_time:163543ms step_avg:128.67ms
step:1282/1395 train_time:163676ms step_avg:128.68ms
step:1283/1395 train_time:163810ms step_avg:128.68ms
step:1284/1395 train_time:163946ms step_avg:128.69ms
step:1285/1395 train_time:164080ms step_avg:128.69ms
step:1286/1395 train_time:164214ms step_avg:128.69ms
step:1287/1395 train_time:164349ms step_avg:128.70ms
step:1288/1395 train_time:164483ms step_avg:128.70ms
step:1289/1395 train_time:164618ms step_avg:128.71ms
step:1290/1395 train_time:164752ms step_avg:128.71ms
step:1291/1395 train_time:164888ms step_avg:128.72ms
step:1292/1395 train_time:165022ms step_avg:128.72ms
step:1293/1395 train_time:165158ms step_avg:128.73ms
step:1294/1395 train_time:165292ms step_avg:128.73ms
step:1295/1395 train_time:165427ms step_avg:128.74ms
step:1296/1395 train_time:165560ms step_avg:128.74ms
step:1297/1395 train_time:165696ms step_avg:128.75ms
step:1298/1395 train_time:165830ms step_avg:128.75ms
step:1299/1395 train_time:165964ms step_avg:128.75ms
step:1300/1395 train_time:166098ms step_avg:128.76ms
step:1301/1395 train_time:166231ms step_avg:128.76ms
step:1302/1395 train_time:166364ms step_avg:128.76ms
step:1303/1395 train_time:166499ms step_avg:128.77ms
step:1304/1395 train_time:166635ms step_avg:128.78ms
step:1305/1395 train_time:166770ms step_avg:128.78ms
step:1306/1395 train_time:166903ms step_avg:128.78ms
step:1307/1395 train_time:167039ms step_avg:128.79ms
step:1308/1395 train_time:167173ms step_avg:128.79ms
step:1309/1395 train_time:167309ms step_avg:128.80ms
step:1310/1395 train_time:167444ms step_avg:128.80ms
step:1311/1395 train_time:167578ms step_avg:128.81ms
step:1312/1395 train_time:167711ms step_avg:128.81ms
step:1313/1395 train_time:167846ms step_avg:128.81ms
step:1314/1395 train_time:167979ms step_avg:128.82ms
step:1315/1395 train_time:168113ms step_avg:128.82ms
step:1316/1395 train_time:168247ms step_avg:128.83ms
step:1317/1395 train_time:168380ms step_avg:128.83ms
step:1318/1395 train_time:168513ms step_avg:128.83ms
step:1319/1395 train_time:168649ms step_avg:128.84ms
step:1320/1395 train_time:168783ms step_avg:128.84ms
step:1321/1395 train_time:168918ms step_avg:128.85ms
step:1322/1395 train_time:169057ms step_avg:128.85ms
step:1323/1395 train_time:169191ms step_avg:128.86ms
step:1324/1395 train_time:169324ms step_avg:128.86ms
step:1325/1395 train_time:169459ms step_avg:128.87ms
step:1326/1395 train_time:169594ms step_avg:128.87ms
step:1327/1395 train_time:169728ms step_avg:128.88ms
step:1328/1395 train_time:169862ms step_avg:128.88ms
step:1329/1395 train_time:170001ms step_avg:128.89ms
step:1330/1395 train_time:170136ms step_avg:128.89ms
step:1331/1395 train_time:170274ms step_avg:128.90ms
step:1332/1395 train_time:170411ms step_avg:128.90ms
step:1333/1395 train_time:170546ms step_avg:128.91ms
step:1334/1395 train_time:170680ms step_avg:128.91ms
step:1335/1395 train_time:170813ms step_avg:128.92ms
step:1336/1395 train_time:170950ms step_avg:128.92ms
step:1337/1395 train_time:171084ms step_avg:128.93ms
step:1338/1395 train_time:171219ms step_avg:128.93ms
step:1339/1395 train_time:171354ms step_avg:128.93ms
step:1340/1395 train_time:171492ms step_avg:128.94ms
step:1341/1395 train_time:171625ms step_avg:128.94ms
step:1342/1395 train_time:171759ms step_avg:128.95ms
step:1343/1395 train_time:171894ms step_avg:128.95ms
step:1344/1395 train_time:172027ms step_avg:128.96ms
step:1345/1395 train_time:172162ms step_avg:128.96ms
step:1346/1395 train_time:172297ms step_avg:128.96ms
step:1347/1395 train_time:172433ms step_avg:128.97ms
step:1348/1395 train_time:172568ms step_avg:128.97ms
step:1349/1395 train_time:172704ms step_avg:128.98ms
step:1350/1395 train_time:172838ms step_avg:128.98ms
step:1351/1395 train_time:172973ms step_avg:128.99ms
step:1352/1395 train_time:173112ms step_avg:129.00ms
step:1353/1395 train_time:173248ms step_avg:129.00ms
step:1354/1395 train_time:173384ms step_avg:129.01ms
step:1355/1395 train_time:173518ms step_avg:129.01ms
step:1356/1395 train_time:173652ms step_avg:129.01ms
step:1357/1395 train_time:173788ms step_avg:129.02ms
step:1358/1395 train_time:173924ms step_avg:129.02ms
step:1359/1395 train_time:174060ms step_avg:129.03ms
step:1360/1395 train_time:174196ms step_avg:129.03ms
step:1361/1395 train_time:174332ms step_avg:129.04ms
step:1362/1395 train_time:174470ms step_avg:129.05ms
step:1363/1395 train_time:174607ms step_avg:129.05ms
step:1364/1395 train_time:174743ms step_avg:129.06ms
step:1365/1395 train_time:174877ms step_avg:129.06ms
step:1366/1395 train_time:175012ms step_avg:129.06ms
step:1367/1395 train_time:175150ms step_avg:129.07ms
step:1368/1395 train_time:175286ms step_avg:129.08ms
step:1369/1395 train_time:175423ms step_avg:129.08ms
step:1370/1395 train_time:175561ms step_avg:129.09ms
step:1371/1395 train_time:175697ms step_avg:129.09ms
step:1372/1395 train_time:175834ms step_avg:129.10ms
step:1373/1395 train_time:175969ms step_avg:129.10ms
step:1374/1395 train_time:176106ms step_avg:129.11ms
step:1375/1395 train_time:176240ms step_avg:129.11ms
step:1375/1395 val_loss:3.2822 train_time:176373ms step_avg:129.21ms
step:1376/1395 train_time:176394ms step_avg:129.13ms
step:1377/1395 train_time:176516ms step_avg:129.13ms
step:1378/1395 train_time:176650ms step_avg:129.13ms
step:1379/1395 train_time:176784ms step_avg:129.13ms
step:1380/1395 train_time:176921ms step_avg:129.14ms
step:1381/1395 train_time:177057ms step_avg:129.14ms
step:1382/1395 train_time:177192ms step_avg:129.15ms
step:1383/1395 train_time:177327ms step_avg:129.15ms
step:1384/1395 train_time:177464ms step_avg:129.16ms
step:1385/1395 train_time:177600ms step_avg:129.16ms
step:1386/1395 train_time:177735ms step_avg:129.17ms
step:1387/1395 train_time:177872ms step_avg:129.17ms
step:1388/1395 train_time:178007ms step_avg:129.18ms
step:1389/1395 train_time:178142ms step_avg:129.18ms
step:1390/1395 train_time:178277ms step_avg:129.19ms
step:1391/1395 train_time:178412ms step_avg:129.19ms
step:1392/1395 train_time:178548ms step_avg:129.20ms
step:1393/1395 train_time:178681ms step_avg:129.20ms
step:1394/1395 train_time:178817ms step_avg:129.20ms
step:1395/1395 train_time:178953ms step_avg:129.21ms
step:1395/1395 val_loss:3.2778 train_time:179088ms step_avg:129.31ms
peak memory allocated: 37653 MiB reserved: 39156 MiB
