import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True

# -----------------------------------------------------------------------------
# Custom operators

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

def lm_head_fp8(x: Tensor, w: Tensor) -> Tensor:
    _x = x.flatten(0, -2)
    out: Tensor = torch.ops.nanogpt.mm(_x, w, x_s=2.0, w_s=32.0, grad_s=2.0**29)[0]
    return out.reshape(*x.shape[:-1], -1)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev():
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int):
        super().__init__(in_features, out_features, bias=False)

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x):
        return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len=65536):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std
        self.qkv_w = nn.Parameter(torch.empty(3, dim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads) # dim // num_heads = head_dim
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # Set attention scale such that the minimum attainable attention entropy
        # (but not necessary the attention entropy itself) is close to 0. By @leloykun
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3*self.num_heads, -1).chunk(3, dim=-2)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, model_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(model_dim, num_heads, layer_idx) if layer_idx != 7 else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, ve, x0, block_mask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, num_embeddings: int, embedding_dim: int):
        super().__init__()
        self.embed = nn.ModuleList([nn.Embedding(num_embeddings, embedding_dim) for _ in range(3)])

    def forward(self, input_seq) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2], None, None, None, None, None, None, ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual learning
        self.value_embeds = ValueEmbedding(vocab_size, model_dim)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128))
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        assert input_seq.ndim == 1
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        docs = (input_seq == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        def create_doc_swc_block_masks(sliding_window_num_blocks: Tensor):
            kv_idx = block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & document_bm
            full_bm  = causal_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            def build_bm(sw_num_blocks: Tensor) -> BlockMask:
                return BlockMask.from_kv_blocks(
                    torch.clamp_max(kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                    kv_indices,
                    torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                    full_kv_indices,
                    BLOCK_SIZE=BLOCK_SIZE,
                    mask_mod=document_causal,
                )
            return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

        # Long-short SWA block masks by @leloykun & @YouJiacheng
        long_bm, short_bm = create_doc_swc_block_masks(sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = lm_head_fp8(x, self.lm_head.weight) if self.training else self.lm_head(x)
        # @Grad62304977 added tanh softcapping, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use cycle(files) if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    batch_size = 8*64*1024 # batch size in tokens
    num_iterations = 1393 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 64*1024 # FlexAttention sequence length
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
             if console:
                 print(s)
             print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_loader = distributed_data_generator(args.train_files, args.batch_size, rank, world_size)

model = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim >= 2]
embed_params = [model.embed.weight, *model.value_embeds.parameters()]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), fused=True, eps=1e-10)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(it: int):
    t = 1 - it / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_bs = world_size * args.seq_len
        assert args.val_tokens % val_bs == 0
        val_steps = args.val_tokens // val_bs
        val_loader = distributed_data_generator(args.val_files, val_bs, rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION BEGIN -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB"
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.7 (main, Jan 17 2025, 03:57:17) [GCC 13.2.0]
Running PyTorch 2.7.0.dev20250110+cu126 compiled for CUDA 12.6
Fri Jan 17 06:49:36 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.05             Driver Version: 550.127.05     CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   33C    P0            116W /  700W |    7746MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   37C    P0            123W /  700W |    3456MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   39C    P0            123W /  700W |    3456MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   33C    P0            120W /  700W |    3456MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   34C    P0            121W /  700W |    3456MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   40C    P0            124W /  700W |    3456MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   41C    P0            125W /  700W |    3456MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   36C    P0            120W /  700W |    3216MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1393 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1393 train_time:17675ms step_avg:nanms
step:2/1393 train_time:17960ms step_avg:nanms
step:3/1393 train_time:18080ms step_avg:nanms
step:4/1393 train_time:18200ms step_avg:nanms
step:5/1393 train_time:18321ms step_avg:nanms
step:6/1393 train_time:18443ms step_avg:nanms
step:7/1393 train_time:18565ms step_avg:nanms
step:8/1393 train_time:18687ms step_avg:nanms
step:9/1393 train_time:18808ms step_avg:nanms
step:10/1393 train_time:18931ms step_avg:nanms
step:11/1393 train_time:126ms step_avg:nanms
step:12/1393 train_time:249ms step_avg:nanms
step:13/1393 train_time:372ms step_avg:124.04ms
step:14/1393 train_time:495ms step_avg:123.78ms
step:15/1393 train_time:618ms step_avg:123.67ms
step:16/1393 train_time:741ms step_avg:123.53ms
step:17/1393 train_time:862ms step_avg:123.15ms
step:18/1393 train_time:985ms step_avg:123.13ms
step:19/1393 train_time:1108ms step_avg:123.12ms
step:20/1393 train_time:1231ms step_avg:123.05ms
step:21/1393 train_time:1355ms step_avg:123.19ms
step:22/1393 train_time:1480ms step_avg:123.30ms
step:23/1393 train_time:1604ms step_avg:123.38ms
step:24/1393 train_time:1727ms step_avg:123.38ms
step:25/1393 train_time:1849ms step_avg:123.25ms
step:26/1393 train_time:1971ms step_avg:123.20ms
step:27/1393 train_time:2096ms step_avg:123.28ms
step:28/1393 train_time:2220ms step_avg:123.31ms
step:29/1393 train_time:2343ms step_avg:123.30ms
step:30/1393 train_time:2467ms step_avg:123.33ms
step:31/1393 train_time:2590ms step_avg:123.32ms
step:32/1393 train_time:2713ms step_avg:123.32ms
step:33/1393 train_time:2836ms step_avg:123.29ms
step:34/1393 train_time:2958ms step_avg:123.26ms
step:35/1393 train_time:3082ms step_avg:123.27ms
step:36/1393 train_time:3206ms step_avg:123.31ms
step:37/1393 train_time:3329ms step_avg:123.29ms
step:38/1393 train_time:3451ms step_avg:123.25ms
step:39/1393 train_time:3574ms step_avg:123.23ms
step:40/1393 train_time:3697ms step_avg:123.25ms
step:41/1393 train_time:3821ms step_avg:123.25ms
step:42/1393 train_time:3942ms step_avg:123.20ms
step:43/1393 train_time:4064ms step_avg:123.16ms
step:44/1393 train_time:4188ms step_avg:123.19ms
step:45/1393 train_time:4310ms step_avg:123.15ms
step:46/1393 train_time:4435ms step_avg:123.19ms
step:47/1393 train_time:4558ms step_avg:123.19ms
step:48/1393 train_time:4682ms step_avg:123.20ms
step:49/1393 train_time:4804ms step_avg:123.17ms
step:50/1393 train_time:4927ms step_avg:123.17ms
step:51/1393 train_time:5050ms step_avg:123.17ms
step:52/1393 train_time:5173ms step_avg:123.16ms
step:53/1393 train_time:5297ms step_avg:123.19ms
step:54/1393 train_time:5419ms step_avg:123.16ms
step:55/1393 train_time:5542ms step_avg:123.17ms
step:56/1393 train_time:5664ms step_avg:123.14ms
step:57/1393 train_time:5788ms step_avg:123.15ms
step:58/1393 train_time:5912ms step_avg:123.17ms
step:59/1393 train_time:6034ms step_avg:123.14ms
step:60/1393 train_time:6158ms step_avg:123.15ms
step:61/1393 train_time:6283ms step_avg:123.20ms
step:62/1393 train_time:6406ms step_avg:123.19ms
step:63/1393 train_time:6527ms step_avg:123.16ms
step:64/1393 train_time:6651ms step_avg:123.16ms
step:65/1393 train_time:6773ms step_avg:123.15ms
step:66/1393 train_time:6896ms step_avg:123.15ms
step:67/1393 train_time:7020ms step_avg:123.16ms
step:68/1393 train_time:7142ms step_avg:123.14ms
step:69/1393 train_time:7265ms step_avg:123.14ms
step:70/1393 train_time:7389ms step_avg:123.15ms
step:71/1393 train_time:7512ms step_avg:123.15ms
step:72/1393 train_time:7635ms step_avg:123.14ms
step:73/1393 train_time:7758ms step_avg:123.14ms
step:74/1393 train_time:7880ms step_avg:123.12ms
step:75/1393 train_time:8005ms step_avg:123.15ms
step:76/1393 train_time:8127ms step_avg:123.13ms
step:77/1393 train_time:8249ms step_avg:123.12ms
step:78/1393 train_time:8373ms step_avg:123.13ms
step:79/1393 train_time:8497ms step_avg:123.15ms
step:80/1393 train_time:8620ms step_avg:123.15ms
step:81/1393 train_time:8743ms step_avg:123.14ms
step:82/1393 train_time:8867ms step_avg:123.15ms
step:83/1393 train_time:8990ms step_avg:123.15ms
step:84/1393 train_time:9113ms step_avg:123.15ms
step:85/1393 train_time:9236ms step_avg:123.15ms
step:86/1393 train_time:9360ms step_avg:123.15ms
step:87/1393 train_time:9483ms step_avg:123.16ms
step:88/1393 train_time:9607ms step_avg:123.17ms
step:89/1393 train_time:9729ms step_avg:123.15ms
step:90/1393 train_time:9852ms step_avg:123.14ms
step:91/1393 train_time:9973ms step_avg:123.12ms
step:92/1393 train_time:10097ms step_avg:123.13ms
step:93/1393 train_time:10219ms step_avg:123.12ms
step:94/1393 train_time:10342ms step_avg:123.12ms
step:95/1393 train_time:10465ms step_avg:123.12ms
step:96/1393 train_time:10588ms step_avg:123.12ms
step:97/1393 train_time:10711ms step_avg:123.12ms
step:98/1393 train_time:10834ms step_avg:123.12ms
step:99/1393 train_time:10957ms step_avg:123.11ms
step:100/1393 train_time:11080ms step_avg:123.11ms
step:101/1393 train_time:11203ms step_avg:123.11ms
step:102/1393 train_time:11325ms step_avg:123.10ms
step:103/1393 train_time:11448ms step_avg:123.09ms
step:104/1393 train_time:11571ms step_avg:123.10ms
step:105/1393 train_time:11696ms step_avg:123.11ms
step:106/1393 train_time:11819ms step_avg:123.11ms
step:107/1393 train_time:11942ms step_avg:123.11ms
step:108/1393 train_time:12065ms step_avg:123.11ms
step:109/1393 train_time:12189ms step_avg:123.12ms
step:110/1393 train_time:12311ms step_avg:123.11ms
step:111/1393 train_time:12435ms step_avg:123.12ms
step:112/1393 train_time:12558ms step_avg:123.12ms
step:113/1393 train_time:12682ms step_avg:123.12ms
step:114/1393 train_time:12806ms step_avg:123.14ms
step:115/1393 train_time:12928ms step_avg:123.13ms
step:116/1393 train_time:13052ms step_avg:123.13ms
step:117/1393 train_time:13175ms step_avg:123.13ms
step:118/1393 train_time:13299ms step_avg:123.14ms
step:119/1393 train_time:13422ms step_avg:123.13ms
step:120/1393 train_time:13546ms step_avg:123.14ms
step:121/1393 train_time:13670ms step_avg:123.16ms
step:122/1393 train_time:13794ms step_avg:123.16ms
step:123/1393 train_time:13919ms step_avg:123.18ms
step:124/1393 train_time:14041ms step_avg:123.17ms
step:125/1393 train_time:14164ms step_avg:123.17ms
step:125/1393 val_loss:4.3784 train_time:14287ms step_avg:124.23ms
step:126/1393 train_time:14307ms step_avg:123.33ms
step:127/1393 train_time:14429ms step_avg:123.33ms
step:128/1393 train_time:14554ms step_avg:123.34ms
step:129/1393 train_time:14678ms step_avg:123.34ms
step:130/1393 train_time:14802ms step_avg:123.35ms
step:131/1393 train_time:14924ms step_avg:123.34ms
step:132/1393 train_time:15047ms step_avg:123.34ms
step:133/1393 train_time:15170ms step_avg:123.33ms
step:134/1393 train_time:15292ms step_avg:123.33ms
step:135/1393 train_time:15416ms step_avg:123.33ms
step:136/1393 train_time:15542ms step_avg:123.35ms
step:137/1393 train_time:15666ms step_avg:123.35ms
step:138/1393 train_time:15789ms step_avg:123.35ms
step:139/1393 train_time:15913ms step_avg:123.35ms
step:140/1393 train_time:16034ms step_avg:123.34ms
step:141/1393 train_time:16157ms step_avg:123.33ms
step:142/1393 train_time:16281ms step_avg:123.34ms
step:143/1393 train_time:16405ms step_avg:123.35ms
step:144/1393 train_time:16529ms step_avg:123.35ms
step:145/1393 train_time:16654ms step_avg:123.36ms
step:146/1393 train_time:16778ms step_avg:123.37ms
step:147/1393 train_time:16902ms step_avg:123.37ms
step:148/1393 train_time:17024ms step_avg:123.36ms
step:149/1393 train_time:17148ms step_avg:123.37ms
step:150/1393 train_time:17273ms step_avg:123.38ms
step:151/1393 train_time:17397ms step_avg:123.39ms
step:152/1393 train_time:17523ms step_avg:123.40ms
step:153/1393 train_time:17647ms step_avg:123.41ms
step:154/1393 train_time:17770ms step_avg:123.41ms
step:155/1393 train_time:17893ms step_avg:123.40ms
step:156/1393 train_time:18016ms step_avg:123.40ms
step:157/1393 train_time:18140ms step_avg:123.40ms
step:158/1393 train_time:18265ms step_avg:123.41ms
step:159/1393 train_time:18390ms step_avg:123.42ms
step:160/1393 train_time:18513ms step_avg:123.42ms
step:161/1393 train_time:18637ms step_avg:123.42ms
step:162/1393 train_time:18760ms step_avg:123.42ms
step:163/1393 train_time:18884ms step_avg:123.42ms
step:164/1393 train_time:19007ms step_avg:123.42ms
step:165/1393 train_time:19129ms step_avg:123.42ms
step:166/1393 train_time:19253ms step_avg:123.41ms
step:167/1393 train_time:19376ms step_avg:123.41ms
step:168/1393 train_time:19500ms step_avg:123.42ms
step:169/1393 train_time:19622ms step_avg:123.41ms
step:170/1393 train_time:19745ms step_avg:123.41ms
step:171/1393 train_time:19868ms step_avg:123.41ms
step:172/1393 train_time:19992ms step_avg:123.41ms
step:173/1393 train_time:20114ms step_avg:123.40ms
step:174/1393 train_time:20240ms step_avg:123.41ms
step:175/1393 train_time:20362ms step_avg:123.41ms
step:176/1393 train_time:20485ms step_avg:123.40ms
step:177/1393 train_time:20607ms step_avg:123.40ms
step:178/1393 train_time:20730ms step_avg:123.40ms
step:179/1393 train_time:20854ms step_avg:123.40ms
step:180/1393 train_time:20978ms step_avg:123.40ms
step:181/1393 train_time:21102ms step_avg:123.40ms
step:182/1393 train_time:21225ms step_avg:123.40ms
step:183/1393 train_time:21348ms step_avg:123.40ms
step:184/1393 train_time:21473ms step_avg:123.41ms
step:185/1393 train_time:21596ms step_avg:123.40ms
step:186/1393 train_time:21720ms step_avg:123.41ms
step:187/1393 train_time:21844ms step_avg:123.41ms
step:188/1393 train_time:21968ms step_avg:123.41ms
step:189/1393 train_time:22092ms step_avg:123.42ms
step:190/1393 train_time:22216ms step_avg:123.42ms
step:191/1393 train_time:22340ms step_avg:123.43ms
step:192/1393 train_time:22464ms step_avg:123.43ms
step:193/1393 train_time:22588ms step_avg:123.43ms
step:194/1393 train_time:22711ms step_avg:123.43ms
step:195/1393 train_time:22833ms step_avg:123.42ms
step:196/1393 train_time:22957ms step_avg:123.43ms
step:197/1393 train_time:23080ms step_avg:123.42ms
step:198/1393 train_time:23203ms step_avg:123.42ms
step:199/1393 train_time:23326ms step_avg:123.42ms
step:200/1393 train_time:23448ms step_avg:123.41ms
step:201/1393 train_time:23572ms step_avg:123.42ms
step:202/1393 train_time:23696ms step_avg:123.42ms
step:203/1393 train_time:23819ms step_avg:123.41ms
step:204/1393 train_time:23941ms step_avg:123.41ms
step:205/1393 train_time:24065ms step_avg:123.41ms
step:206/1393 train_time:24188ms step_avg:123.41ms
step:207/1393 train_time:24313ms step_avg:123.42ms
step:208/1393 train_time:24436ms step_avg:123.42ms
step:209/1393 train_time:24561ms step_avg:123.42ms
step:210/1393 train_time:24684ms step_avg:123.42ms
step:211/1393 train_time:24807ms step_avg:123.42ms
step:212/1393 train_time:24931ms step_avg:123.42ms
step:213/1393 train_time:25055ms step_avg:123.42ms
step:214/1393 train_time:25179ms step_avg:123.43ms
step:215/1393 train_time:25303ms step_avg:123.43ms
step:216/1393 train_time:25427ms step_avg:123.43ms
step:217/1393 train_time:25550ms step_avg:123.43ms
step:218/1393 train_time:25674ms step_avg:123.43ms
step:219/1393 train_time:25799ms step_avg:123.44ms
step:220/1393 train_time:25923ms step_avg:123.44ms
step:221/1393 train_time:26045ms step_avg:123.44ms
step:222/1393 train_time:26170ms step_avg:123.44ms
step:223/1393 train_time:26294ms step_avg:123.45ms
step:224/1393 train_time:26419ms step_avg:123.45ms
step:225/1393 train_time:26542ms step_avg:123.45ms
step:226/1393 train_time:26666ms step_avg:123.46ms
step:227/1393 train_time:26790ms step_avg:123.46ms
step:228/1393 train_time:26913ms step_avg:123.46ms
step:229/1393 train_time:27038ms step_avg:123.46ms
step:230/1393 train_time:27161ms step_avg:123.46ms
step:231/1393 train_time:27285ms step_avg:123.46ms
step:232/1393 train_time:27409ms step_avg:123.46ms
step:233/1393 train_time:27532ms step_avg:123.46ms
step:234/1393 train_time:27656ms step_avg:123.46ms
step:235/1393 train_time:27781ms step_avg:123.47ms
step:236/1393 train_time:27904ms step_avg:123.47ms
step:237/1393 train_time:28027ms step_avg:123.47ms
step:238/1393 train_time:28152ms step_avg:123.47ms
step:239/1393 train_time:28275ms step_avg:123.47ms
step:240/1393 train_time:28399ms step_avg:123.48ms
step:241/1393 train_time:28523ms step_avg:123.48ms
step:242/1393 train_time:28646ms step_avg:123.47ms
step:243/1393 train_time:28773ms step_avg:123.49ms
step:244/1393 train_time:28898ms step_avg:123.50ms
step:245/1393 train_time:29021ms step_avg:123.49ms
step:246/1393 train_time:29144ms step_avg:123.49ms
step:247/1393 train_time:29269ms step_avg:123.50ms
step:248/1393 train_time:29392ms step_avg:123.50ms
step:249/1393 train_time:29516ms step_avg:123.50ms
step:250/1393 train_time:29639ms step_avg:123.50ms
step:250/1393 val_loss:3.9773 train_time:29762ms step_avg:124.01ms
step:251/1393 train_time:29783ms step_avg:123.58ms
step:252/1393 train_time:29902ms step_avg:123.56ms
step:253/1393 train_time:30029ms step_avg:123.58ms
step:254/1393 train_time:30152ms step_avg:123.57ms
step:255/1393 train_time:30275ms step_avg:123.57ms
step:256/1393 train_time:30398ms step_avg:123.57ms
step:257/1393 train_time:30521ms step_avg:123.57ms
step:258/1393 train_time:30644ms step_avg:123.57ms
step:259/1393 train_time:30767ms step_avg:123.56ms
step:260/1393 train_time:30892ms step_avg:123.57ms
step:261/1393 train_time:31017ms step_avg:123.58ms
step:262/1393 train_time:31141ms step_avg:123.58ms
step:263/1393 train_time:31266ms step_avg:123.58ms
step:264/1393 train_time:31389ms step_avg:123.58ms
step:265/1393 train_time:31513ms step_avg:123.58ms
step:266/1393 train_time:31637ms step_avg:123.58ms
step:267/1393 train_time:31761ms step_avg:123.58ms
step:268/1393 train_time:31884ms step_avg:123.58ms
step:269/1393 train_time:32008ms step_avg:123.58ms
step:270/1393 train_time:32133ms step_avg:123.59ms
step:271/1393 train_time:32257ms step_avg:123.59ms
step:272/1393 train_time:32381ms step_avg:123.59ms
step:273/1393 train_time:32506ms step_avg:123.60ms
step:274/1393 train_time:32629ms step_avg:123.59ms
step:275/1393 train_time:32752ms step_avg:123.59ms
step:276/1393 train_time:32875ms step_avg:123.59ms
step:277/1393 train_time:32998ms step_avg:123.59ms
step:278/1393 train_time:33124ms step_avg:123.60ms
step:279/1393 train_time:33248ms step_avg:123.60ms
step:280/1393 train_time:33371ms step_avg:123.60ms
step:281/1393 train_time:33495ms step_avg:123.60ms
step:282/1393 train_time:33619ms step_avg:123.60ms
step:283/1393 train_time:33744ms step_avg:123.60ms
step:284/1393 train_time:33868ms step_avg:123.61ms
step:285/1393 train_time:33992ms step_avg:123.61ms
step:286/1393 train_time:34116ms step_avg:123.61ms
step:287/1393 train_time:34240ms step_avg:123.61ms
step:288/1393 train_time:34363ms step_avg:123.61ms
step:289/1393 train_time:34487ms step_avg:123.61ms
step:290/1393 train_time:34610ms step_avg:123.61ms
step:291/1393 train_time:34734ms step_avg:123.61ms
step:292/1393 train_time:34857ms step_avg:123.61ms
step:293/1393 train_time:34983ms step_avg:123.62ms
step:294/1393 train_time:35106ms step_avg:123.61ms
step:295/1393 train_time:35230ms step_avg:123.61ms
step:296/1393 train_time:35355ms step_avg:123.62ms
step:297/1393 train_time:35480ms step_avg:123.62ms
step:298/1393 train_time:35604ms step_avg:123.63ms
step:299/1393 train_time:35728ms step_avg:123.62ms
step:300/1393 train_time:35851ms step_avg:123.62ms
step:301/1393 train_time:35975ms step_avg:123.63ms
step:302/1393 train_time:36099ms step_avg:123.63ms
step:303/1393 train_time:36225ms step_avg:123.63ms
step:304/1393 train_time:36348ms step_avg:123.63ms
step:305/1393 train_time:36473ms step_avg:123.64ms
step:306/1393 train_time:36597ms step_avg:123.64ms
step:307/1393 train_time:36721ms step_avg:123.64ms
step:308/1393 train_time:36844ms step_avg:123.64ms
step:309/1393 train_time:36968ms step_avg:123.64ms
step:310/1393 train_time:37093ms step_avg:123.64ms
step:311/1393 train_time:37216ms step_avg:123.64ms
step:312/1393 train_time:37343ms step_avg:123.65ms
step:313/1393 train_time:37469ms step_avg:123.66ms
step:314/1393 train_time:37596ms step_avg:123.67ms
step:315/1393 train_time:37723ms step_avg:123.68ms
step:316/1393 train_time:37849ms step_avg:123.69ms
step:317/1393 train_time:37975ms step_avg:123.70ms
step:318/1393 train_time:38101ms step_avg:123.71ms
step:319/1393 train_time:38228ms step_avg:123.72ms
step:320/1393 train_time:38354ms step_avg:123.72ms
step:321/1393 train_time:38481ms step_avg:123.73ms
step:322/1393 train_time:38608ms step_avg:123.74ms
step:323/1393 train_time:38733ms step_avg:123.75ms
step:324/1393 train_time:38859ms step_avg:123.75ms
step:325/1393 train_time:38985ms step_avg:123.76ms
step:326/1393 train_time:39112ms step_avg:123.77ms
step:327/1393 train_time:39237ms step_avg:123.78ms
step:328/1393 train_time:39365ms step_avg:123.79ms
step:329/1393 train_time:39490ms step_avg:123.79ms
step:330/1393 train_time:39616ms step_avg:123.80ms
step:331/1393 train_time:39742ms step_avg:123.81ms
step:332/1393 train_time:39869ms step_avg:123.82ms
step:333/1393 train_time:39994ms step_avg:123.82ms
step:334/1393 train_time:40122ms step_avg:123.83ms
step:335/1393 train_time:40247ms step_avg:123.84ms
step:336/1393 train_time:40375ms step_avg:123.85ms
step:337/1393 train_time:40501ms step_avg:123.86ms
step:338/1393 train_time:40628ms step_avg:123.87ms
step:339/1393 train_time:40753ms step_avg:123.87ms
step:340/1393 train_time:40880ms step_avg:123.88ms
step:341/1393 train_time:41006ms step_avg:123.88ms
step:342/1393 train_time:41133ms step_avg:123.89ms
step:343/1393 train_time:41259ms step_avg:123.90ms
step:344/1393 train_time:41386ms step_avg:123.91ms
step:345/1393 train_time:41512ms step_avg:123.92ms
step:346/1393 train_time:41639ms step_avg:123.93ms
step:347/1393 train_time:41765ms step_avg:123.93ms
step:348/1393 train_time:41890ms step_avg:123.94ms
step:349/1393 train_time:42016ms step_avg:123.94ms
step:350/1393 train_time:42143ms step_avg:123.95ms
step:351/1393 train_time:42269ms step_avg:123.96ms
step:352/1393 train_time:42395ms step_avg:123.96ms
step:353/1393 train_time:42522ms step_avg:123.97ms
step:354/1393 train_time:42649ms step_avg:123.98ms
step:355/1393 train_time:42775ms step_avg:123.99ms
step:356/1393 train_time:42902ms step_avg:123.99ms
step:357/1393 train_time:43028ms step_avg:124.00ms
step:358/1393 train_time:43154ms step_avg:124.00ms
step:359/1393 train_time:43279ms step_avg:124.01ms
step:360/1393 train_time:43406ms step_avg:124.02ms
step:361/1393 train_time:43533ms step_avg:124.03ms
step:362/1393 train_time:43660ms step_avg:124.03ms
step:363/1393 train_time:43788ms step_avg:124.05ms
step:364/1393 train_time:43913ms step_avg:124.05ms
step:365/1393 train_time:44040ms step_avg:124.06ms
step:366/1393 train_time:44166ms step_avg:124.06ms
step:367/1393 train_time:44294ms step_avg:124.07ms
step:368/1393 train_time:44419ms step_avg:124.08ms
step:369/1393 train_time:44546ms step_avg:124.08ms
step:370/1393 train_time:44673ms step_avg:124.09ms
step:371/1393 train_time:44799ms step_avg:124.10ms
step:372/1393 train_time:44926ms step_avg:124.10ms
step:373/1393 train_time:45052ms step_avg:124.11ms
step:374/1393 train_time:45177ms step_avg:124.11ms
step:375/1393 train_time:45304ms step_avg:124.12ms
step:375/1393 val_loss:3.7848 train_time:45429ms step_avg:124.46ms
step:376/1393 train_time:45449ms step_avg:124.18ms
step:377/1393 train_time:45572ms step_avg:124.17ms
step:378/1393 train_time:45699ms step_avg:124.18ms
step:379/1393 train_time:45825ms step_avg:124.19ms
step:380/1393 train_time:45951ms step_avg:124.19ms
step:381/1393 train_time:46077ms step_avg:124.20ms
step:382/1393 train_time:46202ms step_avg:124.20ms
step:383/1393 train_time:46328ms step_avg:124.20ms
step:384/1393 train_time:46454ms step_avg:124.21ms
step:385/1393 train_time:46581ms step_avg:124.22ms
step:386/1393 train_time:46708ms step_avg:124.22ms
step:387/1393 train_time:46834ms step_avg:124.23ms
step:388/1393 train_time:46961ms step_avg:124.23ms
step:389/1393 train_time:47086ms step_avg:124.24ms
step:390/1393 train_time:47212ms step_avg:124.24ms
step:391/1393 train_time:47338ms step_avg:124.25ms
step:392/1393 train_time:47463ms step_avg:124.25ms
step:393/1393 train_time:47591ms step_avg:124.26ms
step:394/1393 train_time:47718ms step_avg:124.27ms
step:395/1393 train_time:47844ms step_avg:124.27ms
step:396/1393 train_time:47972ms step_avg:124.28ms
step:397/1393 train_time:48097ms step_avg:124.28ms
step:398/1393 train_time:48223ms step_avg:124.29ms
step:399/1393 train_time:48350ms step_avg:124.29ms
step:400/1393 train_time:48476ms step_avg:124.30ms
step:401/1393 train_time:48603ms step_avg:124.30ms
step:402/1393 train_time:48730ms step_avg:124.31ms
step:403/1393 train_time:48858ms step_avg:124.32ms
step:404/1393 train_time:48984ms step_avg:124.33ms
step:405/1393 train_time:49111ms step_avg:124.33ms
step:406/1393 train_time:49238ms step_avg:124.34ms
step:407/1393 train_time:49365ms step_avg:124.34ms
step:408/1393 train_time:49491ms step_avg:124.35ms
step:409/1393 train_time:49616ms step_avg:124.35ms
step:410/1393 train_time:49742ms step_avg:124.35ms
step:411/1393 train_time:49870ms step_avg:124.36ms
step:412/1393 train_time:49997ms step_avg:124.37ms
step:413/1393 train_time:50124ms step_avg:124.38ms
step:414/1393 train_time:50251ms step_avg:124.38ms
step:415/1393 train_time:50378ms step_avg:124.39ms
step:416/1393 train_time:50504ms step_avg:124.39ms
step:417/1393 train_time:50631ms step_avg:124.40ms
step:418/1393 train_time:50757ms step_avg:124.40ms
step:419/1393 train_time:50884ms step_avg:124.41ms
step:420/1393 train_time:51011ms step_avg:124.42ms
step:421/1393 train_time:51139ms step_avg:124.42ms
step:422/1393 train_time:51265ms step_avg:124.43ms
step:423/1393 train_time:51392ms step_avg:124.44ms
step:424/1393 train_time:51519ms step_avg:124.44ms
step:425/1393 train_time:51646ms step_avg:124.45ms
step:426/1393 train_time:51773ms step_avg:124.45ms
step:427/1393 train_time:51898ms step_avg:124.46ms
step:428/1393 train_time:52026ms step_avg:124.46ms
step:429/1393 train_time:52152ms step_avg:124.47ms
step:430/1393 train_time:52279ms step_avg:124.47ms
step:431/1393 train_time:52406ms step_avg:124.48ms
step:432/1393 train_time:52533ms step_avg:124.49ms
step:433/1393 train_time:52659ms step_avg:124.49ms
step:434/1393 train_time:52785ms step_avg:124.49ms
step:435/1393 train_time:52912ms step_avg:124.50ms
step:436/1393 train_time:53038ms step_avg:124.50ms
step:437/1393 train_time:53164ms step_avg:124.51ms
step:438/1393 train_time:53293ms step_avg:124.52ms
step:439/1393 train_time:53420ms step_avg:124.52ms
step:440/1393 train_time:53547ms step_avg:124.53ms
step:441/1393 train_time:53674ms step_avg:124.53ms
step:442/1393 train_time:53800ms step_avg:124.54ms
step:443/1393 train_time:53926ms step_avg:124.54ms
step:444/1393 train_time:54053ms step_avg:124.55ms
step:445/1393 train_time:54179ms step_avg:124.55ms
step:446/1393 train_time:54306ms step_avg:124.55ms
step:447/1393 train_time:54433ms step_avg:124.56ms
step:448/1393 train_time:54561ms step_avg:124.57ms
step:449/1393 train_time:54688ms step_avg:124.57ms
step:450/1393 train_time:54815ms step_avg:124.58ms
step:451/1393 train_time:54941ms step_avg:124.58ms
step:452/1393 train_time:55069ms step_avg:124.59ms
step:453/1393 train_time:55195ms step_avg:124.59ms
step:454/1393 train_time:55322ms step_avg:124.60ms
step:455/1393 train_time:55449ms step_avg:124.60ms
step:456/1393 train_time:55575ms step_avg:124.61ms
step:457/1393 train_time:55702ms step_avg:124.61ms
step:458/1393 train_time:55828ms step_avg:124.62ms
step:459/1393 train_time:55955ms step_avg:124.62ms
step:460/1393 train_time:56082ms step_avg:124.63ms
step:461/1393 train_time:56209ms step_avg:124.63ms
step:462/1393 train_time:56336ms step_avg:124.64ms
step:463/1393 train_time:56462ms step_avg:124.64ms
step:464/1393 train_time:56590ms step_avg:124.65ms
step:465/1393 train_time:56718ms step_avg:124.65ms
step:466/1393 train_time:56844ms step_avg:124.66ms
step:467/1393 train_time:56971ms step_avg:124.66ms
step:468/1393 train_time:57097ms step_avg:124.67ms
step:469/1393 train_time:57224ms step_avg:124.67ms
step:470/1393 train_time:57353ms step_avg:124.68ms
step:471/1393 train_time:57479ms step_avg:124.68ms
step:472/1393 train_time:57606ms step_avg:124.69ms
step:473/1393 train_time:57732ms step_avg:124.69ms
step:474/1393 train_time:57859ms step_avg:124.70ms
step:475/1393 train_time:57985ms step_avg:124.70ms
step:476/1393 train_time:58113ms step_avg:124.71ms
step:477/1393 train_time:58239ms step_avg:124.71ms
step:478/1393 train_time:58366ms step_avg:124.71ms
step:479/1393 train_time:58493ms step_avg:124.72ms
step:480/1393 train_time:58619ms step_avg:124.72ms
step:481/1393 train_time:58747ms step_avg:124.73ms
step:482/1393 train_time:58874ms step_avg:124.73ms
step:483/1393 train_time:59000ms step_avg:124.74ms
step:484/1393 train_time:59126ms step_avg:124.74ms
step:485/1393 train_time:59253ms step_avg:124.74ms
step:486/1393 train_time:59379ms step_avg:124.75ms
step:487/1393 train_time:59506ms step_avg:124.75ms
step:488/1393 train_time:59633ms step_avg:124.75ms
step:489/1393 train_time:59759ms step_avg:124.76ms
step:490/1393 train_time:59887ms step_avg:124.77ms
step:491/1393 train_time:60016ms step_avg:124.77ms
step:492/1393 train_time:60142ms step_avg:124.78ms
step:493/1393 train_time:60270ms step_avg:124.78ms
step:494/1393 train_time:60396ms step_avg:124.78ms
step:495/1393 train_time:60523ms step_avg:124.79ms
step:496/1393 train_time:60652ms step_avg:124.80ms
step:497/1393 train_time:60778ms step_avg:124.80ms
step:498/1393 train_time:60905ms step_avg:124.81ms
step:499/1393 train_time:61032ms step_avg:124.81ms
step:500/1393 train_time:61160ms step_avg:124.82ms
step:500/1393 val_loss:3.6651 train_time:61286ms step_avg:125.07ms
step:501/1393 train_time:61306ms step_avg:124.86ms
step:502/1393 train_time:61427ms step_avg:124.85ms
step:503/1393 train_time:61557ms step_avg:124.86ms
step:504/1393 train_time:61683ms step_avg:124.86ms
step:505/1393 train_time:61809ms step_avg:124.87ms
step:506/1393 train_time:61935ms step_avg:124.87ms
step:507/1393 train_time:62061ms step_avg:124.87ms
step:508/1393 train_time:62187ms step_avg:124.87ms
step:509/1393 train_time:62313ms step_avg:124.88ms
step:510/1393 train_time:62441ms step_avg:124.88ms
step:511/1393 train_time:62570ms step_avg:124.89ms
step:512/1393 train_time:62696ms step_avg:124.89ms
step:513/1393 train_time:62824ms step_avg:124.90ms
step:514/1393 train_time:62950ms step_avg:124.90ms
step:515/1393 train_time:63077ms step_avg:124.90ms
step:516/1393 train_time:63203ms step_avg:124.91ms
step:517/1393 train_time:63329ms step_avg:124.91ms
step:518/1393 train_time:63460ms step_avg:124.92ms
step:519/1393 train_time:63589ms step_avg:124.93ms
step:520/1393 train_time:63719ms step_avg:124.94ms
step:521/1393 train_time:63849ms step_avg:124.95ms
step:522/1393 train_time:63978ms step_avg:124.96ms
step:523/1393 train_time:64107ms step_avg:124.96ms
step:524/1393 train_time:64237ms step_avg:124.97ms
step:525/1393 train_time:64364ms step_avg:124.98ms
step:526/1393 train_time:64494ms step_avg:124.99ms
step:527/1393 train_time:64623ms step_avg:125.00ms
step:528/1393 train_time:64752ms step_avg:125.00ms
step:529/1393 train_time:64882ms step_avg:125.01ms
step:530/1393 train_time:65009ms step_avg:125.02ms
step:531/1393 train_time:65138ms step_avg:125.03ms
step:532/1393 train_time:65267ms step_avg:125.03ms
step:533/1393 train_time:65396ms step_avg:125.04ms
step:534/1393 train_time:65524ms step_avg:125.05ms
step:535/1393 train_time:65655ms step_avg:125.06ms
step:536/1393 train_time:65784ms step_avg:125.06ms
step:537/1393 train_time:65912ms step_avg:125.07ms
step:538/1393 train_time:66041ms step_avg:125.08ms
step:539/1393 train_time:66169ms step_avg:125.08ms
step:540/1393 train_time:66299ms step_avg:125.09ms
step:541/1393 train_time:66427ms step_avg:125.10ms
step:542/1393 train_time:66558ms step_avg:125.11ms
step:543/1393 train_time:66686ms step_avg:125.11ms
step:544/1393 train_time:66815ms step_avg:125.12ms
step:545/1393 train_time:66945ms step_avg:125.13ms
step:546/1393 train_time:67074ms step_avg:125.14ms
step:547/1393 train_time:67202ms step_avg:125.14ms
step:548/1393 train_time:67330ms step_avg:125.15ms
step:549/1393 train_time:67459ms step_avg:125.16ms
step:550/1393 train_time:67589ms step_avg:125.16ms
step:551/1393 train_time:67718ms step_avg:125.17ms
step:552/1393 train_time:67846ms step_avg:125.18ms
step:553/1393 train_time:67977ms step_avg:125.19ms
step:554/1393 train_time:68105ms step_avg:125.19ms
step:555/1393 train_time:68234ms step_avg:125.20ms
step:556/1393 train_time:68362ms step_avg:125.20ms
step:557/1393 train_time:68490ms step_avg:125.21ms
step:558/1393 train_time:68619ms step_avg:125.22ms
step:559/1393 train_time:68749ms step_avg:125.23ms
step:560/1393 train_time:68878ms step_avg:125.23ms
step:561/1393 train_time:69007ms step_avg:125.24ms
step:562/1393 train_time:69137ms step_avg:125.25ms
step:563/1393 train_time:69265ms step_avg:125.25ms
step:564/1393 train_time:69395ms step_avg:125.26ms
step:565/1393 train_time:69522ms step_avg:125.27ms
step:566/1393 train_time:69652ms step_avg:125.27ms
step:567/1393 train_time:69781ms step_avg:125.28ms
step:568/1393 train_time:69910ms step_avg:125.29ms
step:569/1393 train_time:70039ms step_avg:125.29ms
step:570/1393 train_time:70167ms step_avg:125.30ms
step:571/1393 train_time:70297ms step_avg:125.31ms
step:572/1393 train_time:70426ms step_avg:125.31ms
step:573/1393 train_time:70556ms step_avg:125.32ms
step:574/1393 train_time:70685ms step_avg:125.33ms
step:575/1393 train_time:70814ms step_avg:125.33ms
step:576/1393 train_time:70942ms step_avg:125.34ms
step:577/1393 train_time:71070ms step_avg:125.34ms
step:578/1393 train_time:71200ms step_avg:125.35ms
step:579/1393 train_time:71328ms step_avg:125.36ms
step:580/1393 train_time:71458ms step_avg:125.36ms
step:581/1393 train_time:71586ms step_avg:125.37ms
step:582/1393 train_time:71716ms step_avg:125.38ms
step:583/1393 train_time:71844ms step_avg:125.38ms
step:584/1393 train_time:71974ms step_avg:125.39ms
step:585/1393 train_time:72102ms step_avg:125.39ms
step:586/1393 train_time:72232ms step_avg:125.40ms
step:587/1393 train_time:72361ms step_avg:125.41ms
step:588/1393 train_time:72490ms step_avg:125.42ms
step:589/1393 train_time:72620ms step_avg:125.42ms
step:590/1393 train_time:72749ms step_avg:125.43ms
step:591/1393 train_time:72877ms step_avg:125.43ms
step:592/1393 train_time:73006ms step_avg:125.44ms
step:593/1393 train_time:73136ms step_avg:125.45ms
step:594/1393 train_time:73264ms step_avg:125.45ms
step:595/1393 train_time:73393ms step_avg:125.46ms
step:596/1393 train_time:73522ms step_avg:125.46ms
step:597/1393 train_time:73650ms step_avg:125.47ms
step:598/1393 train_time:73781ms step_avg:125.48ms
step:599/1393 train_time:73909ms step_avg:125.48ms
step:600/1393 train_time:74038ms step_avg:125.49ms
step:601/1393 train_time:74169ms step_avg:125.50ms
step:602/1393 train_time:74298ms step_avg:125.50ms
step:603/1393 train_time:74427ms step_avg:125.51ms
step:604/1393 train_time:74557ms step_avg:125.52ms
step:605/1393 train_time:74685ms step_avg:125.52ms
step:606/1393 train_time:74815ms step_avg:125.53ms
step:607/1393 train_time:74943ms step_avg:125.53ms
step:608/1393 train_time:75072ms step_avg:125.54ms
step:609/1393 train_time:75200ms step_avg:125.54ms
step:610/1393 train_time:75329ms step_avg:125.55ms
step:611/1393 train_time:75459ms step_avg:125.56ms
step:612/1393 train_time:75589ms step_avg:125.56ms
step:613/1393 train_time:75717ms step_avg:125.57ms
step:614/1393 train_time:75846ms step_avg:125.57ms
step:615/1393 train_time:75975ms step_avg:125.58ms
step:616/1393 train_time:76102ms step_avg:125.58ms
step:617/1393 train_time:76232ms step_avg:125.59ms
step:618/1393 train_time:76361ms step_avg:125.59ms
step:619/1393 train_time:76489ms step_avg:125.60ms
step:620/1393 train_time:76618ms step_avg:125.60ms
step:621/1393 train_time:76748ms step_avg:125.61ms
step:622/1393 train_time:76877ms step_avg:125.62ms
step:623/1393 train_time:77006ms step_avg:125.62ms
step:624/1393 train_time:77135ms step_avg:125.63ms
step:625/1393 train_time:77263ms step_avg:125.63ms
step:625/1393 val_loss:3.5830 train_time:77392ms step_avg:125.84ms
step:626/1393 train_time:77412ms step_avg:125.67ms
step:627/1393 train_time:77534ms step_avg:125.66ms
step:628/1393 train_time:77666ms step_avg:125.67ms
step:629/1393 train_time:77794ms step_avg:125.68ms
step:630/1393 train_time:77923ms step_avg:125.68ms
step:631/1393 train_time:78050ms step_avg:125.68ms
step:632/1393 train_time:78178ms step_avg:125.69ms
step:633/1393 train_time:78307ms step_avg:125.69ms
step:634/1393 train_time:78437ms step_avg:125.70ms
step:635/1393 train_time:78567ms step_avg:125.71ms
step:636/1393 train_time:78697ms step_avg:125.71ms
step:637/1393 train_time:78828ms step_avg:125.72ms
step:638/1393 train_time:78956ms step_avg:125.73ms
step:639/1393 train_time:79084ms step_avg:125.73ms
step:640/1393 train_time:79212ms step_avg:125.73ms
step:641/1393 train_time:79341ms step_avg:125.74ms
step:642/1393 train_time:79470ms step_avg:125.74ms
step:643/1393 train_time:79600ms step_avg:125.75ms
step:644/1393 train_time:79728ms step_avg:125.75ms
step:645/1393 train_time:79857ms step_avg:125.76ms
step:646/1393 train_time:79986ms step_avg:125.76ms
step:647/1393 train_time:80115ms step_avg:125.77ms
step:648/1393 train_time:80246ms step_avg:125.78ms
step:649/1393 train_time:80376ms step_avg:125.78ms
step:650/1393 train_time:80506ms step_avg:125.79ms
step:651/1393 train_time:80634ms step_avg:125.79ms
step:652/1393 train_time:80763ms step_avg:125.80ms
step:653/1393 train_time:80892ms step_avg:125.80ms
step:654/1393 train_time:81022ms step_avg:125.81ms
step:655/1393 train_time:81151ms step_avg:125.82ms
step:656/1393 train_time:81280ms step_avg:125.82ms
step:657/1393 train_time:81409ms step_avg:125.83ms
step:658/1393 train_time:81538ms step_avg:125.83ms
step:659/1393 train_time:81668ms step_avg:125.84ms
step:660/1393 train_time:81796ms step_avg:125.84ms
step:661/1393 train_time:81926ms step_avg:125.85ms
step:662/1393 train_time:82056ms step_avg:125.85ms
step:663/1393 train_time:82184ms step_avg:125.86ms
step:664/1393 train_time:82314ms step_avg:125.86ms
step:665/1393 train_time:82444ms step_avg:125.87ms
step:666/1393 train_time:82573ms step_avg:125.87ms
step:667/1393 train_time:82703ms step_avg:125.88ms
step:668/1393 train_time:82833ms step_avg:125.89ms
step:669/1393 train_time:82961ms step_avg:125.89ms
step:670/1393 train_time:83089ms step_avg:125.89ms
step:671/1393 train_time:83220ms step_avg:125.90ms
step:672/1393 train_time:83348ms step_avg:125.90ms
step:673/1393 train_time:83477ms step_avg:125.91ms
step:674/1393 train_time:83606ms step_avg:125.91ms
step:675/1393 train_time:83736ms step_avg:125.92ms
step:676/1393 train_time:83866ms step_avg:125.92ms
step:677/1393 train_time:83995ms step_avg:125.93ms
step:678/1393 train_time:84125ms step_avg:125.94ms
step:679/1393 train_time:84253ms step_avg:125.94ms
step:680/1393 train_time:84383ms step_avg:125.95ms
step:681/1393 train_time:84513ms step_avg:125.95ms
step:682/1393 train_time:84643ms step_avg:125.96ms
step:683/1393 train_time:84772ms step_avg:125.96ms
step:684/1393 train_time:84902ms step_avg:125.97ms
step:685/1393 train_time:85031ms step_avg:125.97ms
step:686/1393 train_time:85159ms step_avg:125.98ms
step:687/1393 train_time:85288ms step_avg:125.98ms
step:688/1393 train_time:85417ms step_avg:125.98ms
step:689/1393 train_time:85547ms step_avg:125.99ms
step:690/1393 train_time:85678ms step_avg:126.00ms
step:691/1393 train_time:85808ms step_avg:126.00ms
step:692/1393 train_time:85937ms step_avg:126.01ms
step:693/1393 train_time:86067ms step_avg:126.01ms
step:694/1393 train_time:86195ms step_avg:126.02ms
step:695/1393 train_time:86325ms step_avg:126.02ms
step:696/1393 train_time:86454ms step_avg:126.03ms
step:697/1393 train_time:86583ms step_avg:126.03ms
step:698/1393 train_time:86712ms step_avg:126.03ms
step:699/1393 train_time:86841ms step_avg:126.04ms
step:700/1393 train_time:86969ms step_avg:126.04ms
step:701/1393 train_time:87098ms step_avg:126.05ms
step:702/1393 train_time:87227ms step_avg:126.05ms
step:703/1393 train_time:87356ms step_avg:126.05ms
step:704/1393 train_time:87485ms step_avg:126.06ms
step:705/1393 train_time:87615ms step_avg:126.06ms
step:706/1393 train_time:87745ms step_avg:126.07ms
step:707/1393 train_time:87874ms step_avg:126.08ms
step:708/1393 train_time:88005ms step_avg:126.08ms
step:709/1393 train_time:88135ms step_avg:126.09ms
step:710/1393 train_time:88264ms step_avg:126.09ms
step:711/1393 train_time:88393ms step_avg:126.09ms
step:712/1393 train_time:88523ms step_avg:126.10ms
step:713/1393 train_time:88651ms step_avg:126.10ms
step:714/1393 train_time:88779ms step_avg:126.11ms
step:715/1393 train_time:88908ms step_avg:126.11ms
step:716/1393 train_time:89038ms step_avg:126.12ms
step:717/1393 train_time:89167ms step_avg:126.12ms
step:718/1393 train_time:89297ms step_avg:126.13ms
step:719/1393 train_time:89427ms step_avg:126.13ms
step:720/1393 train_time:89556ms step_avg:126.13ms
step:721/1393 train_time:89685ms step_avg:126.14ms
step:722/1393 train_time:89814ms step_avg:126.14ms
step:723/1393 train_time:89945ms step_avg:126.15ms
step:724/1393 train_time:90073ms step_avg:126.15ms
step:725/1393 train_time:90206ms step_avg:126.16ms
step:726/1393 train_time:90338ms step_avg:126.17ms
step:727/1393 train_time:90468ms step_avg:126.18ms
step:728/1393 train_time:90599ms step_avg:126.18ms
step:729/1393 train_time:90730ms step_avg:126.19ms
step:730/1393 train_time:90862ms step_avg:126.20ms
step:731/1393 train_time:90993ms step_avg:126.20ms
step:732/1393 train_time:91125ms step_avg:126.21ms
step:733/1393 train_time:91257ms step_avg:126.22ms
step:734/1393 train_time:91387ms step_avg:126.23ms
step:735/1393 train_time:91519ms step_avg:126.23ms
step:736/1393 train_time:91651ms step_avg:126.24ms
step:737/1393 train_time:91781ms step_avg:126.25ms
step:738/1393 train_time:91912ms step_avg:126.25ms
step:739/1393 train_time:92044ms step_avg:126.26ms
step:740/1393 train_time:92174ms step_avg:126.27ms
step:741/1393 train_time:92306ms step_avg:126.27ms
step:742/1393 train_time:92437ms step_avg:126.28ms
step:743/1393 train_time:92568ms step_avg:126.29ms
step:744/1393 train_time:92700ms step_avg:126.29ms
step:745/1393 train_time:92832ms step_avg:126.30ms
step:746/1393 train_time:92963ms step_avg:126.31ms
step:747/1393 train_time:93094ms step_avg:126.31ms
step:748/1393 train_time:93225ms step_avg:126.32ms
step:749/1393 train_time:93356ms step_avg:126.33ms
step:750/1393 train_time:93489ms step_avg:126.34ms
step:750/1393 val_loss:3.5296 train_time:93620ms step_avg:126.51ms
step:751/1393 train_time:93640ms step_avg:126.37ms
step:752/1393 train_time:93764ms step_avg:126.37ms
step:753/1393 train_time:93896ms step_avg:126.37ms
step:754/1393 train_time:94026ms step_avg:126.38ms
step:755/1393 train_time:94156ms step_avg:126.38ms
step:756/1393 train_time:94287ms step_avg:126.39ms
step:757/1393 train_time:94418ms step_avg:126.40ms
step:758/1393 train_time:94548ms step_avg:126.40ms
step:759/1393 train_time:94680ms step_avg:126.41ms
step:760/1393 train_time:94811ms step_avg:126.41ms
step:761/1393 train_time:94943ms step_avg:126.42ms
step:762/1393 train_time:95074ms step_avg:126.43ms
step:763/1393 train_time:95205ms step_avg:126.43ms
step:764/1393 train_time:95336ms step_avg:126.44ms
step:765/1393 train_time:95467ms step_avg:126.45ms
step:766/1393 train_time:95598ms step_avg:126.45ms
step:767/1393 train_time:95728ms step_avg:126.46ms
step:768/1393 train_time:95860ms step_avg:126.46ms
step:769/1393 train_time:95992ms step_avg:126.47ms
step:770/1393 train_time:96124ms step_avg:126.48ms
step:771/1393 train_time:96254ms step_avg:126.48ms
step:772/1393 train_time:96386ms step_avg:126.49ms
step:773/1393 train_time:96517ms step_avg:126.50ms
step:774/1393 train_time:96648ms step_avg:126.50ms
step:775/1393 train_time:96780ms step_avg:126.51ms
step:776/1393 train_time:96911ms step_avg:126.52ms
step:777/1393 train_time:97043ms step_avg:126.52ms
step:778/1393 train_time:97174ms step_avg:126.53ms
step:779/1393 train_time:97305ms step_avg:126.53ms
step:780/1393 train_time:97436ms step_avg:126.54ms
step:781/1393 train_time:97566ms step_avg:126.55ms
step:782/1393 train_time:97698ms step_avg:126.55ms
step:783/1393 train_time:97828ms step_avg:126.56ms
step:784/1393 train_time:97959ms step_avg:126.56ms
step:785/1393 train_time:98090ms step_avg:126.57ms
step:786/1393 train_time:98221ms step_avg:126.57ms
step:787/1393 train_time:98352ms step_avg:126.58ms
step:788/1393 train_time:98483ms step_avg:126.59ms
step:789/1393 train_time:98613ms step_avg:126.59ms
step:790/1393 train_time:98744ms step_avg:126.59ms
step:791/1393 train_time:98876ms step_avg:126.60ms
step:792/1393 train_time:99007ms step_avg:126.61ms
step:793/1393 train_time:99139ms step_avg:126.61ms
step:794/1393 train_time:99269ms step_avg:126.62ms
step:795/1393 train_time:99402ms step_avg:126.63ms
step:796/1393 train_time:99532ms step_avg:126.63ms
step:797/1393 train_time:99664ms step_avg:126.64ms
step:798/1393 train_time:99796ms step_avg:126.64ms
step:799/1393 train_time:99929ms step_avg:126.65ms
step:800/1393 train_time:100061ms step_avg:126.66ms
step:801/1393 train_time:100192ms step_avg:126.66ms
step:802/1393 train_time:100324ms step_avg:126.67ms
step:803/1393 train_time:100455ms step_avg:126.68ms
step:804/1393 train_time:100585ms step_avg:126.68ms
step:805/1393 train_time:100718ms step_avg:126.69ms
step:806/1393 train_time:100849ms step_avg:126.69ms
step:807/1393 train_time:100981ms step_avg:126.70ms
step:808/1393 train_time:101112ms step_avg:126.71ms
step:809/1393 train_time:101242ms step_avg:126.71ms
step:810/1393 train_time:101375ms step_avg:126.72ms
step:811/1393 train_time:101505ms step_avg:126.72ms
step:812/1393 train_time:101637ms step_avg:126.73ms
step:813/1393 train_time:101767ms step_avg:126.73ms
step:814/1393 train_time:101898ms step_avg:126.74ms
step:815/1393 train_time:102029ms step_avg:126.74ms
step:816/1393 train_time:102160ms step_avg:126.75ms
step:817/1393 train_time:102291ms step_avg:126.75ms
step:818/1393 train_time:102422ms step_avg:126.76ms
step:819/1393 train_time:102553ms step_avg:126.76ms
step:820/1393 train_time:102685ms step_avg:126.77ms
step:821/1393 train_time:102816ms step_avg:126.78ms
step:822/1393 train_time:102946ms step_avg:126.78ms
step:823/1393 train_time:103078ms step_avg:126.79ms
step:824/1393 train_time:103209ms step_avg:126.79ms
step:825/1393 train_time:103340ms step_avg:126.80ms
step:826/1393 train_time:103472ms step_avg:126.80ms
step:827/1393 train_time:103604ms step_avg:126.81ms
step:828/1393 train_time:103735ms step_avg:126.82ms
step:829/1393 train_time:103865ms step_avg:126.82ms
step:830/1393 train_time:103997ms step_avg:126.83ms
step:831/1393 train_time:104128ms step_avg:126.83ms
step:832/1393 train_time:104260ms step_avg:126.84ms
step:833/1393 train_time:104391ms step_avg:126.84ms
step:834/1393 train_time:104524ms step_avg:126.85ms
step:835/1393 train_time:104655ms step_avg:126.85ms
step:836/1393 train_time:104786ms step_avg:126.86ms
step:837/1393 train_time:104918ms step_avg:126.87ms
step:838/1393 train_time:105049ms step_avg:126.87ms
step:839/1393 train_time:105181ms step_avg:126.88ms
step:840/1393 train_time:105314ms step_avg:126.88ms
step:841/1393 train_time:105446ms step_avg:126.89ms
step:842/1393 train_time:105577ms step_avg:126.90ms
step:843/1393 train_time:105708ms step_avg:126.90ms
step:844/1393 train_time:105840ms step_avg:126.91ms
step:845/1393 train_time:105972ms step_avg:126.91ms
step:846/1393 train_time:106104ms step_avg:126.92ms
step:847/1393 train_time:106237ms step_avg:126.93ms
step:848/1393 train_time:106368ms step_avg:126.93ms
step:849/1393 train_time:106500ms step_avg:126.94ms
step:850/1393 train_time:106631ms step_avg:126.94ms
step:851/1393 train_time:106765ms step_avg:126.95ms
step:852/1393 train_time:106898ms step_avg:126.96ms
step:853/1393 train_time:107029ms step_avg:126.96ms
step:854/1393 train_time:107160ms step_avg:126.97ms
step:855/1393 train_time:107292ms step_avg:126.97ms
step:856/1393 train_time:107423ms step_avg:126.98ms
step:857/1393 train_time:107554ms step_avg:126.98ms
step:858/1393 train_time:107686ms step_avg:126.99ms
step:859/1393 train_time:107818ms step_avg:126.99ms
step:860/1393 train_time:107950ms step_avg:127.00ms
step:861/1393 train_time:108083ms step_avg:127.01ms
step:862/1393 train_time:108214ms step_avg:127.01ms
step:863/1393 train_time:108345ms step_avg:127.02ms
step:864/1393 train_time:108477ms step_avg:127.02ms
step:865/1393 train_time:108607ms step_avg:127.03ms
step:866/1393 train_time:108740ms step_avg:127.03ms
step:867/1393 train_time:108871ms step_avg:127.04ms
step:868/1393 train_time:109002ms step_avg:127.04ms
step:869/1393 train_time:109133ms step_avg:127.05ms
step:870/1393 train_time:109265ms step_avg:127.05ms
step:871/1393 train_time:109397ms step_avg:127.06ms
step:872/1393 train_time:109528ms step_avg:127.06ms
step:873/1393 train_time:109659ms step_avg:127.07ms
step:874/1393 train_time:109790ms step_avg:127.07ms
step:875/1393 train_time:109923ms step_avg:127.08ms
step:875/1393 val_loss:3.4784 train_time:110053ms step_avg:127.23ms
step:876/1393 train_time:110073ms step_avg:127.11ms
step:877/1393 train_time:110195ms step_avg:127.10ms
step:878/1393 train_time:110327ms step_avg:127.11ms
step:879/1393 train_time:110459ms step_avg:127.11ms
step:880/1393 train_time:110589ms step_avg:127.11ms
step:881/1393 train_time:110720ms step_avg:127.12ms
step:882/1393 train_time:110850ms step_avg:127.12ms
step:883/1393 train_time:110982ms step_avg:127.13ms
step:884/1393 train_time:111114ms step_avg:127.13ms
step:885/1393 train_time:111249ms step_avg:127.14ms
step:886/1393 train_time:111381ms step_avg:127.15ms
step:887/1393 train_time:111511ms step_avg:127.15ms
step:888/1393 train_time:111641ms step_avg:127.15ms
step:889/1393 train_time:111773ms step_avg:127.16ms
step:890/1393 train_time:111903ms step_avg:127.16ms
step:891/1393 train_time:112034ms step_avg:127.17ms
step:892/1393 train_time:112165ms step_avg:127.17ms
step:893/1393 train_time:112299ms step_avg:127.18ms
step:894/1393 train_time:112432ms step_avg:127.18ms
step:895/1393 train_time:112563ms step_avg:127.19ms
step:896/1393 train_time:112694ms step_avg:127.19ms
step:897/1393 train_time:112826ms step_avg:127.20ms
step:898/1393 train_time:112957ms step_avg:127.20ms
step:899/1393 train_time:113090ms step_avg:127.21ms
step:900/1393 train_time:113221ms step_avg:127.21ms
step:901/1393 train_time:113353ms step_avg:127.22ms
step:902/1393 train_time:113484ms step_avg:127.22ms
step:903/1393 train_time:113617ms step_avg:127.23ms
step:904/1393 train_time:113748ms step_avg:127.23ms
step:905/1393 train_time:113880ms step_avg:127.24ms
step:906/1393 train_time:114011ms step_avg:127.24ms
step:907/1393 train_time:114143ms step_avg:127.25ms
step:908/1393 train_time:114275ms step_avg:127.25ms
step:909/1393 train_time:114406ms step_avg:127.26ms
step:910/1393 train_time:114540ms step_avg:127.27ms
step:911/1393 train_time:114671ms step_avg:127.27ms
step:912/1393 train_time:114802ms step_avg:127.27ms
step:913/1393 train_time:114935ms step_avg:127.28ms
step:914/1393 train_time:115066ms step_avg:127.29ms
step:915/1393 train_time:115197ms step_avg:127.29ms
step:916/1393 train_time:115331ms step_avg:127.30ms
step:917/1393 train_time:115463ms step_avg:127.30ms
step:918/1393 train_time:115593ms step_avg:127.31ms
step:919/1393 train_time:115726ms step_avg:127.31ms
step:920/1393 train_time:115857ms step_avg:127.32ms
step:921/1393 train_time:115989ms step_avg:127.32ms
step:922/1393 train_time:116120ms step_avg:127.32ms
step:923/1393 train_time:116250ms step_avg:127.33ms
step:924/1393 train_time:116381ms step_avg:127.33ms
step:925/1393 train_time:116514ms step_avg:127.34ms
step:926/1393 train_time:116645ms step_avg:127.34ms
step:927/1393 train_time:116778ms step_avg:127.35ms
step:928/1393 train_time:116909ms step_avg:127.35ms
step:929/1393 train_time:117041ms step_avg:127.36ms
step:930/1393 train_time:117172ms step_avg:127.36ms
step:931/1393 train_time:117304ms step_avg:127.37ms
step:932/1393 train_time:117438ms step_avg:127.37ms
step:933/1393 train_time:117571ms step_avg:127.38ms
step:934/1393 train_time:117704ms step_avg:127.39ms
step:935/1393 train_time:117840ms step_avg:127.39ms
step:936/1393 train_time:117972ms step_avg:127.40ms
step:937/1393 train_time:118106ms step_avg:127.41ms
step:938/1393 train_time:118240ms step_avg:127.41ms
step:939/1393 train_time:118373ms step_avg:127.42ms
step:940/1393 train_time:118506ms step_avg:127.43ms
step:941/1393 train_time:118640ms step_avg:127.43ms
step:942/1393 train_time:118772ms step_avg:127.44ms
step:943/1393 train_time:118906ms step_avg:127.44ms
step:944/1393 train_time:119040ms step_avg:127.45ms
step:945/1393 train_time:119174ms step_avg:127.46ms
step:946/1393 train_time:119306ms step_avg:127.46ms
step:947/1393 train_time:119439ms step_avg:127.47ms
step:948/1393 train_time:119574ms step_avg:127.48ms
step:949/1393 train_time:119707ms step_avg:127.48ms
step:950/1393 train_time:119840ms step_avg:127.49ms
step:951/1393 train_time:119975ms step_avg:127.50ms
step:952/1393 train_time:120108ms step_avg:127.50ms
step:953/1393 train_time:120241ms step_avg:127.51ms
step:954/1393 train_time:120374ms step_avg:127.52ms
step:955/1393 train_time:120506ms step_avg:127.52ms
step:956/1393 train_time:120640ms step_avg:127.53ms
step:957/1393 train_time:120774ms step_avg:127.53ms
step:958/1393 train_time:120907ms step_avg:127.54ms
step:959/1393 train_time:121040ms step_avg:127.54ms
step:960/1393 train_time:121176ms step_avg:127.55ms
step:961/1393 train_time:121308ms step_avg:127.56ms
step:962/1393 train_time:121440ms step_avg:127.56ms
step:963/1393 train_time:121575ms step_avg:127.57ms
step:964/1393 train_time:121706ms step_avg:127.57ms
step:965/1393 train_time:121839ms step_avg:127.58ms
step:966/1393 train_time:121972ms step_avg:127.59ms
step:967/1393 train_time:122105ms step_avg:127.59ms
step:968/1393 train_time:122239ms step_avg:127.60ms
step:969/1393 train_time:122373ms step_avg:127.61ms
step:970/1393 train_time:122506ms step_avg:127.61ms
step:971/1393 train_time:122639ms step_avg:127.62ms
step:972/1393 train_time:122771ms step_avg:127.62ms
step:973/1393 train_time:122903ms step_avg:127.62ms
step:974/1393 train_time:123037ms step_avg:127.63ms
step:975/1393 train_time:123171ms step_avg:127.64ms
step:976/1393 train_time:123303ms step_avg:127.64ms
step:977/1393 train_time:123437ms step_avg:127.65ms
step:978/1393 train_time:123570ms step_avg:127.65ms
step:979/1393 train_time:123701ms step_avg:127.66ms
step:980/1393 train_time:123835ms step_avg:127.67ms
step:981/1393 train_time:123968ms step_avg:127.67ms
step:982/1393 train_time:124101ms step_avg:127.68ms
step:983/1393 train_time:124235ms step_avg:127.68ms
step:984/1393 train_time:124367ms step_avg:127.69ms
step:985/1393 train_time:124500ms step_avg:127.69ms
step:986/1393 train_time:124635ms step_avg:127.70ms
step:987/1393 train_time:124768ms step_avg:127.70ms
step:988/1393 train_time:124901ms step_avg:127.71ms
step:989/1393 train_time:125035ms step_avg:127.72ms
step:990/1393 train_time:125168ms step_avg:127.72ms
step:991/1393 train_time:125302ms step_avg:127.73ms
step:992/1393 train_time:125434ms step_avg:127.73ms
step:993/1393 train_time:125571ms step_avg:127.74ms
step:994/1393 train_time:125703ms step_avg:127.75ms
step:995/1393 train_time:125837ms step_avg:127.75ms
step:996/1393 train_time:125970ms step_avg:127.76ms
step:997/1393 train_time:126103ms step_avg:127.76ms
step:998/1393 train_time:126237ms step_avg:127.77ms
step:999/1393 train_time:126370ms step_avg:127.78ms
step:1000/1393 train_time:126504ms step_avg:127.78ms
step:1000/1393 val_loss:3.4144 train_time:126635ms step_avg:127.91ms
step:1001/1393 train_time:126655ms step_avg:127.81ms
step:1002/1393 train_time:126781ms step_avg:127.80ms
step:1003/1393 train_time:126917ms step_avg:127.81ms
step:1004/1393 train_time:127050ms step_avg:127.82ms
step:1005/1393 train_time:127183ms step_avg:127.82ms
step:1006/1393 train_time:127314ms step_avg:127.83ms
step:1007/1393 train_time:127446ms step_avg:127.83ms
step:1008/1393 train_time:127578ms step_avg:127.83ms
step:1009/1393 train_time:127714ms step_avg:127.84ms
step:1010/1393 train_time:127849ms step_avg:127.85ms
step:1011/1393 train_time:127983ms step_avg:127.86ms
step:1012/1393 train_time:128117ms step_avg:127.86ms
step:1013/1393 train_time:128249ms step_avg:127.87ms
step:1014/1393 train_time:128383ms step_avg:127.87ms
step:1015/1393 train_time:128515ms step_avg:127.88ms
step:1016/1393 train_time:128648ms step_avg:127.88ms
step:1017/1393 train_time:128783ms step_avg:127.89ms
step:1018/1393 train_time:128915ms step_avg:127.89ms
step:1019/1393 train_time:129049ms step_avg:127.90ms
step:1020/1393 train_time:129184ms step_avg:127.90ms
step:1021/1393 train_time:129316ms step_avg:127.91ms
step:1022/1393 train_time:129448ms step_avg:127.91ms
step:1023/1393 train_time:129582ms step_avg:127.92ms
step:1024/1393 train_time:129714ms step_avg:127.92ms
step:1025/1393 train_time:129847ms step_avg:127.93ms
step:1026/1393 train_time:129981ms step_avg:127.93ms
step:1027/1393 train_time:130113ms step_avg:127.94ms
step:1028/1393 train_time:130248ms step_avg:127.95ms
step:1029/1393 train_time:130384ms step_avg:127.95ms
step:1030/1393 train_time:130517ms step_avg:127.96ms
step:1031/1393 train_time:130648ms step_avg:127.96ms
step:1032/1393 train_time:130782ms step_avg:127.97ms
step:1033/1393 train_time:130915ms step_avg:127.97ms
step:1034/1393 train_time:131049ms step_avg:127.98ms
step:1035/1393 train_time:131183ms step_avg:127.98ms
step:1036/1393 train_time:131315ms step_avg:127.99ms
step:1037/1393 train_time:131448ms step_avg:127.99ms
step:1038/1393 train_time:131583ms step_avg:128.00ms
step:1039/1393 train_time:131717ms step_avg:128.00ms
step:1040/1393 train_time:131850ms step_avg:128.01ms
step:1041/1393 train_time:131983ms step_avg:128.01ms
step:1042/1393 train_time:132116ms step_avg:128.02ms
step:1043/1393 train_time:132249ms step_avg:128.02ms
step:1044/1393 train_time:132386ms step_avg:128.03ms
step:1045/1393 train_time:132520ms step_avg:128.04ms
step:1046/1393 train_time:132653ms step_avg:128.04ms
step:1047/1393 train_time:132786ms step_avg:128.05ms
step:1048/1393 train_time:132919ms step_avg:128.05ms
step:1049/1393 train_time:133052ms step_avg:128.06ms
step:1050/1393 train_time:133185ms step_avg:128.06ms
step:1051/1393 train_time:133318ms step_avg:128.07ms
step:1052/1393 train_time:133451ms step_avg:128.07ms
step:1053/1393 train_time:133585ms step_avg:128.08ms
step:1054/1393 train_time:133718ms step_avg:128.08ms
step:1055/1393 train_time:133851ms step_avg:128.09ms
step:1056/1393 train_time:133985ms step_avg:128.09ms
step:1057/1393 train_time:134118ms step_avg:128.10ms
step:1058/1393 train_time:134252ms step_avg:128.10ms
step:1059/1393 train_time:134386ms step_avg:128.11ms
step:1060/1393 train_time:134519ms step_avg:128.11ms
step:1061/1393 train_time:134651ms step_avg:128.12ms
step:1062/1393 train_time:134785ms step_avg:128.12ms
step:1063/1393 train_time:134919ms step_avg:128.13ms
step:1064/1393 train_time:135051ms step_avg:128.13ms
step:1065/1393 train_time:135185ms step_avg:128.14ms
step:1066/1393 train_time:135319ms step_avg:128.14ms
step:1067/1393 train_time:135452ms step_avg:128.15ms
step:1068/1393 train_time:135586ms step_avg:128.15ms
step:1069/1393 train_time:135720ms step_avg:128.16ms
step:1070/1393 train_time:135852ms step_avg:128.16ms
step:1071/1393 train_time:135986ms step_avg:128.17ms
step:1072/1393 train_time:136118ms step_avg:128.17ms
step:1073/1393 train_time:136251ms step_avg:128.18ms
step:1074/1393 train_time:136385ms step_avg:128.18ms
step:1075/1393 train_time:136518ms step_avg:128.19ms
step:1076/1393 train_time:136650ms step_avg:128.19ms
step:1077/1393 train_time:136785ms step_avg:128.20ms
step:1078/1393 train_time:136917ms step_avg:128.20ms
step:1079/1393 train_time:137053ms step_avg:128.21ms
step:1080/1393 train_time:137187ms step_avg:128.21ms
step:1081/1393 train_time:137320ms step_avg:128.22ms
step:1082/1393 train_time:137452ms step_avg:128.22ms
step:1083/1393 train_time:137585ms step_avg:128.22ms
step:1084/1393 train_time:137720ms step_avg:128.23ms
step:1085/1393 train_time:137852ms step_avg:128.23ms
step:1086/1393 train_time:137987ms step_avg:128.24ms
step:1087/1393 train_time:138121ms step_avg:128.25ms
step:1088/1393 train_time:138254ms step_avg:128.25ms
step:1089/1393 train_time:138388ms step_avg:128.26ms
step:1090/1393 train_time:138524ms step_avg:128.26ms
step:1091/1393 train_time:138657ms step_avg:128.27ms
step:1092/1393 train_time:138790ms step_avg:128.27ms
step:1093/1393 train_time:138924ms step_avg:128.28ms
step:1094/1393 train_time:139057ms step_avg:128.28ms
step:1095/1393 train_time:139189ms step_avg:128.28ms
step:1096/1393 train_time:139324ms step_avg:128.29ms
step:1097/1393 train_time:139458ms step_avg:128.30ms
step:1098/1393 train_time:139592ms step_avg:128.30ms
step:1099/1393 train_time:139726ms step_avg:128.31ms
step:1100/1393 train_time:139859ms step_avg:128.31ms
step:1101/1393 train_time:139991ms step_avg:128.31ms
step:1102/1393 train_time:140125ms step_avg:128.32ms
step:1103/1393 train_time:140259ms step_avg:128.32ms
step:1104/1393 train_time:140391ms step_avg:128.33ms
step:1105/1393 train_time:140527ms step_avg:128.33ms
step:1106/1393 train_time:140660ms step_avg:128.34ms
step:1107/1393 train_time:140793ms step_avg:128.34ms
step:1108/1393 train_time:140928ms step_avg:128.35ms
step:1109/1393 train_time:141061ms step_avg:128.35ms
step:1110/1393 train_time:141195ms step_avg:128.36ms
step:1111/1393 train_time:141329ms step_avg:128.36ms
step:1112/1393 train_time:141463ms step_avg:128.37ms
step:1113/1393 train_time:141596ms step_avg:128.37ms
step:1114/1393 train_time:141730ms step_avg:128.38ms
step:1115/1393 train_time:141865ms step_avg:128.38ms
step:1116/1393 train_time:141998ms step_avg:128.39ms
step:1117/1393 train_time:142132ms step_avg:128.39ms
step:1118/1393 train_time:142267ms step_avg:128.40ms
step:1119/1393 train_time:142400ms step_avg:128.40ms
step:1120/1393 train_time:142533ms step_avg:128.41ms
step:1121/1393 train_time:142667ms step_avg:128.41ms
step:1122/1393 train_time:142799ms step_avg:128.42ms
step:1123/1393 train_time:142931ms step_avg:128.42ms
step:1124/1393 train_time:143066ms step_avg:128.43ms
step:1125/1393 train_time:143198ms step_avg:128.43ms
step:1125/1393 val_loss:3.3644 train_time:143331ms step_avg:128.55ms
step:1126/1393 train_time:143351ms step_avg:128.45ms
step:1127/1393 train_time:143478ms step_avg:128.45ms
step:1128/1393 train_time:143612ms step_avg:128.45ms
step:1129/1393 train_time:143745ms step_avg:128.46ms
step:1130/1393 train_time:143876ms step_avg:128.46ms
step:1131/1393 train_time:144009ms step_avg:128.46ms
step:1132/1393 train_time:144141ms step_avg:128.47ms
step:1133/1393 train_time:144272ms step_avg:128.47ms
step:1134/1393 train_time:144408ms step_avg:128.48ms
step:1135/1393 train_time:144544ms step_avg:128.48ms
step:1136/1393 train_time:144678ms step_avg:128.49ms
step:1137/1393 train_time:144811ms step_avg:128.49ms
step:1138/1393 train_time:144950ms step_avg:128.50ms
step:1139/1393 train_time:145084ms step_avg:128.51ms
step:1140/1393 train_time:145219ms step_avg:128.51ms
step:1141/1393 train_time:145353ms step_avg:128.52ms
step:1142/1393 train_time:145487ms step_avg:128.52ms
step:1143/1393 train_time:145624ms step_avg:128.53ms
step:1144/1393 train_time:145759ms step_avg:128.53ms
step:1145/1393 train_time:145894ms step_avg:128.54ms
step:1146/1393 train_time:146029ms step_avg:128.55ms
step:1147/1393 train_time:146163ms step_avg:128.55ms
step:1148/1393 train_time:146299ms step_avg:128.56ms
step:1149/1393 train_time:146433ms step_avg:128.56ms
step:1150/1393 train_time:146568ms step_avg:128.57ms
step:1151/1393 train_time:146704ms step_avg:128.58ms
step:1152/1393 train_time:146838ms step_avg:128.58ms
step:1153/1393 train_time:146973ms step_avg:128.59ms
step:1154/1393 train_time:147109ms step_avg:128.59ms
step:1155/1393 train_time:147244ms step_avg:128.60ms
step:1156/1393 train_time:147383ms step_avg:128.61ms
step:1157/1393 train_time:147517ms step_avg:128.61ms
step:1158/1393 train_time:147652ms step_avg:128.62ms
step:1159/1393 train_time:147786ms step_avg:128.62ms
step:1160/1393 train_time:147921ms step_avg:128.63ms
step:1161/1393 train_time:148055ms step_avg:128.63ms
step:1162/1393 train_time:148190ms step_avg:128.64ms
step:1163/1393 train_time:148328ms step_avg:128.65ms
step:1164/1393 train_time:148461ms step_avg:128.65ms
step:1165/1393 train_time:148595ms step_avg:128.65ms
step:1166/1393 train_time:148730ms step_avg:128.66ms
step:1167/1393 train_time:148864ms step_avg:128.66ms
step:1168/1393 train_time:148999ms step_avg:128.67ms
step:1169/1393 train_time:149134ms step_avg:128.67ms
step:1170/1393 train_time:149269ms step_avg:128.68ms
step:1171/1393 train_time:149403ms step_avg:128.69ms
step:1172/1393 train_time:149538ms step_avg:128.69ms
step:1173/1393 train_time:149672ms step_avg:128.70ms
step:1174/1393 train_time:149813ms step_avg:128.71ms
step:1175/1393 train_time:149949ms step_avg:128.71ms
step:1176/1393 train_time:150083ms step_avg:128.72ms
step:1177/1393 train_time:150221ms step_avg:128.72ms
step:1178/1393 train_time:150355ms step_avg:128.73ms
step:1179/1393 train_time:150490ms step_avg:128.73ms
step:1180/1393 train_time:150626ms step_avg:128.74ms
step:1181/1393 train_time:150759ms step_avg:128.74ms
step:1182/1393 train_time:150893ms step_avg:128.75ms
step:1183/1393 train_time:151030ms step_avg:128.76ms
step:1184/1393 train_time:151166ms step_avg:128.76ms
step:1185/1393 train_time:151301ms step_avg:128.77ms
step:1186/1393 train_time:151435ms step_avg:128.77ms
step:1187/1393 train_time:151575ms step_avg:128.78ms
step:1188/1393 train_time:151710ms step_avg:128.79ms
step:1189/1393 train_time:151846ms step_avg:128.79ms
step:1190/1393 train_time:151982ms step_avg:128.80ms
step:1191/1393 train_time:152117ms step_avg:128.80ms
step:1192/1393 train_time:152251ms step_avg:128.81ms
step:1193/1393 train_time:152385ms step_avg:128.81ms
step:1194/1393 train_time:152519ms step_avg:128.82ms
step:1195/1393 train_time:152654ms step_avg:128.82ms
step:1196/1393 train_time:152791ms step_avg:128.83ms
step:1197/1393 train_time:152926ms step_avg:128.83ms
step:1198/1393 train_time:153064ms step_avg:128.84ms
step:1199/1393 train_time:153198ms step_avg:128.85ms
step:1200/1393 train_time:153332ms step_avg:128.85ms
step:1201/1393 train_time:153466ms step_avg:128.85ms
step:1202/1393 train_time:153603ms step_avg:128.86ms
step:1203/1393 train_time:153740ms step_avg:128.87ms
step:1204/1393 train_time:153874ms step_avg:128.87ms
step:1205/1393 train_time:154010ms step_avg:128.88ms
step:1206/1393 train_time:154147ms step_avg:128.89ms
step:1207/1393 train_time:154281ms step_avg:128.89ms
step:1208/1393 train_time:154416ms step_avg:128.89ms
step:1209/1393 train_time:154550ms step_avg:128.90ms
step:1210/1393 train_time:154688ms step_avg:128.91ms
step:1211/1393 train_time:154824ms step_avg:128.91ms
step:1212/1393 train_time:154957ms step_avg:128.92ms
step:1213/1393 train_time:155092ms step_avg:128.92ms
step:1214/1393 train_time:155227ms step_avg:128.93ms
step:1215/1393 train_time:155364ms step_avg:128.93ms
step:1216/1393 train_time:155501ms step_avg:128.94ms
step:1217/1393 train_time:155636ms step_avg:128.94ms
step:1218/1393 train_time:155769ms step_avg:128.95ms
step:1219/1393 train_time:155903ms step_avg:128.95ms
step:1220/1393 train_time:156038ms step_avg:128.96ms
step:1221/1393 train_time:156172ms step_avg:128.96ms
step:1222/1393 train_time:156306ms step_avg:128.97ms
step:1223/1393 train_time:156441ms step_avg:128.97ms
step:1224/1393 train_time:156573ms step_avg:128.97ms
step:1225/1393 train_time:156710ms step_avg:128.98ms
step:1226/1393 train_time:156843ms step_avg:128.98ms
step:1227/1393 train_time:156977ms step_avg:128.99ms
step:1228/1393 train_time:157112ms step_avg:128.99ms
step:1229/1393 train_time:157246ms step_avg:129.00ms
step:1230/1393 train_time:157383ms step_avg:129.00ms
step:1231/1393 train_time:157517ms step_avg:129.01ms
step:1232/1393 train_time:157656ms step_avg:129.01ms
step:1233/1393 train_time:157790ms step_avg:129.02ms
step:1234/1393 train_time:157925ms step_avg:129.02ms
step:1235/1393 train_time:158060ms step_avg:129.03ms
step:1236/1393 train_time:158193ms step_avg:129.03ms
step:1237/1393 train_time:158329ms step_avg:129.04ms
step:1238/1393 train_time:158468ms step_avg:129.05ms
step:1239/1393 train_time:158603ms step_avg:129.05ms
step:1240/1393 train_time:158740ms step_avg:129.06ms
step:1241/1393 train_time:158875ms step_avg:129.06ms
step:1242/1393 train_time:159010ms step_avg:129.07ms
step:1243/1393 train_time:159145ms step_avg:129.07ms
step:1244/1393 train_time:159280ms step_avg:129.08ms
step:1245/1393 train_time:159416ms step_avg:129.08ms
step:1246/1393 train_time:159551ms step_avg:129.09ms
step:1247/1393 train_time:159686ms step_avg:129.09ms
step:1248/1393 train_time:159821ms step_avg:129.10ms
step:1249/1393 train_time:159955ms step_avg:129.10ms
step:1250/1393 train_time:160090ms step_avg:129.10ms
step:1250/1393 val_loss:3.3171 train_time:160225ms step_avg:129.21ms
step:1251/1393 train_time:160245ms step_avg:129.13ms
step:1252/1393 train_time:160367ms step_avg:129.12ms
step:1253/1393 train_time:160501ms step_avg:129.12ms
step:1254/1393 train_time:160634ms step_avg:129.13ms
step:1255/1393 train_time:160773ms step_avg:129.13ms
step:1256/1393 train_time:160906ms step_avg:129.14ms
step:1257/1393 train_time:161042ms step_avg:129.14ms
step:1258/1393 train_time:161175ms step_avg:129.15ms
step:1259/1393 train_time:161312ms step_avg:129.15ms
step:1260/1393 train_time:161446ms step_avg:129.16ms
step:1261/1393 train_time:161580ms step_avg:129.16ms
step:1262/1393 train_time:161717ms step_avg:129.17ms
step:1263/1393 train_time:161851ms step_avg:129.17ms
step:1264/1393 train_time:161985ms step_avg:129.17ms
step:1265/1393 train_time:162120ms step_avg:129.18ms
step:1266/1393 train_time:162256ms step_avg:129.18ms
step:1267/1393 train_time:162390ms step_avg:129.19ms
step:1268/1393 train_time:162525ms step_avg:129.19ms
step:1269/1393 train_time:162661ms step_avg:129.20ms
step:1270/1393 train_time:162795ms step_avg:129.20ms
step:1271/1393 train_time:162930ms step_avg:129.21ms
step:1272/1393 train_time:163064ms step_avg:129.21ms
step:1273/1393 train_time:163198ms step_avg:129.21ms
step:1274/1393 train_time:163332ms step_avg:129.22ms
step:1275/1393 train_time:163468ms step_avg:129.22ms
step:1276/1393 train_time:163603ms step_avg:129.23ms
step:1277/1393 train_time:163737ms step_avg:129.23ms
step:1278/1393 train_time:163872ms step_avg:129.24ms
step:1279/1393 train_time:164008ms step_avg:129.24ms
step:1280/1393 train_time:164144ms step_avg:129.25ms
step:1281/1393 train_time:164280ms step_avg:129.25ms
step:1282/1393 train_time:164414ms step_avg:129.26ms
step:1283/1393 train_time:164550ms step_avg:129.26ms
step:1284/1393 train_time:164686ms step_avg:129.27ms
step:1285/1393 train_time:164819ms step_avg:129.27ms
step:1286/1393 train_time:164954ms step_avg:129.27ms
step:1287/1393 train_time:165089ms step_avg:129.28ms
step:1288/1393 train_time:165224ms step_avg:129.28ms
step:1289/1393 train_time:165362ms step_avg:129.29ms
step:1290/1393 train_time:165501ms step_avg:129.30ms
step:1291/1393 train_time:165638ms step_avg:129.30ms
step:1292/1393 train_time:165771ms step_avg:129.31ms
step:1293/1393 train_time:165909ms step_avg:129.31ms
step:1294/1393 train_time:166043ms step_avg:129.32ms
step:1295/1393 train_time:166177ms step_avg:129.32ms
step:1296/1393 train_time:166313ms step_avg:129.33ms
step:1297/1393 train_time:166449ms step_avg:129.33ms
step:1298/1393 train_time:166584ms step_avg:129.34ms
step:1299/1393 train_time:166719ms step_avg:129.34ms
step:1300/1393 train_time:166853ms step_avg:129.34ms
step:1301/1393 train_time:166988ms step_avg:129.35ms
step:1302/1393 train_time:167122ms step_avg:129.35ms
step:1303/1393 train_time:167261ms step_avg:129.36ms
step:1304/1393 train_time:167398ms step_avg:129.36ms
step:1305/1393 train_time:167533ms step_avg:129.37ms
step:1306/1393 train_time:167668ms step_avg:129.37ms
step:1307/1393 train_time:167804ms step_avg:129.38ms
step:1308/1393 train_time:167940ms step_avg:129.38ms
step:1309/1393 train_time:168077ms step_avg:129.39ms
step:1310/1393 train_time:168211ms step_avg:129.39ms
step:1311/1393 train_time:168345ms step_avg:129.40ms
step:1312/1393 train_time:168481ms step_avg:129.40ms
step:1313/1393 train_time:168617ms step_avg:129.41ms
step:1314/1393 train_time:168751ms step_avg:129.41ms
step:1315/1393 train_time:168885ms step_avg:129.41ms
step:1316/1393 train_time:169019ms step_avg:129.42ms
step:1317/1393 train_time:169155ms step_avg:129.42ms
step:1318/1393 train_time:169288ms step_avg:129.43ms
step:1319/1393 train_time:169425ms step_avg:129.43ms
step:1320/1393 train_time:169561ms step_avg:129.44ms
step:1321/1393 train_time:169695ms step_avg:129.44ms
step:1322/1393 train_time:169833ms step_avg:129.45ms
step:1323/1393 train_time:169968ms step_avg:129.45ms
step:1324/1393 train_time:170103ms step_avg:129.45ms
step:1325/1393 train_time:170238ms step_avg:129.46ms
step:1326/1393 train_time:170374ms step_avg:129.46ms
step:1327/1393 train_time:170508ms step_avg:129.47ms
step:1328/1393 train_time:170642ms step_avg:129.47ms
step:1329/1393 train_time:170782ms step_avg:129.48ms
step:1330/1393 train_time:170917ms step_avg:129.48ms
step:1331/1393 train_time:171055ms step_avg:129.49ms
step:1332/1393 train_time:171192ms step_avg:129.49ms
step:1333/1393 train_time:171328ms step_avg:129.50ms
step:1334/1393 train_time:171464ms step_avg:129.50ms
step:1335/1393 train_time:171597ms step_avg:129.51ms
step:1336/1393 train_time:171734ms step_avg:129.51ms
step:1337/1393 train_time:171871ms step_avg:129.52ms
step:1338/1393 train_time:172005ms step_avg:129.52ms
step:1339/1393 train_time:172140ms step_avg:129.53ms
step:1340/1393 train_time:172276ms step_avg:129.53ms
step:1341/1393 train_time:172411ms step_avg:129.53ms
step:1342/1393 train_time:172546ms step_avg:129.54ms
step:1343/1393 train_time:172680ms step_avg:129.54ms
step:1344/1393 train_time:172815ms step_avg:129.55ms
step:1345/1393 train_time:172950ms step_avg:129.55ms
step:1346/1393 train_time:173086ms step_avg:129.56ms
step:1347/1393 train_time:173223ms step_avg:129.56ms
step:1348/1393 train_time:173359ms step_avg:129.57ms
step:1349/1393 train_time:173495ms step_avg:129.57ms
step:1350/1393 train_time:173630ms step_avg:129.57ms
step:1351/1393 train_time:173765ms step_avg:129.58ms
step:1352/1393 train_time:173904ms step_avg:129.59ms
step:1353/1393 train_time:174042ms step_avg:129.59ms
step:1354/1393 train_time:174180ms step_avg:129.60ms
step:1355/1393 train_time:174314ms step_avg:129.60ms
step:1356/1393 train_time:174448ms step_avg:129.60ms
step:1357/1393 train_time:174585ms step_avg:129.61ms
step:1358/1393 train_time:174721ms step_avg:129.61ms
step:1359/1393 train_time:174856ms step_avg:129.62ms
step:1360/1393 train_time:174995ms step_avg:129.63ms
step:1361/1393 train_time:175130ms step_avg:129.63ms
step:1362/1393 train_time:175267ms step_avg:129.64ms
step:1363/1393 train_time:175405ms step_avg:129.64ms
step:1364/1393 train_time:175542ms step_avg:129.65ms
step:1365/1393 train_time:175677ms step_avg:129.65ms
step:1366/1393 train_time:175811ms step_avg:129.65ms
step:1367/1393 train_time:175947ms step_avg:129.66ms
step:1368/1393 train_time:176084ms step_avg:129.66ms
step:1369/1393 train_time:176224ms step_avg:129.67ms
step:1370/1393 train_time:176364ms step_avg:129.68ms
step:1371/1393 train_time:176501ms step_avg:129.69ms
step:1372/1393 train_time:176639ms step_avg:129.69ms
step:1373/1393 train_time:176775ms step_avg:129.70ms
step:1374/1393 train_time:176913ms step_avg:129.70ms
step:1375/1393 train_time:177047ms step_avg:129.70ms
step:1375/1393 val_loss:3.2826 train_time:177182ms step_avg:129.80ms
step:1376/1393 train_time:177203ms step_avg:129.72ms
step:1377/1393 train_time:177326ms step_avg:129.72ms
step:1378/1393 train_time:177462ms step_avg:129.72ms
step:1379/1393 train_time:177598ms step_avg:129.73ms
step:1380/1393 train_time:177733ms step_avg:129.73ms
step:1381/1393 train_time:177871ms step_avg:129.74ms
step:1382/1393 train_time:178008ms step_avg:129.74ms
step:1383/1393 train_time:178143ms step_avg:129.75ms
step:1384/1393 train_time:178282ms step_avg:129.75ms
step:1385/1393 train_time:178418ms step_avg:129.76ms
step:1386/1393 train_time:178552ms step_avg:129.76ms
step:1387/1393 train_time:178690ms step_avg:129.77ms
step:1388/1393 train_time:178826ms step_avg:129.77ms
step:1389/1393 train_time:178961ms step_avg:129.78ms
step:1390/1393 train_time:179096ms step_avg:129.78ms
step:1391/1393 train_time:179233ms step_avg:129.79ms
step:1392/1393 train_time:179369ms step_avg:129.79ms
step:1393/1393 train_time:179504ms step_avg:129.79ms
step:1393/1393 val_loss:3.2784 train_time:179640ms step_avg:129.89ms
peak memory allocated: 37653 MiB reserved: 39156 MiB
